%% LyX 1.4.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twoside,english]{article}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2cm}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{amssymb}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{graphicx}                              %for PNG images (pdflatex)
%\usepackage{graphics}                              %for EPS images (latex)
\usepackage[linkbordercolor={1.0 1.0 0.0}]{hyperref} %for \url tag
\usepackage{color}                                 %for defining custom colors
\usepackage{framed}                                %for shaded and framed paragraphs
\usepackage{textcomp}                              %for various symbols, e.g. Registered Mark
\usepackage{geometry}                              %for defining page size
\usepackage{longtable}                             %for breaking tables
%
\geometry{verbose,a4paper,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2cm}
\hypersetup{
  pdfauthor = {T.Frågåt},
  pdftitle ={Arc batch system back-end interface guide with support for GLUE 2},
  pdfsubject = {Description and developer's guide for ARC1},
  pdfkeywords = {ARC,ARC1,NorduGrid,manual,Backend,LRMS,Infosystem,GLUE2},
  pdfcreator = {PDFLaTeX with hyperref package},
  pdfproducer = {PDFLaTeX}
}
%
\bibliographystyle{IEEEtran}                       %a nice bibliography style
%
\def\efill{\hfill\nopagebreak}%
\hyphenation{Nordu-Grid}
\setlength{\parindent}{0cm}
\setlength{\FrameRule}{1pt}
\setlength{\FrameSep}{8pt}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\arraystretch}{1.3}
\newcommand{\dothis}{\colorbox{shadecolor}}
\newcommand{\globus}{Globus Toolkit\textsuperscript{\textregistered}~2~}
\newcommand{\GT}{Globus Toolkit\textsuperscript{\textregistered}}
\newcommand{\ngdl}{\url{http://ftp.nordugrid.org/download}~}
\definecolor{shadecolor}{rgb}{1,1,0.6}
\definecolor{salmon}{rgb}{1,0.9,1}
\definecolor{bordeaux}{rgb}{0.75,0.,0.}
\definecolor{cyan}{rgb}{0,1,1}
%

\usepackage{babel}
\makeatother
\begin{document}
\def\today{\number\day/\number\month/\number\year}



\begin{titlepage}



\begin{tabular}{rl}\resizebox*{3cm}{!}{\includegraphics[width=3cm,height=2cm]{ng-logo.png}} &\parbox[b]{2cm}{\textbf \it {\hspace*{-1.5cm}NORDUGRID\vspace*{0.5cm}}} \end{tabular}



\hrulefill

{\raggedleft NORDUGRID-TECH-XX\par}



{\raggedleft \today\par}



\vspace*{2cm}

\begin{center}
\textsc{\Large Arc batch system back-end interface guide with support for GLUE 2}
\par\end{center}{\Large \par}

\vspace*{0.5cm}

\begin{center}
\textit{\large Description and developer's guide for ARC1}
\par\end{center}{\large \par}

\vspace*{1.5cm}

{\centering \large Adrian Taga\footnote{v.a.taga@fys.uio.no}, Thomas Frågåt\footnote{thomasf@fys.uio.no} \par}

\end{titlepage}

\thispagestyle{empty}

$ $

\newpage

\tableofcontents{}

\newpage


\section{Introduction\label{sec:intro}}

This document describes the Advanced Resource Connector~\cite{arc1}
batch system back-end infrastrucure. It aims to describe the infrastructure
in enough detail that a developer can add a new local resource management
system (LRMS) to the ARC middleware. It also intends to serve as a
reference manual describing existing batch system interfaces. Note
that the current solution described in this manual is not a real interface,
it is too diffuse to be that.

The batch system back-ends are what tie the ARC grid middleware (through
(ARC Resource-coupled EXecution servcie, A-REX~\cite{arex}) to the underlying
cluster management system or LRMS. The back-ends consist of set of shell and
Perl scripts whose role are twofold: 

\begin{enumerate}
\item to allow the GM, which is resided in A-REX, to manipulate jobs on the LRMS including job submit,
cancel operations etc.
\item to collect information about jobs, users, batch system and the cluster
itself for the Information System. 
\end{enumerate}
The former will be referred to as the job control back-end interface
while the latter is the information system interface of the batch
system back-ends. These two will be treated separately in the following
sections.

\subsection{ARC Classic}
The ARC Classic has its own solution, please refer to ~\cite{backends-arc0}.

\subsection{ARC1}
As of ARC version 0.9, which is to be the next generation ARC, the scripts are located in different directories
within the NorduGrid subversion tree~\cite{svn}. 
The job control interface scripts can be found under \texttt{arc1/trunk/src/services/a-rex/lrms/}
while the information collectors are located in \texttt{arc1/trunk/src/services/a-rex/infoproviders/}.

The next generation of ARC supports both the classic NorduGrid information schema~\cite{is} and
currently a minimal set of the GLUE specification version 2.0 schema~\cite{glue2}.

\section{Job control interface}

Job control part of the LRMS interface is handled by the Grid Manager~\cite{gm}.
It takes care of preparing a native batch system submission script,
managing the actual submission of the batch system job, cancellation
of job on request and scanning for completed batch jobs. Besides the
LRMS job control interface it is also the GM which provides e.g. the
data staging and communication with the grid client, provides RTE
environments, arranges file staging (to the node via LRMS capability),
dealing with stdout/stderr, etc. The job control batch system interface
of the GM requires three programs. These programs can be implemented
any way the designer sees fit, but all the existing back-end interfaces
use shell scripting for portability and ease of tailoring to a specific
site. The GM will call the following programs: cancel-LRMS-job, submit-LRMS-job,
and scan-LRMS-job. LRMS is replaced with the short hand name for the
LRMS e.g. cancel-pbs-job. The scripts are described one by one in
the following subsections. Useful information can also be found in
the Section \char`\"{}8.6 LRMS Support\char`\"{} and Section \char`\"{}8.7
Runtime Environment\char`\"{} of the Grid-Manager guide~\cite{gm}. 


\subsection{Cancel-LRMS-job}

If a grid user cancels his job, the message will reach the grid-manager.
The manager will then call the cancel-LRMS-job for the suitable back-end.
The cancel script is called with a text file containing information
about the job. It is called the GRAMi file. It contains such information
as the original xRSL option and the job id in the local LRMS. Cancel-LRMS-job
must then use that information to find the job and remove it from
the queue or actually cancel it if it is running in the LRMS.


\subsection{Submit-LRMS-job}

The submit program is the most involved. It is called by the GM once
a new job arrives and needs to be submitted to the LRMS. Like cancel-LRMS-job
it is given the grami file as argument on execution. Submit-LRMS-job
then has to set up the session directories, run-time environment and
anything else needed. Then it should submit the job to the local LRMS.
This is normally done by generating a native job script for the LRMS
and then running the local submit command, but it could also be done
through an API if the LRMS supports it.


\subsection{Scan-LRMS-job}

Scan-LRMS-job is run periodically. Its job is to scan the LRMS for
jobs that have finished. Once it has found a finished job it should
write the exit-code of that job to the file job.\{gridid\}.lrms\_done
in the ARC jobstatus directory. Then it should call the gm-kick program
the GM will then notice that the job has finished and start finalizing
the job.

Generally two approaches are taken to find jobs that are done. One
is asking the LRMS. First all started grid jobs are found in the jobstatus
directory%
\footnote{normally /var/spool/nordugrid/jobstatus/, but can be set via the controldir
variable of arc.conf%
}. This can be done by checking if the status is INLRMS in the jobs
status file%
\footnote{job.\{gridid\}.status%
}. Then the LRMS is asked for the status of those jobs. If they are
done they are marked as such and the GM is activated. The problem
with this approach is that for most LRMSs the information about finished
jobs are only available for a short period after the job finished.
Therefore appropriate steps have to be taken if the job is known to
have been started but is no longer present in the LRMS. The normal
approach is to analyze the jobs status output in the session directory.

The second approach is to parse the LRMSs log files. This method has
some drawbacks e.g. the GM has to be allowed read access to the logs.
The back-end will then have to remember where in the log it was last
time it ran. This information will have to be stored in a file some
where on the front-end.


\subsection{Configuration parser}

Some of the back-ends will need information from the configuration
file. Since this functionality can be shared  among the back-ends,
a configuration file parser written in bash has been provided separately
for easy maintenance and extendability. 


\subsubsection{Functions}

\texttt{config\_parse\_file <config\_file>}

\begin{quote}
Parses a config file. It returns exit status 1 if the file cannot
be read, or 0 otherwise. Badly formed lines are silently ignored.
Option values can be surrounded by sigle quotes or double quotes.
Values without quotes are also accepted. Multi-valued options are
not supported currently. Only the last defined value is retained. 
\end{quote}
\texttt{config\_update\_from\_section <section\_name>}

\begin{quote}
Imports options from section <section\_name> of the config file into
environment variables of the form 'CONFIG\_optionname'. Already existing
environment variables are overwritten.
\end{quote}
Example:

\texttt{source \$ARC\_LOCATION/libexec/config\_parser.sh}

\texttt{config\_parse\_file /etc/arc.conf || exit 1}

\texttt{config\_update\_from\_section common}

\texttt{config\_update\_from\_section grid-manager}

\texttt{config\_update\_from\_section infosys}

\texttt{echo \$CONFIG\_pbs\_bin\_path }


\section{Information System interface}

The main purpose of the information system batch interface is to populate
the nordugrid information model and the GLUE2 model with locally collected
information obtained from the batch system. Important to recall that the
locally collected information is broader than what the batch-system can
offer, information taken from the grid-layer (mostly grid manager), from
the front-end machine and from the \texttt{arc.conf} configuration
file are also needed and used to populate the nordugrid information
model~\cite{is} and the GLUE2 schema~\cite{glue2}.

The information system interface consists of set of Perl scripts which
generates an XML output to \texttt{STDOUT}.

The following subsection describes the LRMS interface part of the
information system. To support a particular LRMS a dedicated Perl module
should be written that implements the interface presented in the next
subsections. Furthermore hooks should be added to the \texttt{LRMS.pm} module,
so that it uses the correct Perl module depending on the LRMS type.

The information collector scripts are divided between several seperate scripts:
\begin{itemize}
\item \texttt{cluster+qju.pl} - driver for information collection. It calls all other infomation collectors and prints results in XML. The information collection is done by one single invocation of this script
\item \texttt{InfoCollector.pm} - base class for all information collectors (i.e.: all files \texttt{*Info.pm})
\item \texttt{InfoChecker.pm} - used by InfoCollector to validate options and results against a simple 'schema' (not XML Schema)
\item \texttt{GMJobsInfo.pm} - collects information about jobs from grid manager status files
\item \texttt{HostInfo.pm} - collects other information that can be collected on the front end (hostname, software version, disk space for users, installed certificates, Runtime environments ...)
\item \texttt{LRMSInfo.pm} - collects information that is LRMS specific (queues, jobs, local user limits ...)
\item \texttt{BATCH\_SYSTEM\_NAME.pm} - plugins for LRMSInfo. Only Fork, SGE and PBS are updated to the new framework
\item \texttt{ARC0ClusterInfo.pm} - combines all information about A-REX and produces information structured according to the classic NG schema
\item \texttt{ARC1ClusterInfo.pm} - combines all information about A-REX and produces information structured according to the GLUE2 schema
\item \texttt{ARC0ClusterSchema.pm} - description of the info produced by ARC0ClusterInfo
\item \texttt{ARC1ClusterSchema.pm} - description of the info produced by ARC1ClusterInfo
\end{itemize}

All information about the hosting environment and queues is collected by one invocation of the perl script named
\texttt{cluster+qju.pl}. The XML output is printed to \texttt{STDOUT} and it includes
two representations of the output data, namely the classic NorduGrid information schema and an incomplete GLUE2 schema representation.


\subsection{The LRMS Perl module interface}

The LRMS module should implement four functions as defined by the
\texttt{LRMS.pm}: \texttt{Cluster\_info}, that provides general information
about the local cluster, \texttt{Queue\_info}, that provides information
about a specified queue, \texttt{Jobs\_info} that gives information
about a list of specified jobs, and finally \texttt{user\_info} that
gives information about a grid user mapped to a local UNIX account.
All the functions are called with a hash containing the contents of
arc.conf.


\subsubsection{cluster\_info}

\texttt{Cluster\_info} is called only with the config hash. It should
then return an associative array with the following keys and the appropriate
value.

\begin{tabular}{|c|c|}
\hline 
key&
value\tabularnewline
\hline
\hline 
lrms\_type&
the type of LRMS e.g PBS\tabularnewline
\hline 
lrms\_version&
the version of the LRMS\tabularnewline
\hline 
totalcpus&
total number of CPUs in the cluster\tabularnewline
\hline 
queuedcpus&
total number of CPUs requested by LRMS queuing jobs (both grid and
non-grid)\tabularnewline
\hline 
usedcpus&
CPUs in the LRMS that are currently in use either by grid or non-grid
jobs\tabularnewline
\hline 
cpudistribution&
number of CPUs in a node and number of each type e.g. {}``8cpu:5
2cpu:100''\tabularnewline
\hline
\end{tabular}


\subsubsection{queue\_info}

In addition to the config \texttt{queue\_info} is called with the
name of the queue to get information on. The result should take the
following form.

\begin{tabular}{|c|c|}
\hline 
key&
value\tabularnewline
\hline
\hline 
status&
available slots in queue, negative number signals error\tabularnewline
\hline 
maxrunning&
limit on number of running jobs\tabularnewline
\hline 
maxqueuable&
limit on number of jobs queued on this queue\tabularnewline
\hline 
maxuserrun&
limit on number of running jobs per user\tabularnewline
\hline 
maxcputime&
limit on maximum CPU time for a job in this queue\tabularnewline
\hline 
mincputime&
limit on minimum CPU time for a job in this queue\tabularnewline
\hline 
defaultcput&
default CPU time limit for a job in this queue\tabularnewline
\hline 
maxwalltime&
limit on maximum wall time for a job in this queue\tabularnewline
\hline 
minwalltime&
limit on minimum wall time for a job in this queue\tabularnewline
\hline 
defaultwallt&
default wall time limit for a job in this queue\tabularnewline
\hline 
running&
number of CPUs used by running jobs (both grid and non-grid) in the
queue\tabularnewline
\hline 
queued&
CPUs requested by queuing jobs in the queue\tabularnewline
\hline 
totalcpus&
number of CPUs available to the queue\tabularnewline
\hline
\end{tabular}


\subsubsection{jobs\_info}

Like the rest the first argument is the config hash, then comes the
queue name and finally a list of job ids. Once again an associative
mapping should be made for each job. The key value pairs are:

\begin{tabular}{|c|c|}
\hline 
key&
value\tabularnewline
\hline
\hline 
status&
LRMS jobstatus mapping%
\footnote{LRMS job states mapping is described in~\cite{is} %
}:Running->'R', Queued->'Q', Suspended->'S', Exiting->'E', Other->'O'\tabularnewline
\hline 
rank&
the job's position in the LRMS queue\tabularnewline
\hline 
mem&
the memory usage of the job in kBs\tabularnewline
\hline 
walltime&
consumed wall-time in minutes\tabularnewline
\hline 
cputime&
consumed CPU-time in minutes\tabularnewline
\hline 
reqwalltime&
wall-time requested by job in minutes\tabularnewline
\hline 
reqcputime&
CPU-time requested by job in minutes\tabularnewline
\hline 
nodes&
list of execution hosts\tabularnewline
\hline 
comment&
array of string comments about the job in LRMS, can be an empty array\tabularnewline
\hline
\end{tabular}


\subsubsection{user\_info}

\texttt{Users\_info} is given the config hash, a queue name and a
list of local UNIX accounts (UIDs). It should then return an associative
array containing for each UID the number of free CPUs available for
that local account on the given queue and the number of queued jobs
on that queue.

\begin{tabular}{|c|c|}
\hline 
key&
value\tabularnewline
\hline
\hline 
freecpus&
number of freely available CPUs with their time limits in minutes
(see nordugrid-authuser-freecpus in~\cite{is})\tabularnewline
\hline 
queuelength&
estimated queue length for the specified user\tabularnewline
\hline
\end{tabular}


\section{Batch system specifics}

This section presents the batch system specific implementation details
including information on supported versions, constraints on batch
system configuration, known limitations, \texttt{arc.conf} parameters
and list of batch system features being utilized within the interface
are described.


\subsection{PBS}

The Portable Batch System (PBS) is one of the most popular batch systems.
PBS comes in many flavours such as OpenPBS (unsupported), Terascale
Open-Source Resource and QUEue Manager (TORQUE) and PBSPro (currently
owned by Altair Engineering). ARC supports all the flavours and versions
of PBS.


\subsubsection{Recommended batch system configuration}

PBS is a very powerful Local Resource Manager System with dozens of
configurable options. Server, queue and node attributes can be used
to configure the cluster's behaviour. In order to correctly interface
PBS to ARC (mainly the information provider scripts) there are a couple
of configuration REQUIREMENTS asked to be implemented by the local
system administrator:

\begin{enumerate}
\item The computing nodes MUST be declared as cluster nodes (job-exclusive),
at the moment time-shared nodes are not supported by the ARC setup.
If you intend to run more than one job on a single processor then
you can use the virtual processor feature of PBS. 
\item For each queue, you MUST set one of the max\_user\_run or max\_running
attributes and its value SHOULD BE IN AGREEMENT with the number of
available resources (i.e. don't set the max\_running = 10 if you have
only six (virtual) processors in your system). If you set both max\_running
and max\_user\_run then obviously max\_user\_run has to be less equal
than max\_running. 
\item For the time being, do NOT set server limits like max\_running, please
use queue-based limits instead. 
\item Avoid using the max\_load and the ideal\_load directives. The node's
mom config file (<PBS home on the node>/mom\_priv/config) should not
contain any max\_load or ideal\_load directives. PBS closes down a
node (no jobs are allocated to it) when the load on the node reaches
the max\_load value. The max\_load value is meant for controlling
time-shared nodes. In case of job-exclusive nodes there is no need
for setting these directives, moreover incorrectly set values can
close down your node. 
\item Routing queues are not supported, those can't be used within ARC.
\end{enumerate}
Additional useful configuration hints:

\begin{itemize}
\item If possible, please use queue-based attributes instead of server level
ones (for the time being, do not use server level attributes at all). 
\item You may use the \char`\"{}acl\_user\_enable = True\char`\"{} with
\char`\"{}acl\_users = user1,user2\char`\"{} attribute to enable user
access control for the queue. 
\item It is advisory to set the max\_queuable attribute in order to avoid
a painfully long dead queue. 
\item You can use node properties from the \texttt{<PBS home on the server>/server\_priv/nodes}
file together with the \texttt{resources\_default.neednodes} to assign
a queue to a certain type of node.
\end{itemize}
Checking your PBS configuration:

\begin{itemize}
\item The node definition can be checked by \texttt{<PBS installation path>/bin/pbsnodes
-a.} All the nodes MUST have ntype=cluster. 
\item The required queue attributes can be checked as \texttt{<PBS installation
path>/bin/qstat -f -Q queuename.} There MUST be a max\_user\_run or
a max\_running attribute listed with a REASONABLE value. 
\end{itemize}

\subsubsection{Relevant \texttt{arc.conf} options}

Below the PBS specific configuration variables are collected.

\begin{itemize}
\item The PBS batch system back-end is enabled via setting the \texttt{lrms=\char`\"{}pbs\char`\"{}}
in the \texttt{{[}common]} configuration block. No need to specify
the flavour or the version number of the PBS, simply use the \texttt{{}``pbs''}
keyword as lrms configuration value.
\item \texttt{pbs\_bin\_path} configuration variable of the \texttt{{[}common]}
block should be set to the path to the qstat,pbsnodes,qmgr etc PBS
binaries.
\item \texttt{pbs\_log\_path} configuration variable of the \texttt{{[}common]}
block should be set to the path of the PBS server logfiles which are
used by the Grid-Manager to determine whether a PBS job is completed.
If not specified, the Grid-Manager will use the \texttt{qstat} command
to find completed jobs. 
\item \texttt{lrmsconfig} from the \texttt{{[}cluster]} block can be used
as an optional free text field to describe further details about the
PBS configuration (e.g. \texttt{lrmsconfig=\char`\"{}single job per
processor\char`\"{}}).
\item \texttt{dedicated\_node\_string} from the \texttt{{[}cluster]} block
specifies the string which is used in the PBS node config to distinguish
the grid nodes from the rest. Suppose only a subset of nodes are available
for grid jobs, and these nodes have a common \texttt{node property}
string, this case the \texttt{dedicated\_node\_string} should be set
to this value and only the nodes with the corresponding PBS \texttt{node
property} are counted as grid enabled nodes. Setting the \texttt{dedicated\_node\_string}
to the value of the PBS \texttt{node property} of the grid-enabled
nodes will influence how the totalcpus, user freecpus is calculated.
No need to set this attribute if the cluster is fully available for
the grid and the PBS configuration does not use the \texttt{node property}
method to assign certain nodes to grid queues.
\item \texttt{{[}queue/queuename]} block. For each grid-enabled (or grid
visible) PBS queue a corresponding \texttt{{[}queue]} block must be
defined. \texttt{queuename} should be the PBS queue name.
\item \texttt{scheduling\_policy} from the \texttt{{[}queue/queuename]}
block describes the scheduling policy of the queue. PBS by default
offers the FIFO scheduler, many sites run the MAUI. At the moment
\texttt{FIFO} \& \texttt{MAUI} are supported values. If you have a
MAUI scheduler you should specify the \char`\"{}MAUI\char`\"{} value
since it modifies the way the queue resources are calculated. By default
the \char`\"{}FIFO\char`\"{} scheduler type is assumed.
\item \texttt{maui\_bin\_path} from the \texttt{{[}queue/queuename]} block
sets the path of the maui commands like \texttt{showbf} when \char`\"{}MAUI\char`\"{}
is specified as \texttt{scheduling\_policy} value. This parameter
can be set in the \texttt{{[}common]} block as well. 
\item \texttt{queue\_node\_string} of the \texttt{{[}queue/queuename]} block
can be used similar to the \texttt{dedicated\_node\_string}. In PBS
you can assign nodes to a queue (or a queue to nodes) by using the
\texttt{node property} PBS node configuration method and assigning
the marked nodes to the queue (setting the \texttt{resources\_default.neednodes}
= \texttt{queue\_node\_string} for that queue). This parameter should
contain the \texttt{node property} string of the queue-assigned nodes.
Setting the \texttt{queue\_node\_string} changes how the queue-totalcpus,
user freecpus are determined for this queue. 
\end{itemize}

\subsubsection{Implementation details}

The job control batch interface makes use of the \texttt{qsub} command
to submit native PBS jobscripts to the batch system. The following
options are used: 

-l nodes, cput, walltime, pvmem, pmem, 

-W stagein, stageout

-e, -j eo

-q

-A 

-N 

For job cancellation the \texttt{qdel} command is used. To find completed
jobs, i.e. to scan for finished jobs the \texttt{qstat} command or
the \texttt{PBS server log file} is used.

The information system interface utilizes the \texttt{qstat -f -Q
queuename} and \texttt{qstat -f queuename} commands to obtain detailed
job and queue information. \texttt{qmgr -c \char`\"{}list server\char`\"{}}
is used to determine PBS flavour and version. The \texttt{pbsnodes}
command is used to calculate total/used/free cpus within the cluster.
In case of a Maui scheduller the \texttt{showbf} command is used to
determine user freecpu values. All these external PBS commands are
interfaced via parsing the commands' output.


\subsubsection{Known limitations}

Some of the limitations are already mentioned under the PBS deployment
requirements. No support for routing queues, difficulty of treating
overlapping queues, the complexity of node string specifications for
parallel jobs are the main shortcomings.


\subsection{Fork}

The Fork back-end is a simple back-end that interfaces to the local
machine i.e. there is no batch system underneath. It simply forks
the job, hence the name. The back-end then uses standard posix commands(e.g.
ps or kill) to manage the job.


\subsubsection{Recommended batch system configuration}

Since fork is a simple back-end and does not use any batch system,
there is no specific configuration needed of the underlying system.


\subsubsection{Relevant arc.conf options}

\begin{itemize}
\item The Fork back-end is enabled by setting \texttt{lrms=\char`\"{}fork\char`\"{}}
in the \texttt{{[}common]} configuration block. 
\item The queue must be named {}``fork'' in the queue section.
\item fork\_job\_limit=\char`\"{}cpunumber\char`\"{}, this option is used
to set the number of running grid jobs on the fork machine, allowing
a multi core machine to use some or all of its cores for Grid jobs.
The default value is 1. 
\end{itemize}

\subsubsection{Implementation details and known limitations}

The Fork backend implements an interface to the {}``fork'' unix
command which is not a batch system. Therefore the backend should
rather be seen as an interface to the operating system itself. Most
of the {}``batch system values'' are determined from the operating
system (e.g. cpu load) or manually set in the configuration file. 

Fork is not a batch system, therefore many of the queue specific attributes
or detailed job information is not available. The support for the
{}``Fork batch system'' was introduced so that quick deployments
and testing of the middleware can be possible without dealing with
deployment of a real batch system since fork is available on every
unix box. The Fork backend is not recommended to be used in production.
The backend by its nature, has lots of limitations, for example does
not support parallel jobs. 


\subsection{SGE}

Sun Grid Engine, formely known as Codine, is an open source batch
system maintained by Sun. Runs on Linux, Solaris.


\subsubsection{Recommended batch system configuration}

Set up one or more SGE queues for access by grid users. Queues can
be shared by normal and grid users. In case you want to set up nore
than one ARC queue, make sure that the corresponding SGE queues have
no shared nodes among them. Otherwise the counts of free and occupied
CPUs might be wrong. Only SGE versions 6 and above are supported.


\subsubsection{Relevant arc.conf options}

The SGE back-end requires that the following options are specified:

\begin{itemize}
\item The SGE batch system back-end is enabled by setting \texttt{lrms=\char`\"{}sge\char`\"{}}
in the \texttt{{[}common]} configuration block. 
\item \texttt{sge\_root} must be set to SGE's install root.
\item \texttt{sge\_bin\_path} configuration variable of the \texttt{{[}common]}
block must be set to the path of the SGE binaries.
\item \texttt{sge\_cell}, \texttt{sge\_qmaster\_port} and \texttt{sge\_execd\_port}
options might be necessary to set in special cases. See the \texttt{arc.conf(5)}
man page for more details.
\item \texttt{sge\_jobopts} configuration variable of the \texttt{{[}queue]}
block can be used to add custom SGE options to job scripts submitted
to SGE. Consult SGE documentation for possible options.
\end{itemize}
Example:

\texttt{lrms=\char`\"{}sge\char`\"{}}

\texttt{sge\_root=\char`\"{}/opt/n1ge6\char`\"{}}

\texttt{sge\_bin\_path=\char`\"{}/opt/n1ge6/bin/lx24-x86\char`\"{}}

\texttt{...}

\texttt{{[}queue/long]}

\texttt{sge\_jobopts=\char`\"{}-P atlas -r yes\char`\"{}}


\subsubsection{Implementation details}

The SGE backend's commands are similar to the PBS commands. These
commands are used in the code:

Submit job: 

\begin{itemize}
\item qsub -S /bin/sh (specifies the interpreting shell for the job)
\end{itemize}
Get jobs status: 

If the job state is not suspended, running or pending then its state
is failed.

\begin{itemize}
\item qstat -u '{*}' -s rs (show the running and suspended jobs status)
\item qstat -u '{*}' -s p (show the pending jobs status)
\item qstat -j job\_id (long job information)
\item qacct -j job\_id (finished job report)
\end{itemize}
Job terminating:

\begin{itemize}
\item qdel job\_id (delete Sun Grid Engine job from the queue)
\end{itemize}
Queue commands:

\begin{itemize}
\item qconf -spl (show a list of all currently defined parallel environments)
\item qconf -sql (show a list of all queues)
\item qconf -sep (show a list of all licensed processors/slots)
\item qstat -g c (display cluster queue summary)
\item qconf -sconf global (show global configuration)
\item qconf -sq queue\_name (show the given queue cofiguration)
\end{itemize}
Other:

\begin{itemize}
\item qstat -help (show Sun Grid Engine's version and type)
\end{itemize}

\subsubsection{Known limitations}

Multi-CPU support is not well tested. All users are shown with the
same quotas in infosys, even if they are mapped to different local
users. The requirement that one ARC queue maps to one SGE queue is
too restrictive, as the SGE's notion of a queue differs widely from
ARC's definition. The flexibility available in SGE for defining policies
is difficult to accurately translate into nordugrid's information
schema. The closest equivalent of nordugrid-queue-maxqueuable is a
per-cluster limit in SGE, and the value of nordugrid-queue-localqueued
is not well defined if pending jobs can have multiple destination
queues. 

\section{The ARC GLUE 2 implementation}
ARC1 provides support for both the classic NorduGrid information
schema~\cite{is} as well as an early minimal GLUE 2 implementation.
The output is rendered in XML format.

Currently, the the following GLUE2 entities are included in ARC1:
\begin{itemize}
\item ComputingService
\item ComputingEndpoint
\item ComputingShare
\end{itemize}

While the following entities are still missing (TODO: According to our table in wiki, there should be some other sections as well):
\begin{itemize}
\item ApplicationEnvironment
\item ExecutionEnvironment
\item ComputingActivity
\item Policy including MappingPolicy / AccessPolicy
\end{itemize}

The different entities with their attributes are described in their respective sections below.

\subsection{ComputingService}

\phantomsection
%\addcontentsline{toc}{subsubsection}{LocalID}
\hspace*{0.5cm}
\begin{shaded}
\textbf{LocalID}
\end{shaded}
\begin{tabular}{lp{10cm}}  
Attribute value:& LocalID\_t\\
Example:&  \verb#TODO#\\
Related xRSL:& \verb#NA#\\
UI role:& TODO \\

\end{tabular}

Description: An opaque identifier local to the Computing Service.

\phantomsection
%\addcontentsline{toc}{subsubsection}{Name}
\hspace*{0.5cm}
\begin{shaded}
\textbf{Name}
\end{shaded}
\begin{tabular}{lp{10cm}}  
Attribute value:& String\\
Example:& \verb#Name: TODO#\\
Related xRSL:& TODO\\
UI role:& ?\\
\end{tabular}

Description: Name of the application environment.

\phantomsection
%\addcontentsline{toc}{subsubsection}{Version}
\hspace*{0.5cm}
\begin{shaded}
\textbf{Version}
\end{shaded}
\begin{tabular}{lp{10cm}}  
Attribute value:& String\\
Example:& \verb#Version: TODO#\\
Related xRSL:& TODO\\
UI role:& TODO\\
\end{tabular}

Description: Version of the application environment.

\subsection{ComputingEndpoint}

\subsection{ComputingShare}

\bibliography{grid}

\end{document}

\documentclass{book}
\usepackage{graphicx}                              %for PNG images (pdflatex)
\usepackage[linkbordercolor={1.0 1.0 0.0}]{hyperref} %for \url tag
\usepackage{color}                                 %for defining custom colors
\usepackage{framed}                                %for shaded and framed paragraphs
\usepackage{textcomp}                              %for various symbols, e.g. Registered Mark
\usepackage{geometry}                              %for defining page size
\usepackage{longtable}                             %for breaking tables
%
\geometry{verbose,a4paper,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2cm}
\hypersetup{
  pdfauthor = {Zsombor Nagy},
  pdftitle = {User's manual of the ARC storage system},
  pdfsubject = {Paper subject},
  pdfkeywords = {Paper,keyword,comma-separated},
  pdfcreator = {PDFLaTeX with hyperref package},
  pdfproducer = {PDFLaTeX}
}
%
\bibliographystyle{IEEEtran}                       %a nice bibliography style
%
\def\efill{\hfill\nopagebreak}%
\hyphenation{Nordu-Grid}
\setlength{\parindent}{0cm}
\setlength{\FrameRule}{1pt}
\setlength{\FrameSep}{8pt}
\addtolength{\parskip}{5pt}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\arraystretch}{1.3}
\newcommand{\dothis}{\colorbox{shadecolor}}
\newcommand{\ngdl}{\url{http://ftp.nordugrid.org/download}~}
\definecolor{shadecolor}{rgb}{1,1,0.6}
\definecolor{salmon}{rgb}{1,0.9,1}
\definecolor{bordeaux}{rgb}{0.75,0.,0.}
\definecolor{cyan}{rgb}{0,1,1}
%
%----- DON'T CHANGE HEADER MATTER
\hyphenation{preserve-Original}
\begin{document}
\def\today{\number\day/\number\month/\number\year}

\begin{titlepage}

\begin{tabular}{rl}
\resizebox*{3cm}{!}{\includegraphics{ng-logo.png}}
&\parbox[b]{2cm}{\textbf \it {\hspace*{-1.5cm}NORDUGRID\vspace*{0.5cm}}}
\end{tabular}

\hrulefill

%-------- Change this to NORDUGRID-XXXXXXX-NN

{\raggedleft NORDUGRID-XXXXXXX-NN\par}

{\raggedleft \today\par}

\vspace*{2cm}

%%%%---- The title ----
{\centering \textsc{\Large Administrator's and user's manual of the ARC storage system}\Large \par}
\vspace*{0.5cm}
    
%%%%---- A subtitle, if necessary ----
%{\centering \textit{\large First prototype status and plans}\large \par}
    
\vspace*{1.5cm}
%%%%---- A list of authors ----
    {\centering \large Zsombor Nagy\footnote{zsombor@niif.hu} \large \par}
    {\centering \large Jon Nilsen\footnote{j.k.nilsen@usit.uio.no} \large \par}
    {\centering \large Salman Zubair Toor \footnote{salman.toor@it.uu.se} \large \par}
\end{titlepage}

\tableofcontents                          %Comment if use article style
% \begin{verbatim}
%     Administrator's and user's manual
%     ---------------------------------
% 
%     Administrator's manual
%     - Quick start guide
%         - Checking the code out from the subversion
%         - Compiling and installing
%         - Modifying the configuration template
%         - Running the daemon
%         - Using the CLI tool
%     - Installation:
%         - Getting the source code from subversion
%         - Compilation details
%         - Installation
%     - Configuration:
%         - HED service config
%         - Centralized A-Hash
%         - Distributed A-Hash
%         - Librarian
%         - Shepherd
%         - Hopi
%         - Bartender
%         - Configuring inter-service trust: local listing of DNs, or using an A-Hash to store them
%         - Virtual organizations: how to configure HED to trust a VO
%     - Deployment scenarios:
%         - Single node: putting every service on one single node (including a storage node with Hopi)
%         - Centralized management: using seperate multiple storage nodes with Hopi, but leaving a central A-Hash, a Librarian and a Bartender on the central node
%         - Simple distribution: split the central node to three nodes, each of them has an A-Hash, a Librarian and a Bartender, and we have separate storage nodes
%         - One service per node: we have seperate nodes for all the services, any number of A-Hash nodes, any number of Librarian nodes, Bartender nodes, storage nodes
% 
%     User's manual
%     - Overview: what do you need to access a running ARC storage system
%     - Prototype command line tool
%         - Configuring the command line tool
%         - Creating the root collection
%         - Uploading files
%         - Creating collections
%         - Moving files
%         - Creating hardlinks
%         - Removing files
%         - Removing collections
%         - Close collections
%         - Modify access policies
%         - Working with mount points
%     - Grid jobs
%         - Configuring the ARC storage DMC
%         - Specifying files in job descriptions
%     - Accessing the system with FUSE
%         - Configuring the FUSE module
%         - Mounting the storage system
%         - Supported operations
%         - Limitations
%     
% \end{verbatim}

\newpage

\renewcommand{\thefootnote}{\arabic{footnote}}


\chapter{Quick start guide} % (fold)

\section{Installing a centralized system} % (fold)

In this section we will follow the installation of the ARC storage system on a freshly installed debian lenny machine. 

Get the latest ARC code from the subversion (\verb!http://svn.nordugrid.org/!):

\begin{verbatim}
$ svn co http://svn.nordugrid.org/repos/nordugrid/arc1/trunk arc1
\end{verbatim}

Let's check the README file for all the dependencies, and install them:

\begin{verbatim}
$ sudo aptitude install build-essential
$ sudo aptitude install autoconf
$ sudo aptitude install gettext
$ sudo aptitude install cvs
$ sudo aptitude install libtool
$ sudo aptitude install pkg-config
$ sudo aptitude install libglibmm-2.4-dev
$ sudo aptitude install python-dev
$ sudo aptitude install swig
$ sudo aptitude install libxml2-dev
$ sudo aptitude install libssl-dev
\end{verbatim}

Run the \verb!autogen! and \verb!configure! scripts (you may want to specify target directories for the installation with the \verb!--prefix! option):

\begin{verbatim}
$ cd arc1
$ ./autogen.sh
$ ./configure --disable-java --disable-a-rex-service --disable-isi-service \
    --disable-charon-service --disable-compiler-service \
    --disable-paul-service --disable-sched-service

Unit testing:       yes
Java binding:       no
Python binding:     yes (2.5)

Available third-party features:

RLS:                no
GridFTP:            no
LFC:                no
RSL:                no
SAML:               no
MYSQL CLIENT LIB:   no
gSOAP:              no

Included components:
A-Rex service:      no
ISI service:        no
CHARON service:     no
HOPI service:       yes
SCHED service:      no
STORAGE service:    yes
PAUL service:       no
SRM client (DMC):   no
GSI channel (MCC):  no

\end{verbatim}

We can start to compile it:

\begin{verbatim}
$ make
\end{verbatim}

And then install it (you need to be root if you want to install it to the default location):

\begin{verbatim}
$ sudo make install
\end{verbatim}

Maybe we should run \verb!ldconfig! to refresh the list of libraries:

\begin{verbatim}
$ ldconfig
\end{verbatim}

We can check if the python packages are installed correctly:

\begin{verbatim}
$ python
Python 2.5.2 (r252:60911, Jan  4 2009, 17:40:26) 
[GCC 4.3.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import arc
>>> import storage
>>> import arcom
>>> 
\end{verbatim}

If there is no error on importing any of the packages, we are good to go. If the packages cannot be imported, we should set the PYTHONPATH environment variable to point to their location.

It is important to syncronize the time of the machine - the storage system will make decisions based on differences between timestamps, when we deploy it on multiple machines, different times would cause problems.

We need host (and user) certificates to run the system. For testing purposes we can use the NorduGrid InstantCA solution which is capable of creating a demo-CA with short lifetime. The URL of the InstantCA service is \verb!https://vls.grid.upjs.sk/CA/instantCA!, let's create 3 user and 3 host certificates. (e.g. we can set the Organization Name to `knowarc', the Common name of the CA to `storage-test', the name of the users could be `penny', `billy' and `hammer', the name of the hosts could be `upsilon', `omicron' and `theta', and we need a password for the CA and passwords for the user certificates, both should be at least 4 characters long).

Let's download the generated certificates and untar the archive file to see its contents:

\begin{verbatim}
$ ls -lR
.:
total 28
drwxr-xr-x 2 zsombor zsombor 4096 2009-03-19 12:11 CA
-rw-r--r-- 1 zsombor zsombor  963 2009-03-19 12:11 ca.key
-rw-r--r-- 1 zsombor zsombor  786 2009-03-19 12:11 ca.pem
drwxr-xr-x 2 zsombor zsombor 4096 2009-03-19 12:11 hostCerts
-rw-r--r-- 1 zsombor zsombor 1044 2009-03-19 12:11 readme
-rw-r--r-- 1 zsombor zsombor    3 2009-03-19 12:11 serial.srl
drwxr-xr-x 2 zsombor zsombor 4096 2009-03-19 12:11 userCerts

./CA:
total 8
-rw-r--r-- 1 zsombor zsombor 786 2009-03-19 12:11 66fe707c.0
-rw-r--r-- 1 zsombor zsombor 139 2009-03-19 12:11 66fe707c.signing_policy

./hostCerts:
total 24
-rw-r--r-- 1 zsombor zsombor 786 2009-03-19 12:11 hostcert-omicron.pem
-rw-r--r-- 1 zsombor zsombor 782 2009-03-19 12:11 hostcert-theta.pem
-rw-r--r-- 1 zsombor zsombor 786 2009-03-19 12:11 hostcert-upsilon.pem
-rw-r--r-- 1 zsombor zsombor 887 2009-03-19 12:11 hostkey-omicron.pem
-rw-r--r-- 1 zsombor zsombor 891 2009-03-19 12:11 hostkey-theta.pem
-rw-r--r-- 1 zsombor zsombor 887 2009-03-19 12:11 hostkey-upsilon.pem

./userCerts:
total 24
-rw-r--r-- 1 zsombor zsombor 778 2009-03-19 12:11 usercert-billy.pem
-rw-r--r-- 1 zsombor zsombor 778 2009-03-19 12:11 usercert-hammer.pem
-rw-r--r-- 1 zsombor zsombor 778 2009-03-19 12:11 usercert-penny.pem
-rw-r--r-- 1 zsombor zsombor 963 2009-03-19 12:11 userkey-billy.pem
-rw-r--r-- 1 zsombor zsombor 963 2009-03-19 12:11 userkey-hammer.pem
-rw-r--r-- 1 zsombor zsombor 951 2009-03-19 12:11 userkey-penny.pem
\end{verbatim}

We have certificate and key files for all the hosts and users, and we have the CA file with the proper hashed name.
There is no rule where to put these certificate, but now we will put them in \verb!/etc/grid-security!. Let's create this directory, and a subdirectory called \verb!certificates!. We will use the `theta' host certificates for this machine, let's copy it there:

\begin{verbatim}
$ sudo cp hostCerts/hostcert-theta.pem /etc/grid-security/hostcert.pem
$ sudo cp hostCerts/hostkey-theta.pem /etc/grid-security/hostkey.pem
$ sudo cp CA/* /etc/grid-security/certificates/
$ ls -lR /etc/grid-security/
/etc/grid-security/:
total 12
drwxr-xr-x 2 root root 4096 2009-03-19 12:36 certificates
-rw-r--r-- 1 root root  782 2009-03-19 12:35 hostcert.pem
-rw-r--r-- 1 root root  891 2009-03-19 12:36 hostkey.pem

/etc/grid-security/certificates:
total 8
-rw-r--r-- 1 root root 786 2009-03-19 12:36 66fe707c.0
-rw-r--r-- 1 root root 139 2009-03-19 12:36 66fe707c.signing_policy
\end{verbatim}

And I choose a user certificate and put it into the \verb!~/.arc! directory, conveniently removing the password from the key file:

\begin{verbatim}
$ mkdir ~/.arc
$ cp userCerts/usercert-billy.pem ~/.arc/usercert.pem
$ openssl rsa -in userCerts/userkey-billy.pem -out ~/.arc/userkey.pem
$ chmod 600 ~/.arc/userkey.pem
$ ls -l ~/.arc
total 8
-rw-r--r-- 1 zsombor zsombor 778 2009-03-19 13:06 usercert.pem
-rw------- 1 zsombor zsombor 891 2009-03-19 13:09 userkey.pem
\end{verbatim}

The storage system runs withen the \verb!arched! hosting environment daemon, which needs a configuration file which describes which services we want to run, and on which ports do we want to listen, etc.
Let's copy the template configuration files to the \verb!/etc/arc! directory:

\begin{verbatim}
$ sudo mkdir /etc/arc
$ sudo cp arc1/src/services/storage/storage_service.xml.example /etc/arc/storage_service.xml
$ sudo cp arc1/src/services/storage/clientsslconfig.xml /etc/arc
$ ls -l /etc/arc
total 12
-rw-r--r-- 1 root root  259 2009-03-19 13:26 clientsslconfig.xml
-rw-r--r-- 1 root root 4606 2009-03-19 13:25 storage_service.xml
\end{verbatim}

The template configuration file specifies several directories where the services can store their data. We will create these directories at \verb!/var/spool/arc!, but they can be created anywhere else.

\begin{verbatim}
$ sudo mkdir /var/spool/arc/ahash_data
$ sudo mkdir /var/spool/arc/shepherd_data
$ sudo mkdir /var/spool/arc/shepherd_store
$ sudo mkdir /var/spool/arc/shepherd_transfer
$ ls -l /var/spool/arc
total 16
drwxr-xr-x 2 root root 4096 2009-03-19 13:42 ahash_data
drwxr-xr-x 2 root root 4096 2009-03-19 13:42 shepherd_data
drwxr-xr-x 2 root root 4096 2009-03-19 13:42 shepherd_store
drwxr-xr-x 2 root root 4096 2009-03-19 13:42 shepherd_transfer
\end{verbatim}

In this deployment we plan to run the daemon as root, so it is OK that the root owns these directories. 
If we create these directories somewhere else, we would need to modify the paths in the \verb!storage_service.xml! file.

If we put the host certificates and the CA certificate somewhere else than \verb!/etc/grid-security! then we would need to modify the \verb!storage_service.xml! (look for \verb!KeyPath!, \verb!CertificatePath! and \verb!CACertificatesDir!), and we would need to modify \verb!/etc/arc/clientsslconfig.xml! as well. This file contains the credentials which is used by the services when they connect to an other service. If we put \verb!clientsslconfig.xml! somewhere else than \verb!/etc/arc! then we would need to modify all the \verb!ClientSSLConfig! tags in the \verb!storage_service.xml!.

Let's run the \verb!arched! daemon with this config (specified by the \verb!-c! option) first in the foreground (using the \verb!-f! option) to see immediately if everything is right:

\begin{verbatim}
$ sudo /usr/local/sbin/arched -c /etc/arc/storage_service.xml -f
\end{verbatim}

The template configuration file specifies a loglevel which only prints error messages, so nothing should be written on the screen now.
The \verb!arched! daemon should now listen on the ports: 60001 for our data transfer service called Hopi, and 60000 for all the other services:

\begin{verbatim}
$ netstat -at | grep LISTEN
tcp        0      0 *:60000                 *:*                     LISTEN     
tcp        0      0 *:60001                 *:*                     LISTEN     
\end{verbatim}

Now it is time to set up or prototype client tool to access the system. It is currently called \verb!arc_storage_cli! and it is by default installed to \verb!/usr/local/bin!. We have to tell this CLI tool where it can find our user credentials, and a URL of a Bartender service. The Bartender service is the front-end of the storage system. We can specify these things by environment variables, or by a configuration file called \verb!~/.arc/client.xml!. Let's create this \verb!client.xml! file:

\begin{verbatim}
$ cat ~/.arc/client.xml 
<ArcConfig>
  <KeyPath>/home/<username>/.arc/userkey.pem</KeyPath>
  <CertificatePath>/home/<username>/.arc/usercert.pem</CertificatePath>
  <CACertificatesDir>/etc/grid-security/certificates</CACertificatesDir>
  <BartenderURL>https://localhost:60000/Bartender</BartenderURL>
</ArcConfig>
\end{verbatim}

Now we can use the \verb!arc_storage_cli! to access our local deployment. Let's do a `list' on the root collection (`/'):

\begin{verbatim}
$ arc_storage_cli
Usage:
  arc_storage_cli <method> [<arguments>]
Supported methods:
  stat - get detailed information about an entry
  makeCollection | make | mkdir - create a collection
  unmakeCollection | unmake | rmdir - remove an empty collection
  list | ls - list the content of a collection
  move | mv - move entries within the namespace
  putFile | put - upload a file
  getFile | get - download a file
  delFile | del | rm - remove a file
  modify | mod - modify metadata
  policy | pol - modify access policy rules
  unlink - remove a link to an entry from a collection without removing the entry itself
  credentialsDelegation | cre - delegate credentials for using gateway
  removeCredentials | rem - remove previously delegated credentials
Without arguments, each method prints its own help.

$ arc_storage_cli list
- The URL of the Bartender: https://localhost:60000/Bartender
- The key file: /home/zsombor/.arc/userkey.pem
- The cert file: /home/zsombor/.arc/usercert.pem
- The CA dir: /etc/grid-security/certificates
Usage: list <LN> [<LN> ...]

$ arc_storage_cli list /
- The URL of the Bartender: https://localhost:60000/Bartender
- The key file: /home/zsombor/.arc/userkey.pem
- The cert file: /home/zsombor/.arc/usercert.pem
- The CA dir: /etc/grid-security/certificates
- Calling the Bartender's list method...
[2009-03-19 18:37:12] [Arc.Loader] [ERROR] [12491/152991544]
    Component tcp.client(tcp) could not be created
[2009-03-19 18:37:12] [Arc.Loader] [ERROR] [12491/152991544]
    Component tls.client(tls) could not be created
[2009-03-19 18:37:12] [Arc.Loader] [ERROR] [12491/152991544]
    Component http.client(http) could not be created
[2009-03-19 18:37:12] [Arc.Loader] [ERROR] [12491/152991544]
    Component soap.client(soap) could not be created
ERROR: Wrong status from server.
Maybe wrong URL: 'https://localhost:60000/Bartender'
\end{verbatim}

If you get this error, where the `Component [...] could not be created', we should reinforce the python environment about the location of our python packages:

\begin{verbatim}
$ export PYTHONPATH=/usr/local/lib/python2.5/site-packages/
\end{verbatim}

Without arguments, the \verb!arc_storage_cli! prints the list of available methods. Specifying only a method name prints the syntex of the given method. The \verb!list! method requires Logical Names (LN). In the storage system, every file and collection has a Logical Name, which is a path within the namespace.

Now we have a running (centralized) storage service including a Hopi service, which is a basic HTTP server, for transfering files. We have a Bartender service which is the front-end of the system to the users, so our client always connect to the Bartender service. There are several directories configured in the config file where the services store their data. These directories can always be emptied, which resets the whole system to a clean state. If we run the storage system with clean data directories, then it is completely empty, which means that there is not even a root collection.

\begin{verbatim}
$ arc_storage_cli list /
- The URL of the Bartender: https://localhost:60000/Bartender
- The key file: /home/zsombor/.arc/userkey.pem
- The cert file: /home/zsombor/.arc/usercert.pem
- The CA dir: /etc/grid-security/certificates
- Calling the Bartender's list method...
- done in 0.34 seconds.
'/': not found
\end{verbatim}

(From now on I will omit the first few lines of the \verb!arc_storage_cli!'s output.)

The root collection has the Logical Name of `\verb!/!', and it's role to provide a starting point for all the other Logical Names. We have to create this root collection first:

\begin{verbatim}
$ arc_storage_cli mkdir /
- Calling the Bartender's makeCollection method...
- done in 0.81 seconds.
Creating collection '/': done

$ arc_storage_cli list /
- Calling the Bartender's list method...
- done in 0.55 seconds.
'/': collection
    empty.
\end{verbatim}

Now we do have an empty root collection.

\section{Using the prototype client tool} % (fold)

Assume that we have a running storage system and we want to access it with the \verb!arc_storage_cli! client. We have the \verb!~/.arc/client.xml! set properly, so we can list the contents of the root collection:

\begin{verbatim}
$ arc_storage_cli list /
- Calling the Bartender's list method...
- done in 0.55 seconds.
'/': collection
    empty.
\end{verbatim}

Let's create a small file and upload it:

\begin{verbatim}
$ cat > orange
orange
$ cat orange 
orange

$ arc_storage_cli put orange /
- The size of the file is 7 bytes
- Calculating md5 checksum...
- done in 0.0003 seconds, md5 checksum is 6aefd2842be62cd470709b27aedc7db7
- Calling the Bartender's putFile method...
- done in 1.26 seconds.
- Got transfer URL: http://localhost:60001/hopi/7fcef91e-ac9c-658d-4def-17d52ba33cfb
- Uploading from 'orange'
    to 'http://localhost:60001/hopi/7fcef91e-ac9c-658d-4def-17d52ba33cfb' with http...
    0 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 0.0182 seconds.
'orange' (7 bytes) uploaded as '/orange'.

$ arc_storage_cli list /
- Calling the Bartender's list method...
- done in 0.51 seconds.
'/': collection
  orange              <file>

\end{verbatim}

We can get additional information about the file with the stat method:

\begin{verbatim}
$ arc_storage_cli stat /orange
- Calling the Bartender's stat method...
- done in 0.40 seconds.
'/orange': found
  states
    checksumType: md5
    neededReplicas: 1
    size: 7
    checksum: 6aefd2842be62cd470709b27aedc7db7
  entry
    owner: /DC=eu/DC=KnowARC/O=knowarc/CN=billy
    GUID: 12ce831b-eab8-1f47-29f6-19fb85ebf46a
    type: file
  parents
    0/orange: parent
  locations
    https://localhost:60000/Shepherd 3e9ea612-87c0-1e81-58f3-cec6b0b125c1: alive
  timestamps
    created: 1237504821.91
\end{verbatim}

Here we can see the owner of the file (Billy), the location of the replicas (currently only one), the number of needed replicas (which is also: one), the checksum and size of the file, and the timestamp of the creation.

We can download this file:

\begin{verbatim}
$ arc_storage_cli get /orange /tmp
- Calling the Bartender's getFile method...
- done in 0.51 seconds.
- Got transfer URL: http://localhost:60001/hopi/521c5869-4c93-8e28-aa3a-03f477f34f63
- Downloading from 'http://localhost:60001/hopi/521c5869-4c93-8e28-aa3a-03f477f34f63'
    to '/tmp/orange' with http...
    0 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 1.0071 seconds.
'/orange' (7 bytes) downloaded as '/tmp/orange'.
$ cat /tmp/orange 
orange
\end{verbatim}

We can create collections which can contain files and other collections forming a tree-hierarchy. We can move files and collection around within this namespace.

\begin{verbatim}
$ arc_storage_cli mkdir /fruits
- Calling the Bartender's makeCollection method...
- done in 0.79 seconds.
Creating collection '/fruits': done

$ arc_storage_cli mkdir /fruits/apple
- Calling the Bartender's makeCollection method...
- done in 0.91 seconds.
Creating collection '/fruits/apple': done

$ arc_storage_cli move /orange /fruits/
- Calling the Bartender's move method...
- done in 0.99 seconds.
Moving '/orange' to '/fruits/': moved

$ arc_storage_cli list /fruits
- Calling the Bartender's list method...
- done in 0.60 seconds.
'/fruits': collection
  orange              <file>
  apple               <collection>
\end{verbatim}

We can remove files, and remove collections (but only if the collection is empty):

\begin{verbatim}
$ arc_storage_cli rmdir /fruits
- Calling the Bartender's unmakeCollection method...
- done in 0.41 seconds.
Removing collection '/fruits': collection is not empty

$ arc_storage_cli rmdir /fruits/apple
- Calling the Bartender's unmakeCollection method...
- done in 0.89 seconds.
Removing collection '/fruits/apple': removed

$ arc_storage_cli rm /fruits/orange
- Calling the Bartender's delFile method...
- done in 0.89 seconds.
/fruits/orange: deleted

$ arc_storage_cli rmdir /fruits
- Calling the Bartender's unmakeCollection method...
- done in 0.79 seconds.
Removing collection '/fruits': removed
\end{verbatim}

Let's introduce a new user into the system. If we have some other user certificate and key files, then we can tell \verb!arc_storage_cli! to use them with modifying the \verb!client.xml! or by specifying the \verb!ARC_CERT_FILE! and \verb!ARC_KEY_FILE! environment variables.

\begin{verbatim}
$ ARC_CERT_FILE=usercert-penny.pem ARC_KEY_FILE=userkey-penny.pem \
arc_storage_cli list /
- Calling the Bartender's list method...
- done in 0.31 seconds.
'/': denied

$ ARC_CERT_FILE=usercert-penny.pem ARC_KEY_FILE=userkey-penny.pem \
arc_storage_cli put /tmp/orange /pennys-orange
- The size of the file is 7 bytes
- Calculating md5 checksum...
- done in 0.0001 seconds, md5 checksum is 6aefd2842be62cd470709b27aedc7db7
- Calling the Bartender's putFile method...
- done in 0.81 seconds.
/pennys-orange: failed to add child to parent

\end{verbatim}

Listing the root collection or uploading files into it for Penny is denied, because the root collection is owned by Billy, and nobody else has permissions to it. Let's ask Billy to allow for everyone to access the root collection:

\begin{verbatim}
$ arc_storage_cli pol / change ALL +read +addEntry
- Calling the Bartender's stat method...
- done in 0.30 seconds.
old action list for the user ALL was ''
the new action list of user ALL for LN / will be '+read +addEntry'
- Calling the Bartender's modify method...
- done in 0.50 seconds.
set.
\end{verbatim}

The syntax of the \verb!policy! method is the following:

\begin{verbatim}
$ arc_storage_cli pol
Usage: policy <LN> <changeType> <identity> <action list>
  <changeType> could be 'set', 'change' or 'clear'
    'set': sets the action list to the given user overwriting the old one
    'change': modify the current action list with adding and removing actions
    'clear': clear the action list of the given user
  <identity> could be a '<UserDN>' or a 'VOMS:<VO name>'
  <action list> is a list actions prefixed with '+' or '-'
    e.g. '+read +addEntry -delete'
    possible actions:
        read addEntry removeEntry delete
        modifyPolicy modifyStates modifyMetadata
\end{verbatim}

So we have changed the action list for user `ALL' to `+read +addEntry', which means that everybody can list the contents of this collection, create subcollection in it, upload a file into it, or move some existing entry into it.

Now we can upload a file with Penny:

\begin{verbatim}
$ ARC_CERT_FILE=usercert-penny.pem ARC_KEY_FILE=userkey-penny.pem \
arc_storage_cli put /tmp/orange /pennys-orange
- The size of the file is 7 bytes
- Calculating md5 checksum...
- done in 0.0001 seconds, md5 checksum is 6aefd2842be62cd470709b27aedc7db7
- Calling the Bartender's putFile method...
- done in 1.18 seconds.
- Got transfer URL: http://localhost:60001/hopi/4145891d-0e4c-b6bd-e4d6-dc80554b9d35
- Uploading from '/tmp/orange'
    to 'http://localhost:60001/hopi/4145891d-0e4c-b6bd-e4d6-dc80554b9d35' with http...
    1 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 1.0071 seconds.
'/tmp/orange' (7 bytes) uploaded as '/pennys-orange'.
\end{verbatim}

This file now only can be downloaded by Penny, so let's allow for Billy to download it as well:

\begin{verbatim}
$ ARC_CERT_FILE=usercert-penny.pem ARC_KEY_FILE=userkey-penny.pem \
arc_storage_cli pol /pennys-orange change /DC=eu/DC=KnowARC/O=knowarc/CN=billy +read
- Calling the Bartender's stat method...
- done in 0.41 seconds.
old action list for the user /DC=eu/DC=KnowARC/O=knowarc/CN=billy was ''
the new action list of user /DC=eu/DC=KnowARC/O=knowarc/CN=billy for LN /pennys-orange
    will be '+read'
- Calling the Bartender's modify method...
- done in 0.59 seconds.
set.

$ arc_storage_cli stat /pennys-orange
- Calling the Bartender's stat method...
- done in 0.43 seconds.
'/pennys-orange': found
  locations
    https://localhost:60000/Shepherd 1cc3b115-2350-8334-d206-a7b891912c00: alive
  states
    checksumType: md5
    neededReplicas: 1
    size: 7
    checksum: 6aefd2842be62cd470709b27aedc7db7
  parents
    0/pennys-orange: parent
  timestamps
    created: 1237509633.17
  policy
    /DC=eu/DC=KnowARC/O=knowarc/CN=billy: +read
  entry
    owner: /DC=eu/DC=KnowARC/O=knowarc/CN=penny
    GUID: 0e6a4c9a-d4e7-e0dc-e083-16b83d460d82
    type: file
\end{verbatim}

We can see among the metadata of the file, that Billy has `read' rights. Billy can now download this file:

\begin{verbatim}
$ arc_storage_cli get /pennys-orange /tmp
- Calling the Bartender's getFile method...
- done in 0.50 seconds.
- Got transfer URL: http://localhost:60001/hopi/c16d7e12-42b0-7882-c09e-1cac31dbdbcd
- Downloading from 'http://localhost:60001/hopi/c16d7e12-42b0-7882-c09e-1cac31dbdbcd'
    to '/tmp/pennys-orange' with http...
    0 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 1.0065 seconds.
'/pennys-orange' (7 bytes) downloaded as '/tmp/pennys-orange'.
$ cat /tmp/pennys-orange 
orange
\end{verbatim}

We can specify the needed number of replicas for a new file with the \verb!ARC_NEEDED_REPLICAS! environment variable.

\begin{verbatim}
$ ARC_NEEDED_REPLICAS=2 arc_storage_cli put orange /new-orange
- The size of the file is 7 bytes
- Calculating md5 checksum...
- done in 0.0001 seconds, md5 checksum is 6aefd2842be62cd470709b27aedc7db7
- Calling the Bartender's putFile method...
- done in 1.11 seconds.
- Got transfer URL: http://localhost:60001/hopi/0ad54182-7618-a2af-c490-dea7c4d6fc31
- Uploading from 'orange' to
    'http://localhost:60001/hopi/0ad54182-7618-a2af-c490-dea7c4d6fc31' with http...
    1 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 1.0057 seconds.
'orange' (7 bytes) uploaded as '/new-orange'.

$ arc_storage_cli stat /new-orange
- Calling the Bartender's stat method...
- done in 0.40 seconds.
'/new-orange': found
  states
    checksumType: md5
    neededReplicas: 2
    size: 7
    checksum: 6aefd2842be62cd470709b27aedc7db7
  entry
    owner: /DC=eu/DC=KnowARC/O=knowarc/CN=billy
    GUID: 4e673e28-4769-50af-f02e-d7a67632f87a
    type: file
  parents
    0/new-orange: parent
  locations
    https://localhost:60000/Shepherd 4aacd276-653c-3ca5-c1a2-78a876c129c9: alive
  timestamps
    created: 1237509989.43

\end{verbatim}

We can see that the `neededReplica' metadata says 2, but if we deployed the system in a centralized way for testing purposes, then we only have one storage element (which is a Shepherd service and a Hopi service together), so the file can only have one replica, that's why there is only one locations.

We can modify the number of needed replicas of an existing file with the `modify' method:

\begin{verbatim}
$ arc_storage_cli modify /new-orange set states neededReplicas 1
- Calling the Bartender's modify method...
- done in 0.63 seconds.
set

$ arc_storage_cli stat /new-orange | grep neededReplicas
    neededReplicas: 1
\end{verbatim}

\section{Adding more storage elements}

Let's add a new storage element to the system. We need to deploy a Shepherd service and a storage element service, which this time will be a `byteio' service. The `byteio' service implements a small part of the ByteIO interface, which means that it just put the entire file data into the SOAP message, so it is only suitable for small files.

But first modify the configuration file of our original server. Currently all the transfer URLs which were generated when we upload and download files contained the hostname `localhost'. This is not OK if we want to access it from an other machine, we should provide transfer URLs which contains the external hostname of the machine. So lets change this line in \verb!/etc/arc/storage_service.xml!:

\begin{verbatim}
    <TURLPrefix>http://localhost:60001/hopi/</TURLPrefix>
\end{verbatim}

to the real hostname of the machine. But this is not enough. Currently the Shepherd services uses their URLs as IDs, and the other services use these IDs to connect to a Shepherd service, so we need to change to ID of the Shepherd service to a real external URL as well. And for the sake of clarity let's change all `localhost' entries in the config file to our real hostname. In our case the name of this machine is just `storage' so the Shepherd section should look like this:

\begin{verbatim}
<Service name="pythonservice" id="shepherd">
    <ClassName>storage.shepherd.shepherd.ShepherdService</ClassName>
    <ServiceID>https://storage:60000/Shepherd</ServiceID>
    <CheckPeriod>20</CheckPeriod>
    <MinCheckInterval>0.1</MinCheckInterval>
    <CreatingTimeout>600</CreatingTimeout>
    <StoreClass>arcom.store.cachedpicklestore.CachedPickleStore</StoreClass>
    <StoreCfg><DataDir>/var/spool/arc/shepherd_data</DataDir></StoreCfg>
    <BackendClass>storage.shepherd.hardlinkingbackend.HopiBackend</BackendClass>
    <BackendCfg>
        <DataDir>/var/spool/arc/shepherd_store</DataDir>
        <TransferDir>/var/spool/arc/shepherd_transfer</TransferDir>
        <TURLPrefix>http://storage:60001/hopi/</TURLPrefix>
    </BackendCfg>
    <LibrarianURL>https://storage:60000/Librarian</LibrarianURL>
    <BartenderURL>https://storage:60000/Bartender</BartenderURL>
    <ClientSSLConfig FromFile="/etc/arc/clientsslconfig.xml" />
</Service>
\end{verbatim}

Let's check first if our centralized deployment is still working, stop the \verb!arched! daemon remove the data directories to make it a clean start, and restart it with the modified config.

\begin{verbatim}
$ sudo rm -r /var/spool/arc/*
$ sudo /usr/local/sbin/arched -c /etc/arc/storage_service.xml -f    
\end{verbatim}

Let's see if we can still use the client:

\begin{verbatim}
$ arc_storage_cli list /
- Calling the Bartender's list method...
- done in 0.31 seconds.
'/': not found

$ arc_storage_cli mkdir /
- Calling the Bartender's makeCollection method...
- done in 0.63 seconds.
Creating collection '/': done

$ arc_storage_cli list /
- Calling the Bartender's list method...
- done in 0.50 seconds.
'/': collection
    empty.
\end{verbatim}

Let's upload a file and ask for two replicas:

\begin{verbatim}
$ ARC_NEEDED_REPLICAS=2 arc_storage_cli put orange /
- The size of the file is 7 bytes
- Calculating md5 checksum...
- done in 0.0005 seconds, md5 checksum is 6aefd2842be62cd470709b27aedc7db7
- Calling the Bartender's putFile method...
- done in 1.10 seconds.
- Got transfer URL: http://storage:60001/hopi/139dc57d-8c12-c188-a4a6-973449553811
- Uploading from 'orange'
    to 'http://storage:60001/hopi/139dc57d-8c12-c188-a4a6-973449553811' with http...
    1 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 1.0090 seconds.
'orange' (7 bytes) uploaded as '/orange'.

$ arc_storage_cli stat /orange
- Calling the Bartender's stat method...
- done in 0.41 seconds.
'/orange': found
  states
    checksumType: md5
    neededReplicas: 2
    size: 7
    checksum: 6aefd2842be62cd470709b27aedc7db7
  entry
    owner: /DC=eu/DC=KnowARC/O=knowarc/CN=billy
    GUID: 3fdad88a-944f-79c5-4f23-c5b3d3bc981e
    type: file
  parents
    0/orange: parent
  locations
    https://storage:60000/Shepherd 0b46f53c-419d-ce4e-181a-ec95d16dcc95: alive
  timestamps
    created: 1237519441.96
\end{verbatim}

Currently it has only one replica, because we still don't have more than one storage elements. While \verb!arched! keeps running, let's create a new config for the other machine, which is called \verb!storage2!:

\begin{verbatim}
<?xml version="1.0"?>
<ArcConfig 
  xmlns="http://www.nordugrid.org/schemas/ArcConfig/2007"
  xmlns:tcp="http://www.nordugrid.org/schemas/ArcMCCTCP/2007"
>
    <Server>
        <Pidfile>/var/run/arched2.pid</Pidfile>
        <Logger level="ERROR">/var/log/arched2.log</Logger>
    </Server>
    <ModuleManager>
        <Path>/usr/local/lib/arc/</Path>
    </ModuleManager>
    <Plugins><Name>mcctcp</Name></Plugins>
    <Plugins><Name>mcctls</Name></Plugins>
    <Plugins><Name>mcchttp</Name></Plugins>
    <Plugins><Name>mccsoap</Name></Plugins>
    <Chain>
        <Component name="tcp.service" id="tcp">
            <next id="tls"/> 
            <tcp:Listen>
                <tcp:Port>60006</tcp:Port>
                <tcp:Version>4</tcp:Version>
            </tcp:Listen>
        </Component>
        <Component name="tls.service" id="tls">
            <next id="http"/>
            <KeyPath>/etc/grid-security/hostkey.pem</KeyPath>
            <CertificatePath>/etc/grid-security/hostcert.pem</CertificatePath>
            <CACertificatesDir>/etc/grid-security/certificates</CACertificatesDir>
        </Component>
        <Component name="http.service" id="http">
            <next id="soap">POST</next>
        </Component>
        <Component name="soap.service" id="soap">
            <next id="plexer"/>
        </Component>
        <Plexer name="plexer.service" id="plexer">
            <next id="shepherd">^/Shepherd$</next>
            <next id="byteio">^/byteio/</next>
        </Plexer>
        <Service name="pythonservice" id="shepherd">
            <ClassName>storage.shepherd.shepherd.ShepherdService</ClassName>
            <ServiceID>https://storage2:60006/Shepherd</ServiceID>
            <CheckPeriod>20</CheckPeriod>
            <MinCheckInterval>0.1</MinCheckInterval>
            <CreatingTimeout>600</CreatingTimeout>
            <StoreClass>arcom.store.cachedpicklestore.CachedPickleStore</StoreClass>
            <StoreCfg>
                <DataDir>/var/spool/arc/shepherd_data2</DataDir>
            </StoreCfg>
            <BackendClass>storage.shepherd.byteio.ByteIOBackend</BackendClass>
            <BackendCfg>
                <DataDir>/var/spool/arc/shepherd_store2</DataDir>
                <TransferDir>/var/spool/arc/shepherd_transfer2</TransferDir>
                <TURLPrefix>https://storage2:60006/byteio/</TURLPrefix>
            </BackendCfg>
            <LibrarianURL>https://storage:60000/Librarian</LibrarianURL>
            <BartenderURL>https://storage:60000/Bartender</BartenderURL>
            <ClientSSLConfig FromFile="/etc/arc/clientsslconfig.xml"/>
        </Service>
        <Service name="pythonservice" id="byteio">
            <ClassName>storage.shepherd.byteio.ByteIOService</ClassName>
            <TransferDir>/var/spool/arc/shepherd_transfer2</TransferDir>
            <NotifyURL>https://storage2:60006/Shepherd</NotifyURL>
            <ClientSSLConfig FromFile="/etc/arc/clientsslconfig.xml"/>
        </Service>
    </Chain>
</ArcConfig>    
\end{verbatim}

This configuration contains only two services: a Shepherd, and byteio service. The byteio service is just a very simple storage element service which moves files within SOAP messages. The Shepherd service needs to have a backend for all supported storage element services, so it has a ByteIOBackend configured. This Shepherd connects to our original server's Bartender and Librarian services. (See the technical documentation for details about the services.)

While the original (\verb!storage!) server still runs the services, let's start this new server:

\begin{verbatim}
$ sudo /usr/local/sbin/arched -c /etc/arc/storage_service_2.xml -f
\end{verbatim}

And after a minute or two, let's check our file:

\begin{verbatim}
$ arc_storage_cli stat /orange
- Calling the Bartender's stat method...
- done in 0.41 seconds.
'/orange': found
  states
    checksumType: md5
    neededReplicas: 2
    size: 7
    checksum: 6aefd2842be62cd470709b27aedc7db7
  entry
    owner: /DC=eu/DC=KnowARC/O=knowarc/CN=billy
    GUID: 24b442eb-ab1a-680e-1f79-4c20479d3ea9
    type: file
  parents
    0/orange: parent
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: alive
  timestamps
    created: 1237519828.4
\end{verbatim}

It indeed has two replicas now, one on each server.

Let's download it multiple times:

\begin{verbatim}
$ arc_storage_cli get /orange /tmp/orange1
- Calling the Bartender's getFile method...
- done in 0.50 seconds.
- Got transfer URL: http://storage:60001/hopi/de0d951f-7379-1ce4-0813-48a384c11982
- Downloading from 'http://storage:60001/hopi/de0d951f-7379-1ce4-0813-48a384c11982'
    to '/tmp/orange1' with http...
    0 s:        0.0 kB       0.0 kB/s       0.0 kB/s    . . .       
- done in 1.0060 seconds.
'/orange' (7 bytes) downloaded as '/tmp/orange1'.
    
$ arc_storage_cli get /orange /tmp/orange2
- Calling the Bartender's getFile method...
- done in 0.51 seconds.
- Got transfer URL: https://storage2:60006/byteio/52b6bffa-4812-03a5-93a1-834e38077ac5
- Downloading from 'https://storage2:60006/byteio/52b6bffa-4812-03a5-93a1-834e38077ac5'
    to '/tmp/orange2' with byteio...
- done in 0.2822 seconds.
'/orange' (7 bytes) downloaded as '/tmp/orange2'.
\end{verbatim}

First it was downloaded from the \verb!storage! server via HTTP, second time it was downloaded from the \verb!storage2! server via ByteIO.

Let's create a third storage element on a machine called \verb!storage3! this time using a Hopi service as storage element service like on our main \verb!storage! server:

\begin{verbatim}
zsombor@storage:~$ cat /etc/arc/storage_service_3.xml 
<?xml version="1.0"?>
<ArcConfig 
  xmlns="http://www.nordugrid.org/schemas/ArcConfig/2007"
  xmlns:tcp="http://www.nordugrid.org/schemas/ArcMCCTCP/2007"
>
    <Server>
        <Pidfile>/var/run/arched3.pid</Pidfile>
        <Logger level="ERROR">/var/log/arched3.log</Logger>
    </Server>
    <ModuleManager>
        <Path>/usr/local/lib/arc/</Path>
    </ModuleManager>
    <Plugins><Name>mcctcp</Name></Plugins>
    <Plugins><Name>mcctls</Name></Plugins>
    <Plugins><Name>mcchttp</Name></Plugins>
    <Plugins><Name>mccsoap</Name></Plugins>
    <Chain>
        <Component name="tcp.service" id="tcp">
            <next id="tls"/> 
            <tcp:Listen>
		<tcp:Port>60010</tcp:Port>
		<tcp:Version>4</tcp:Version>
	    </tcp:Listen>
        </Component>
        <Component name="tls.service" id="tls">
            <next id="http"/>
            <KeyPath>/etc/grid-security/hostkey.pem</KeyPath>
            <CertificatePath>/etc/grid-security/hostcert.pem</CertificatePath>
            <CACertificatesDir>/etc/grid-security/certificates</CACertificatesDir>
        </Component>
        <Component name="http.service" id="http">
            <next id="soap">POST</next>
        </Component>
        <Component name="soap.service" id="soap">
            <next id="plexer"/>
        </Component>
        <Plexer name="plexer.service" id="plexer">
            <next id="shepherd">^/Shepherd$</next>
        </Plexer>
        <Service name="pythonservice" id="shepherd">
            <ClassName>storage.shepherd.shepherd.ShepherdService</ClassName>
            <ServiceID>https://storage3:60010/Shepherd</ServiceID>
            <CheckPeriod>20</CheckPeriod>
            <MinCheckInterval>0.1</MinCheckInterval>
            <CreatingTimeout>600</CreatingTimeout>
            <StoreClass>arcom.store.cachedpicklestore.CachedPickleStore</StoreClass>
            <StoreCfg><DataDir>/var/spool/arc/shepherd_data3</DataDir></StoreCfg>
            <BackendClass>storage.shepherd.hardlinkingbackend.HopiBackend</BackendClass>
            <BackendCfg>
                <DataDir>/var/spool/arc/shepherd_store3</DataDir>
                <TransferDir>/var/spool/arc/shepherd_transfer3</TransferDir>
                <TURLPrefix>http://storage3:60011/hopi/</TURLPrefix>
            </BackendCfg>
            <LibrarianURL>https://storage:60000/Librarian</LibrarianURL>
            <BartenderURL>https://storage:60000/Bartender</BartenderURL>
            <ClientSSLConfig FromFile="/etc/arc/clientsslconfig.xml" />
        </Service>
    </Chain>
    <Chain>
        <Component name="tcp.service" id="tcp2">
            <next id="http2"/> 
            <tcp:Listen>
		<tcp:Port>60011</tcp:Port>
		<tcp:Version>4</tcp:Version>
	    </tcp:Listen>
        </Component>
        <Component name="http.service" id="http2">
            <next id="plexer2">GET</next>
            <next id="plexer2">PUT</next>
        </Component>
        <Plexer name="plexer.service" id="plexer2">
            <next id="hopi">^/hopi/</next>
        </Plexer>
        <Service name="hopi" id="hopi">
            <DocumentRoot>/var/spool/arc/shepherd_transfer3</DocumentRoot>
            <SlaveMode>1</SlaveMode>
        </Service>
    </Chain>
</ArcConfig>    
\end{verbatim}

This configuration has two separate ports, one with TLS security and a Shepherd service, and the other without TLS security and with the Hopi service. Let's run this server as well:

\begin{verbatim}
$ sudo /usr/local/sbin/arched -c /etc/arc/storage_service_3.xml -f
\end{verbatim}

Now when checking our file again there should be no change at all:

\begin{verbatim}
$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: alive
\end{verbatim}

Let's stop our server on \verb!storage2!, and watch what happens of our file:

\begin{verbatim}
$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: offline
    
$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: offline
    https://storage3:60010/Shepherd 421aa939-81fa-5cd9-7c2f-2d5f99a98be1: creating

$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: offline
    https://storage3:60010/Shepherd 421aa939-81fa-5cd9-7c2f-2d5f99a98be1: alive
\end{verbatim}

A new replica have been created on the third server to maintain the needed number of 2.

Let's start the services on \verb!storage2! again, and let's see what is happening with our file:

\begin{verbatim}
$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: alive
    https://storage3:60010/Shepherd 421aa939-81fa-5cd9-7c2f-2d5f99a98be1: alive

$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage2:60006/Shepherd c1334c09-5d3d-983b-2081-87fe60d4764a: thirdwheel
    https://storage3:60010/Shepherd 421aa939-81fa-5cd9-7c2f-2d5f99a98be1: alive

$ arc_storage_cli stat /orange
  locations
    https://storage:60000/Shepherd 315d59d3-50cd-1a10-02e6-8e6a528473c3: alive
    https://storage3:60010/Shepherd 421aa939-81fa-5cd9-7c2f-2d5f99a98be1: alive
\end{verbatim}

One of the replicas have been disappeared to maintain the needed number of 2.

\section{Make the A-Hash service replicated} % (fold)
\label{sec:make_the_a_hash_service_replicated}

Now we have a deployement, where one machine has the A-Hash, Librarian and Bartender services, and all three machines have a Shepherd and a storage element service. The A-Hash service stores all the metadata of all the files and collections, so it is critical. If the first machine goes offline then the whole system dies. We can deploy the A-Hash service on all the machines and still can have one consistent metadata database. The replicated version of the A-Hash uses the Berkeley DB to provide a distributed database.

Let's install Berkeley DB as root (but you can use the \verb!--prefix! option of \verb!configure! to install it somewhere else than the default location).

Get the latest Berkeley DB and its patches:
\begin{verbatim}
$ wget http://download.oracle.com/berkeley-db/db-4.7.25.tar.gz
$ wget http://www.oracle.com/technology/products/berkeley-db/db/update/4.7.25/patch.4.7.25.1
$ wget http://www.oracle.com/technology/products/berkeley-db/db/update/4.7.25/patch.4.7.25.2
$ wget http://www.oracle.com/technology/products/berkeley-db/db/update/4.7.25/patch.4.7.25.3
\end{verbatim}

Extract it and apply the patches:
\begin{verbatim}
$ tar zxf db-4.7.25.tar.gz
$ cd db-4.7.25
$ patch -p 0 < ../patch.4.7.25.1
$ patch -p 0 < ../patch.4.7.25.2
$ patch -p 0 < ../patch.4.7.25.3
\end{verbatim}

Configure, make and install it (you can use the \verb!--prefix! option for \verb!configure!)
\begin{verbatim}
$ cd build_unix/
$ ../dist/configure
\end{verbatim}

Now compile and install it:
\begin{verbatim}
$ make
$ sudo make install
\end{verbatim}

Now we need the python wrapper for the Berkeley DB:

\begin{verbatim}
$ wget http://pypi.python.org/packages/source/b/bsddb3/bsddb3-4.7.5.tar.gz
\end{verbatim}

Let's install it to the default location:

\begin{verbatim}
$ tar zxf bsddb3-4.7.5.tar.gz
$ cd bsddb3-4.7.5
$ sudo python setup.py install
\end{verbatim}

If you've installed Berkeley DB somewhere else, or want to install the python wrapper to somewhere else than the default:

\begin{verbatim}
$ BERKELEYDB_DIR=/path/to/berkeleydb python setup.py install --prefix=/where/to/install
\end{verbatim}

Let's test the installation with the \verb!test.py! included in the directory of the python wrapper for the Berkeley DB:
\begin{verbatim}
$ sudo python test.py
[...]
Ran 383 tests in 29.858s

OK
\end{verbatim}



\section{Using the FUSE module} % (fold)
\label{sec:using_the_fuse_module}

Besides the \verb!arc_storage_cli! tool we can access the storage system with the use of a FUSE module. We can install the ARC storage FUSE module following these steps:

\begin{itemize}
\item Install ARC1 storage system.

\item Install \verb!fuse >= 2.7.3! and \verb!fuse-python >= 0.2!:
\begin{verbatim}
$ sudo aptitude install fuse-python
\end{verbatim}

\item Set up \verb!~/.arc/client.xml!:

\begin{verbatim}
$ cat ~/.arc/client.xml 
<ArcConfig>
  <KeyPath>/home/<username>/.arc/userkey.pem</KeyPath>
  <CertificatePath>/home/<username>/.arc/usercert.pem</CertificatePath>
  <CACertificatesDir>/etc/grid-security/certificates</CACertificatesDir>
  <BartenderURL>https://localhost:60000/Bartender</BartenderURL>
</ArcConfig>
\end{verbatim}

\item Create a mountpoint and mount arcfs.py:
\begin{verbatim}
   $ pwd
   /home/me/arc
   $ ln -s arc1/src/services/storage/fuse/arcfs.py arcfs.py
   $ mkdir mnt
   $ python arcfs.py ./mnt 
\end{verbatim}
This mounts ARCFS in \verb!/home/me/arc/mnt!, creates directory
\verb!/home/me/arc/fuse_transfer! and a log file
\verb!/home/me/arc/arcfsmessages!.

\item Make a collection:
\begin{verbatim}   
   $ mkdir -p mnt/home/me
   $ stat mnt/home/me
     File: `mnt/home/me'
     Size: 0               Blocks: 0          IO Block: 4096   directory
   Device: 19h/25d Inode: 3           Links: 2
   Access: (0755/drwxr-xr-x)  Uid: (  500/  me)   Gid: (  100/   users)
   Access: 1970-01-01 01:00:00.000000000 +0100
   Modify: 1970-01-01 01:00:00.000000000 +0100
   Change: 1970-01-01 01:00:00.000000000 +0100
\end{verbatim}
Note that so far there is no security handling implemented, so
that all files have mode 0644, all collections have mode 0755,
everything is owned by the user that mounts the system.

\item Create some entries in your collection:
\begin{verbatim}
   $ cd mnt/home/me
   $ emacs -nw fish
   catfish
   $ cat fish
   catfish
   $ cp fish fisk2
   $ ls -la
   drwxr-xr-x 5 me users  0 1970-01-01 01:00 .
   drwxr-xr-x 3 me users  0 1970-01-01 01:00 ..
   -rw-r--r-- 1 me users  8 1970-01-01 01:00 fish
   -rw-r--r-- 1 me users 28 1970-01-01 01:00 fish~
   -rw-r--r-- 1 me users  8 1970-01-01 01:00 fish2
\end{verbatim}

\item You probably want to remove that anoying emacs backup file
\begin{verbatim}   
   $ rm fish~
   $ ls 
   fish  fish2
\end{verbatim}

\item Maybe you want fish2 in a separate collection
\begin{verbatim}   
   $ mkdir sea_creatures
   $ mv fish2 sea_creatures
\end{verbatim}

\item Maybe you're not happy with the collection name
\begin{verbatim}
   $ mv sea_creatures creatures
   $ ls creatures
   fish2
\end{verbatim}

\item Moving the collection out of ARC storage:
\begin{verbatim}
   $ cd ../../..
   $ mv mnt/home/me/creatures .
\end{verbatim}

\item The collection is no longer in the ARC storage:
\begin{verbatim}
   $ find mnt/home/me/
   mnt/home/me/
   mnt/home/me/fish
\end{verbatim}

\item Unmounting ARCFS; To unmount, refer to fuse documentation. Usually you can do
\begin{verbatim}   
   $ fusermount -u mnt
\end{verbatim}

If this for some reason this doesn't work, you can do
\begin{verbatim}   
   $ sudo umount -f mnt
\end{verbatim}   

or even
\begin{verbatim}   
   $ sudo umount -l mnt
\end{verbatim}   

Now, remounting the system, everything should still be there:
\begin{verbatim}
   $ python arcfs.py ./mnt

   $ find mnt/home/me/
   mnt/home/me/
   mnt/home/me/fish

   $ fusermount -u mnt
\end{verbatim}
\end{itemize}

\end{document}

\documentclass{book}
\usepackage{graphicx}                              %for PNG images (pdflatex)
\usepackage[linkbordercolor={1.0 1.0 0.0}]{hyperref} %for \url tag
\usepackage{color}                                 %for defining custom colors
\usepackage{framed}                                %for shaded and framed paragraphs
\usepackage{textcomp}                              %for various symbols, e.g. Registered Mark
\usepackage{geometry}                              %for defining page size
\usepackage{longtable}                             %for breaking tables
%
\geometry{verbose,a4paper,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2CM}
\hypersetup{
  pdfauthor = {Zsombor Nagy},
  pdftitle = {Documentation of the ARC storage system},
  pdfsubject = {Paper subject},
  pdfkeywords = {Paper,keyword,comma-separated},
  pdfcreator = {PDFLaTeX with hyperref package},
  pdfproducer = {PDFLaTeX}
}
%
\bibliographystyle{IEEEtran}                       %a nice bibliography style
%
\def\efill{\hfill\nopagebreak}%
\hyphenation{Nordu-Grid}
\setlength{\parindent}{0cm}
\setlength{\FrameRule}{1pt}
\setlength{\FrameSep}{8pt}
\addtolength{\parskip}{5pt}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\arraystretch}{1.3}
\newcommand{\dothis}{\colorbox{shadecolor}}
\newcommand{\ngdl}{\url{http://ftp.nordugrid.org/download}~}
\definecolor{shadecolor}{rgb}{1,1,0.6}
\definecolor{salmon}{rgb}{1,0.9,1}
\definecolor{bordeaux}{rgb}{0.75,0.,0.}
\definecolor{cyan}{rgb}{0,1,1}
%
%----- DON'T CHANGE HEADER MATTER
\hyphenation{preserve-Original}
\begin{document}
\def\today{\number\day/\number\month/\number\year}

\begin{titlepage}

\begin{tabular}{rl}
\resizebox*{3cm}{!}{\includegraphics{ng-logo.png}}
&\parbox[b]{2cm}{\textbf \it {\hspace*{-1.5cm}NORDUGRID\vspace*{0.5cm}}}
\end{tabular}

\hrulefill

%-------- Change this to NORDUGRID-XXXXXXX-NN

{\raggedleft NORDUGRID-TECH-17\par}

{\raggedleft \today\par}

\vspace*{2cm}

%%%%---- The title ----
{\centering \textsc{\Large Documentation of the ARC storage system}\Large \par}
\vspace*{0.5cm}
    
%%%%---- A subtitle, if necessary ----
{\centering \textit{\large First prototype status and plans}\large \par}
    
\vspace*{1.5cm}
%%%%---- A list of authors ----
    {\centering \large Zsombor Nagy\footnote{zsombor@niif.hu} \large \par}
    {\centering \large Jon Nilsen\footnote{j.k.nilsen@usit.uio.no} \large \par}
    {\centering \large Salman Zubair Toor \footnote{salman.toor@it.uu.se} \large \par}
\end{titlepage}

\tableofcontents                          %Comment if use article style
\newpage

\renewcommand{\thefootnote}{\arabic{footnote}}


\chapter{Design Overview} % (fold)
\label{cha:overview}


The ARC storage system is a distributed system for storing replicated \emph{file}s on several file storage nodes and manage them in a global namespace.  The files can be grouped into \emph{collection}s (a concept very similar to directories in the common file systems), and a collection can contain sub-collections and sub-sub-collections in any depth. There is a dedicated \emph{root collection} to gather all collections to the global namespace. This hierarchy of collections and files can be referenced using \emph{Logical Name}s (\emph{LN}s). The users can use this global namespace as they were using a local filesystem. Files can be transfered by multiple transfer protocols, and the client side tools hide this from the user. The replicas of the files are stored on different storage nodes. A storage node here is a network-accessible computer having storage space to share, and a storage element service running (e.g.~HTTP(S), FTP(S), GridFTP, ByteIO\footnote{OGSA ByteIO Working Group (BYTEIO-WG), \url{https://forge.gridforum.org/projects/byteio-wg/}}, etc.). For each storage node one of the services of the ARC storage system is needed to manage it and to integrate it into the system. There is a way on the client side to access third-party storage solutions through the namespace of the ARC storage system. The main services of the storage system are the following (see Figure~\ref{fig:services}):
\begin{itemize}
    \item the \textbf{A-Hash} service, which is a replicated database which is used by the Librarian to store metadata;
    \item the \textbf{Librarian} service, which handles the metadata and hierarchy of collections and files, the location of replicas, and health data of the Shepherd services, using the A-Hash as database;
    \item the \textbf{Bartender} service, which provides a high-level interface for the users and for other services;
    \item the \textbf{Shepherd} service, which manages storage element services, and provides a simple interface for storing files on storage nodes.
\end{itemize}

\begin{figure}[ht]
\centering{{\scalebox{0.9}{\includegraphics{arc-storage-services.pdf}}}
\caption{\label{fig:services}The components of the ARC storage: the \textbf{A-H}ash service, the \textbf{L}ibrarian service, the \textbf{B}artender service and the \textbf{S}hepherd service.}}
\end{figure}

\section{Files and collections} % (fold)
\label{sec:files_and_collections}

The storage system is capable of storing files which can be grouped in collections and sub-collections, etc.
Every file and collection has a unique ID in the system called the \emph{GUID}. Compared to the well-known structure of local file systems, these GUIDs are very similar to the concept of \emph{inode}s. And as a directory on a local filesystem is basically just a list of name and inode pairs, a collection on the ARC storage is just a list of name and GUID pairs. There is a dedicated collection which is the \emph{root collection}. This makes the namespace of the ARC storage system a hierarchical namespace where you can start at the root collection, and go to sub-collections and sub-sub-collections to get to a file. This path is called the \emph{Logical Name} (LN). For example if there is a sub-collection called \verb!saturn! in the root collection, and there is a file called \verb!rings! in this sub-collection, then the LN of this file is \verb!/saturn/rings!.

Besides the Logical Names we can refer to a file or collection by simply its GUID, or we can use GUIDs and Logical Names together, as seen on Figure~\ref{fig:namespace}.

The full syntax of Logical Names is \verb#/[path]# or \verb#<GUID>[/<path>]# where [...] indicates optional parts.

\begin{figure}[ht]
\centering{{\scalebox{0.7}{\includegraphics{arc-storage-namespace.pdf}}}
\caption{\label{fig:namespace}Example of the hierarchy of the global namespace} }
\end{figure}

Example on Figure~\ref{fig:namespace}: if we have a collection with GUID \verb#1234#, and there is a collection called \verb#green# in it, and in \verb#green# there is another collection called \verb#orange#, and in \verb#orange# there is a file called \verb#huge#, then we can refer to this file with the Logical Name \verb#1234/green/orange/huge#, which means that from the collection called \verb!1234! we have to follow along the path: \verb!green!, \verb!orange!, \verb!huge!.

There is a dedicated root collection (which has the GUID \verb#0#), and if a LN starts with no GUID prefix, it is implicitly prefixed with the GUID of this well-known root collection, e.g.~\verb#/why/blue# means \verb#0/why/blue#. If a user wants to find the file called \verb#/why/blue#, the system knows where to start the search: the GUID of the root collection. The root collection knows the GUID of \verb#why#, and the (sub-)collection \verb#why# knows the GUID of \verb#blue#. If the GUID of this file is \verb#5678#, and somebody makes another entry in collection \verb#/why# (= \verb#0/why#) with name \verb#red# and GUID \verb#5678#, then the \verb#/why/red# LN points to the same file as \verb#/why/blue#, which concept is very similar to a hardlink in a regular local file system.

% section files_and_collections (end)

\section{Storage nodes and replicas} % (fold)
\label{sec:storage_nodes_and_replicas}

The collections in the ARC storage are logical entities, the content of a collection is stored as metadata of the collection, which means the a collection actually has no physical data. A file however has both metadata and real physical data (the actual bytes of the file). The metadata of a file is stored in the same database where the collections are stored, but the physical data of a file is stored on storage nodes as multiple replicated copies.

A storage node consists of two things: a storage element service which is capable of storing and serving files through a specific protocol (e.g. a web server, an FTP server, a GridFTP server, etc.) and a Shepherd service which provides a simple interface to access the storage node, and which can initiate and manage file transfers through the storage element service. The Shepherd has different backends for the supported storage element services which made it possible the communicate with them.

So we have logical files, which are part of the hierarchical namespace and have a GUID and other metadata, and a logical file has one or more physical replicas. The physical replicas stored on separate storage nodes. In order to connect the logical file to its replicas, we need to have some pointers. Each storage node has a URL and each replica has a unique ID within the storage node called \emph{referenceID}, the URL and the referenceID together is called a \emph{Location}, a Location unambiguously points to one specific replica. So to connect the logical files to the physical ones, each logical file has a list of Locations.

The user can specify for each file how many replicas are needed. The ARC storage system periodically checks the number of replicas, and automatically creates new replicas if there are fewer then needed, and removes replicas if there are more. 

The different replicas of a file could be in different states, e.g. the replica could be valid and alive or just in the process of creation or it could be corrupt or a whole storage node could be offline. This state is always stored as metadata next to the Location of the given replica. For each file there is a checksum calculated, and this checksum is used to detect if a replica gets corrupted. If a storage node (more precisely: the Shepherd service on the storage node) detects that a file is invalid, it reports this so the metadata will be in sync with the real state. And the storage nodes sends heartbeat messages periodically, and if a storage node goes offline, the missing heartbeat triggers the modification of metadata as well. 

% section storage_nodes_and_replicas (end)

\section{The A-Hash} % (fold)
\label{sec:the_a_hash}

The A-Hash is the replicated\footnote{Depends on system configuration. It is possible to configure the storage system with only one central A-Hash too} metadata store of the storage system. It consistently stores `objects' which contain property-value pairs organized in sections. All metadata about files and collections in the storage system are stored in the A-Hash. Additionall system configutation information (e.g. about A-Hash replication, about Shepherd services, etc.) is stored in it as well. The A-Hash itself does not interpret any data.

% section the_a_hash (end)

\section{The Librarians} % (fold)
\label{sec:the_librarians}

The Librarian manages the storage system hierarchy of files and collections including their metadata. It can traverse Logical Names and return the corresponding metadata. The Shephards send their heartbeat messages to the Librarians which if needed respond by modifying the states of files stored on a certain storage node. The Librarian itself is a stateless service, it uses the A-Hash to store and retrieve the metadata and thus there can be any number of independent Librarian services (all using the same A-Hashes). This ensures high-availability and load-balancing of the storage system.

% section the_librarians (end)

\section{The Bartenders} % (fold)
\label{sec:the_bartenders}

The Bartender service provides the high-level interface of the storage system to the clients. Every interaction between a client and the ARC storage system starts by sending a request to a Bartender. Using the Bartender it is possible to create and remove collections, create, get and remove files, move files and collections within the namespace using Logical Names, etc. The Bartender authorizes users and apply access policies of files and collections. It communicates with the Librarian and Shepherd services to accomplish the client’s requests. The file data itself does however not go through the Bartender; file transfers are directly performed between the storage nodes and the clients. There could be any number of independent Bartender services in the system which ensure high-availability and load-balancing. The Bartender also provides a way to access files on third-party storage solutions through its interface by mounting the namespace of the third-party storage into the namespace of the ARC storage (this is accomplished by so called `gateway' modules). 

% section the_bartenders (end)

\section{The Shepherds} % (fold)
\label{sec:the_shepherds}

The Shepherd service manages the storage element service on the storage node. It reports its health state to a Librarian and provides the interface for initiating file transfers. For each kind of storage element service (e.g. a HTTP server, an FTP server, a storage solution with a GridFTP interface, etc.) a Shepherd backend is needed which is capable of managing the given storage element service. The Shepherd service periodically checks the health of the replicas using their checksums, and if a replica is deleted or corrupted, the Shepherd tries to recover it by downloading a valid copy from another storage node. The Shepherd also periodicallys check if a file has the correct number of replicas in the system. If too few or too many replicas are found, new replicas will be created or redundant replicas removed.

% section the_shepherds (end)

\section{Security} % (fold)
\label{sec:security}

The ARC storage system consists of several services (both different kinds of services and multiple instances of the same kind). The A-Hash, Librarian and Shepherd services are `internal' services in the sense that the end-user of the storage system never communicates with them directly. But these internal services are communicating with each other, thus they have to know which other services to trust. We call this aspect of the security architecture of the ARC storage `inter-service authorization'.

The end users always connect to one of the Bartender services, which will decide which permission the user has. This is the `high-level authorization' part of the security architecture.

For transferring the actual file data, the users have to connect to storage element services running on storage nodes. These services also have their own authentication and authorization methods. Managing these aspects is the `transfer-level authorization' part of the security architecture of the ARC storage.

\subsection{Inter-service authorization} % (fold)
\label{sub:inter_service_authorization}

In a deployment of the ARC storage system it is possible to have several A-Hash, Librarian, Shepherd and Bartender services. The Bartenders send requests to the Librarians and the Shepherds, the Shepherds communicate with the Librarians, the Librarians talk with the A-Hashes. If any of these services gets compromised or a new rouge service is inserted in the system, security is lost. That's why it is vital for each service to authorize other services before sending or accepting requests. The services communicate via the HTTPS protocol, which means that they should provide an X.509 certificate for each connection, and that they can examine eachother's certificates. Because of these X.509 certificates each service has Distinguish Name (DN), which can be used for identifying trusted services. A list of trusted DNs can be configured into each service, or a list of trusted services can be stored on a remote location. The services will only accept connections if the DN of the other end is listed in this list of trusted DNs. However the Bartender services will accept any incoming connection, which are from the users, because the users are authenticated differently.

% subsection inter_service_authorization (end)

\subsection{High-level authorization} % (fold)
\label{sub:high_level_authorization}

The Librarian component of the ARC storage system stores all the metadata about files and collections. For each file and collection there are access policies in the form of access control rules, and these are stored among the metadata. The users are identified by their DNs, and an access control rule specify the rights of the given user. A rule can be represented like this:

\begin{verbatim}
    DN +action +action -action
\end{verbatim}

This contains a list of actions, each prefixed with a \verb!+! or \verb!-! character which indicates that the given action is allowed or not allowed for the given DN.

Besides specifying only one user with a DN, there are other types of access control rules: we can have a rule for a whole VO (Virtual Organization) or for ALL users, like this:

\begin{verbatim}
    ALL +action
    VOMS:knowarc.eu +action -action 
\end{verbatim}

These are the actions which can be used for access control:
\begin{itemize}
    \item \emph{read}: user can get the list of entries in the collection; user can download the file
    \item \emph{addEntry}: user can add a new entry to the collection;
    \item \emph{removeEntry}: user can remove any entry from the collection 
    \item \emph{delete}: user can delete the collection if it is empty; user can delete a file
    \item \emph{modifyPolicy}: user can modify the policy of the file/collection
    \item \emph{modifyStates}: user can modify some special metadata of the file/collection (close the collection, change the number of needed replica of the file)
    \item \emph{modifyMetadata}: user can modify the arbitrary metadata section of the file/collection (these are property-value pairs)
\end{itemize}

Additionally, each file and collection has an `owner' which is a user who always can modify the access control rules.

% subsection high_level_authorization (end)


\subsection{Transfer-level authorization} % (fold)
\label{sub:transfer_level_authorization}

Currently the transfer-level authorization is kept very simple. When the Bartender decides that a user has permission to download a file, then the Bartender chooses a replica, and initiates the transfer. The result of this initiation is a URL which is called the transfer URL (TURL). This TURL is unique for each request, even for request to the same replica, and this TURL is only valid for one download. Currently we configure the storage element services to not do any authorization, and we use these one-time URLs to ensure that only the authorized users can access the contents of the storage elements. 

% subsection transfer_level_authorization (end)

% section security (end)

% chapter design_overview (end)

\chapter{Use cases} % (fold)
\label{cha:use_cases}

\section{Listing the contents of a collection} % (fold)
\label{sec:listing_the_contents_of_a_collection}
\begin{figure}[ht]
\centering{{\scalebox{0.6}{\includegraphics{arc-storage-listing.pdf}}}
\caption{\label{fig:listing}Listing the contents of a collection} }
\end{figure}

We want to list the contents of a collection, which has a Logical Name of \verb!/ourvo/users/we!. In order to do this, we have to contact a Bartender, and send a request to it containing the Logical Name we want to list, and the response from the Bartender will contain the list of entries. The steps are represented on Figure~\ref{fig:listing}.

\begin{enumerate}
    \item We need to know the URL of a Bartender. This could be preconfigured on the client side or in future releases it could be acquired from an information system. When we have the URL, we send a `list' request which contains the Logical Name \verb!/ourvo/users/we!.
    \item The Bartender tries to get the metadata of the given LN by sending a `traverseLN' request to a Librarian.
    \item The Librarian service starts the traversing by asking an A-Hash service about the first part of the LN, which is the \verb!/! root collection. The A-Hash service only knows about GUIDs and not about LNs, but the GUID of the root collection is well-known, so the A-Hash can return the metadata of it which contains the list of files and sub-collections in the root collection.
    \item Hopefully the \verb!ourvo! collection can be found in the root collection, which means that new the Librarian knows its GUID, and can ask for its metadata from the A-Hash.
    \item After the A-Hash returns the metadata of the \verb!/ourvo! collection, the Librarian finds the GUID of \verb!users! in it, then gets its metadata.
    \item The A-Hash returns the metadata of \verb!/ourvo/users! which contains the GUID of \verb!we!, so the Librarian can ask for its metadata.
    \item At last the A-Hash returns the metadata of \verb!/ourvo/users/we! to the Librarian, and the Librarian returns it to the Bartender. This metadata contains the list of entries within this collection, and it also contains the access policies for this collection.
    \item The Bartender first checks if based on our DN (or our VO membership) and the access policies of this collection do we have rights to get the contents of this collection or not. If we are approved, then because the `list' request should return additional metadata about each entry in the collection, the Bartender send a `get' message to a Librarian requesting metadata of all the entries in this collection.
    \item The Librarian gets the data from the A-Hash and returns it to the Bartender.
    \item Now the Bartender has all the needed information, so it could return the proper response to us, the user. Our client tool formats and prints the results nicely.
\end{enumerate}

% section listing_the_contents_of_a_collection (end)

\section{Downloading a file} % (fold)
\label{sec:downloading_a_file}
\begin{figure}[ht]
\centering{{\scalebox{0.7}{\includegraphics{arc-storage-downloading.pdf}}}
\caption{\label{fig:downloading}Downloading a file} }
\end{figure}

In this use case we want to download a file which has a Logical Name of \verb#/ourvo/users/we/thefilewewant# (see Figure~\ref{fig:downloading}).

\begin{enumerate}
    \item We connect a Bartender and send a `getFile' request with the LN of our file.
    \item The Bartender contacts a Librarian to traverse the Logical Name and to get the metadata of our file.
    \item The Librarian do the step by step traversing and gets all the data from the A-Hash and returns the metadata of our file to the Bartender. This metadata contains the location of the file's replicas and the access policies of this file.
    \item The Bartender checks based on the access policies and our identity if we are allowed to get the file, and if we are good to go, then it chooses a replica location. A location consists of the URL of a Shepherd service, and the ID of the replica within that Shepherd (which is called a `referenceID'). The Bartender sends a `get' request to the chosen Shepherd.
    \item The Shepherd prepares the file transfer by asking the storage element service to create a new one-time URL for this replica. This URL called the transfer URL, and it will only be valid for one download. The Shepherd returns the TURL to the Bartender.
    \item The Bartender returns the TURL to us.
    \item Now we use this TURL to get the file directly from the storage element service on the storage node.
\end{enumerate}

% section downloading_a_file (end)

\section{Creating a collection} % (fold)
\label{sec:creating_a_collection}
\begin{figure}[ht]
\centering{{\scalebox{0.7}{\includegraphics{arc-storage-makecollection.pdf}}}
\caption{\label{fig:makecollection}Creating a collection} }
\end{figure}

We want to create a new (empty) collection as sub-collection of \verb#/ourvo/common#, and we want to call it \verb!docs! (see Figure~\ref{fig:makecollection})

\begin{enumerate}
    \item We contact a Bartender and send a `makeCollection' request with the LN \verb!/ourvo/common/docs!.
    \item The Bartender asks a Librarian to traverse this LN.
    \item The Librarian try to traverse the Logical Name and it stops at the last possible point and returns the metadata of the last element. Because we want to put our collection to a new path but into an existing collection, we expect that only the \verb#/ourvo/common# part of the LN can be traversed. If the Librarian could traverse the whole LN that would mean that there is already an existing file or collection by that name. If the parent collection does exist then the Librarian can traverse it which means that the parent collection's metadata is returned to the Bartender. 
    \item The Bartender checks the access policies to decide if we have permissions to put something into this collection. Then it asks the Librarian to create a new collection.
    \item The Librarian creates the collection, and returns its GUID. At this point this new collection has no real Logical Name yet, it only has a GUID, but it is not yet put into its parent collection.
    \item The Bartender now asks the Librarian to add this new entry into the parent collection, which means that the new GUID and the name \verb!docs! are added as a pair.
    \item The Librarian returns with a status message.
    \item Finally the Bartender tells us if everything went OK or not.
\end{enumerate}


% section creating_a_collection (end)

\section{Uploading a file} % (fold)
\label{sec:uploading_a_file}

\begin{figure}[ht]
\centering{{\scalebox{0.7}{\includegraphics{arc-storage-uploading.pdf}}}
\caption{\label{fig:uploading}Uploading a file} }
\end{figure}

We have a file on our local disk we want to upload to a collection called \verb#/ourvo/common/docs#. (See Figure~\ref{fig:uploading}.)
\begin{enumerate}
    \item We contact a Bartender to put the file, we give the size and checksum and other metadata. And of course we give the Logical Name where we want to put the file, which in this case will be \verb#/ourvo/common/docs/proposal.pdf#
    \item The Bartender ask a Librarian to traverse this LN.
    \item The Librarian traverses the Logical Name, and we expect it to stop at the \verb#/ourvo/common/docs# part of the LN, because that means that the name is available and the parent collection exists. If everything is fine, the metadata of the parent collection is returned to the Bartender. 
    \item The Bartender checks the access policies to decide if we have permissions to put something into this collection. Then it asks the Librarian to create a new file entry. 
    \item The Librarian creates the entry and returns its GUID.
    \item Then the Bartender add the name \verb#proposal.pdf# and the new GUID to the collection \verb#/ourvo/common/docs# and from now on there will be a valid LN \verb#/ourvo/common/docs/proposal.pdf#. However this LN points to a file which has currently no replica at all. If someone tried to download the file called \verb#/ourvo/common/docs/proposal.pdf# now, would get an error message.
    \item The Bartender asks the Librarian about Shepherd services (which are sitting on storage nodes).
    \item The Librarian returns a list of Shepherd services
    \item The Bartender chooses a Shepherd service and sends it a `put' request to initiate the file upload. 
    \item The Shepherd communicates with the storage element service on the same node to create a new transfer URL (TURL). Then it creates a `referenceID' for this file and then reports to the Librarian that there is a new replica in \verb#creating# state. The Librarian gets the message from the Shepherd and adds the new replica to our new file. Now the file has one replica, which is not uploaded yet into the system. If someone tries to download this file now, still gets an error message.
    \item The Shepherd returns the the TURL to the Bartender.
    \item The Bartender returns the TURL to us.
    \item Then we can upload the file to this TURL.
    \item The Shepherd detects that the file is arrived. It checks the checksum of the file, and if it is OK, then it reports to the Librarian, that this replica is now \verb#alive#. The Librarian alters the state of this location, and now finally the file has one valid replica. 
\end{enumerate}
í
% section uploading_a_file (end)

\section{Removing a file} % (fold)
\label{sec:removing_a_file}

\begin{figure}[ht]
\centering{{\scalebox{0.7}{\includegraphics{arc-storage-removing.pdf}}}
\caption{\label{fig:removing}Removing a file} }
\end{figure}
\begin{enumerate}
    \item If we want to remove a file, we should connect to a Bartender with the LN of the file we want to remove.
    \item The Bartender asks the Librarian to traverse the LN.
    \item The Librarian returns the metadata of the file. The metadata contains information about all the hardlinks which points to this file if there are more than one.
    \item Now the Bartender asks the Librarian to remove the file.
    \item After that, the Bartender asks the Librarian to remove the links to this file from all the parent collections.
    \item Next time a Shepherd which has a replica of this file does its periodic check, it asks the Librarian about the file, and notices that the file does not exist anymore, so it removes the replica itself from the storage node.
\end{enumerate}


% section removing_a_file (end)

% chapter use_cases (end)

\chapter{Technical description} % (fold)
\label{cha:technical_description}

\section{Framework and language} % (fold)
\label{sec:framework_and_language}

The services are written in Python and running in the HED\footnote{The ARC container - \url{https://www.knowarc.eu/documents/Knowarc\_D1.2-2\_07.pdf}} hosting environment. The HED itself is written in C++, but there are language bindings which allow us to write services in other languages, e.g.~in Python or Java. The source code of the storage services are in the NorduGrid Subversion repository\footnote{\url{http://svn.nordugrid.org/trac/nordugrid/browser/arc1/trunk/src/services/storage}}.

Because the next-generation information system of ARC is currently under development, we cannot use it to discover services, that's why currently the URLs of almost all the services are hard-coded in the configuration files of the service. There is one exception: the Shepherd services are reporting their URLs to the Librarians, so a Bartender always could get a very current list of alive Shepherd services.

The HED also has a security framework which the Bartenders use the make access policy decision . The details of this is described in section X.

% section framework_and_language (end)

\section{Data model} % (fold)
\label{sec:data_model}

The ARC storage system stores different kinds of metadata about files, collections, mount point, Shepherd services, etc. Each of these has a unique ID which we call `GUID'.

The A-Hash services provide a functionality to store `objects', where each object has a unique ID, and contains property-value pairs organized in sections. The properties, the values and the section names too are simple character strings. The A-Hash services provide a simple interface to interact with these objects, e.g.~to do conditional and atomic changes.

The Librarian services use the A-Hash services to store the metadata of all kinds, using the GUIDs as unique IDs. For this, we have to represent the metadata as property-value pairs organized in section. The files, the collections and the mount points have some common attributes: the GUID, the type and the owner which are in the \emph{entry} section; the creation and modification timestamps in the \emph{timestamps} section, a list of access control rules in the \emph{policy} section; the list of parent collections where this entry has a `hardlink'; and each entry could have arbitrary property-value pairs in the \emph{metadata} section.

\begin{description}
    \item [entry] section 
    \begin{itemize}
        \item \emph{type}: the type of the entry: `collection', `file' or `mountpoint'
        \item \emph{GUID}: the unique ID of the entry
        \item \emph{owner}: the DN of the user how owns this entry
    \end{itemize}
    \item [timestamps] section 
    \begin{itemize}
        \item \emph{created}: timestamp of creation 
        \item \emph{modified}: timestamp of last modification (the support for this is not implemented yet)
    \end{itemize}
    \item [policy] section 
    \begin{itemize}
        \item \emph{(identity, action list) pairs}: a list of allowed and not allowed actions for different identities (users, VOs, etc.). 
    \end{itemize}
    \item [parents] section
    \begin{itemize}
        \item list of GUIDs of collection where this entry is located, and the name of this entry within the given collections
    \end{itemize}
    \item [metadata] section 
    \begin{itemize}
        \item any other arbitrary property-value pairs
    \end{itemize}
\end{description}

\subsection{Files} % (fold)
\label{sub:files}

A file in the ARC storage has a logical entry in the namespace, and a couple of physical replicas on the storage nodes. We store the locations of the replicas in the \emph{location} section. A location contains the URL of the Shepherd service, the local ID of the file within the storage node and the state of the replica. This state could be `\textbf{alive}' (if the replica passed the checksum test, and the Shepherd reports that the storage node is healthy), `\textbf{invalid}' (if the replica has wrong checksum, or the Shepherd claims it has no such file), `\textbf{offline}' (if the Shepherd is not reachable, but may have a valid replica), `\textbf{creating}' (if the replica is in the state of uploading), `\textbf{thirdwheel}' (if the replica is marked for deletion because of too many replicas), `\textbf{stalled}' (if the the replica was in the state if uploading for a long time but hasn't arrived). We also store (in the \emph{states} section) the size of the file, the number of needed replicas, and a checksum of the file, which is created by some kind of checksumming algorithm (currently only md5 is supported).

\begin{description}
    \item [locations] section 
    \begin{itemize}
        \item \emph{(location, state) pairs}, where a location is a (\emph{URL}, \emph{referenceID}) pair serialized as a string, where \emph{URL} is the address of the Shepherd service storing this replica, \emph{referenceID} is the ID of the file within that Shepherd service.
    \end{itemize}
    \item [states] section 
    \begin{itemize}
        \item \emph{size}: the file size in bytes
        \item \emph{checksum}: checksum of the file
        \item \emph{checksumType}: the name of the checksum method
        \item \emph{neededReplicas}: how many valid replicas should this file have 
    \end{itemize}
\end{description}


% subsection files (end)


% The Librarian uses the A-Hash to store all the data about files and collections. The A-Hash is capable of storing property-value pairs organized in sections, which actually means that it stores (\emph{section}, \emph{property}, \emph{value}) tuples where each member is simply a string, e.g.~(`entry', `type', `collection') or (`ACL', `johnsmith', `owner') or (`timestamps', `created', `1196265901') or (`locations', `64CDF45F-DDFA-4C1D-8D08-BCF7810CB2AB:9A293F27DC86', `sentenced'). There could be only one \emph{value} for a (\emph{section}, \emph{property}) pair.


\subsection{Collections} % (fold)
\label{sub:collections}

A \emph{collection} is a list of files and other collections, which are in parent-children relationships forming a tree-hierarchy. Each entry has a unique name within a collection and a GUID which points to corresponding file or collection, so a collection is basically a list of name-GUID pairs, which list is stored in the \emph{entries} section. The collections could be in a closed state, which means that its contents should not be changed. If you close a collection then it cannot be opened again. However it cannot be guaranteed that the contents of a closed collection remains the same (e.g. if we include in our collection a file which we don't own, then the owner of the file could remove it nevertheless), but if a closed collection is changed, its state will be broken, and this state could never be changed again, which means you will always know if a closed collection is not intact anymore. This state is stored in the \emph{states} section, and its values could be `\textbf{no}' (if the collection is not closed), `\textbf{yes}' (if the collection is closed) and `\textbf{broken}' (if the collection was closed, but something happened, and its contents have been changed).

\begin{description}
    \item [entries] section 
    \begin{itemize}
        \item \emph{(name, GUID) pairs}: a collection is basically a list of name-GUID pairs. 
    \end{itemize}
    \item [states] section 
    \begin{itemize}
        \item \emph{closed}: this indicates if the collection is closed or broken. 
    \end{itemize}
\end{description}

% subsection collections (end)

\subsection{Mount Points} % (fold)
\label{sub:mount_points}

The mount point entry is a reference to an external third-party storage. Its metadata is basically just the URL of the external storage.

\begin{description}
    \item [mountpoint] section 
    \begin{itemize}
        \item \emph{externalURL}: the URL of the external storage.
    \end{itemize}
\end{description}


% subsection mount_points (end)

\subsection{Shepherds} % (fold)
\label{sub:shepherds}

The Librarian stores information about the registered Shepherd services. Each Shepherd reports its URL and the list of its files to a Librarian, and for each Shepherd a GUID is created. There is a special entry (with GUID `1' by default) which stores a list of the registered Shepherd services, storing their GUID and the timestamp of the last heartbeat message from them.

\begin{description}
	\item [nextHeartBeat] section
	\begin{itemize}
		\item (URL, timestamp) pairs contains when was the last heartbeat of this service
	\end{itemize}
	\item [serviceGUID] section
    \begin{itemize}
        \item (URL, GUID) pairs connects the URL of the Shepherd to the GUID where the information is stored about the Shepherd's stored files
    \end{itemize}
\end{description}

And for each Shepherd there is a separate entry with the list of all files stored on the given storage nodes:

\begin{description}
	\item [entry] section
    \begin{itemize}
        \item \emph{type}: `shepherd'
    \end{itemize}
	\item [files] section
	\begin{itemize}
	    \item \emph{(referenceID, GUID) pairs} for each replica stored on the Shepherd
	\end{itemize}
\end{description}

% subsection shepherds (end)


% section data_model (end)

\section{Security implementation} % (fold)
\label{sec:security_implementation}

The ARC HED hosting environment has a security framework, and it has its own policy language for describing access policies and requests. The storage system use a different internal format to store the access policies, but when the time comes to make decisions, it converts the stored policies and the incoming requests to the ARC native policy format, then asks the security framework to decide.

The internal representation of the access policies is based on access rules. A rule contains an identity and a list of actions. Currently the identity could be one of the following:
\begin{itemize}
    \item a Distinguish Name (DN) of a user, e.g. `\verb!/DC=eu/DC=KnowARC/O=NIIFI/CN=james!'
    \item a name of a Virtual Organization (VO) prefixed with `\verb!VOMS:!', e.g. `\verb!VOMS:knowarc.eu!'
    \item `\verb!ALL!' to specify that this rule will be applied to everyone
    \item `\verb!ANONYMOUS!' for a rule which will be applied to unauthenticated users (e.g. when non-secure HTTP is used and there is no DN)
\end{itemize}

Then for each rule there is list of actions, each action is either allowed or denied. In the internal representation of an access rule the list of actions is one single string, where the name of actions are separated with a space, and each name is prefixed with a `\verb!+!' or `\verb!-!' sign indicating if the action is allowed or denied for the given identity. The list of actions can be found in section~\ref{sub:high_level_authorization}.

When there is an incoming connection to a Bartender, the security framework of the HED extract the identity information of the connecting client (DN of client, DN of the issuer CA, extended attributes, etc.), and all these information is accessible by the Bartender. When the given method of the Bartender gets to the point where the access rules are present, and it is clear that what kind of action the user wants to do, then either the request and the policy is converted to the native ARC policy language, and then the policy evaluator of the security framework is called to make the decision. Within the native ARC policy language XML representation of the requests and policies the storage actions are put into an `Attribute' element with the ID `\verb!http://www.nordugrid.org/schemas/policy-arc/types/storage/action!'.

For inter-service authorization we can configure the services to only accept connections from trusted services. We trust a service if we know its DN. For each service we can write trusted DNs in the configuration, or we can periodically get the list from an A-Hash. This can be configured with a `\emph{TrustManager}' section in the service configuration, which contains these entries:

\begin{description}
    \item[DN] is a trusted DN
    \item[CA] is a DN of a trusted CA: we will trust all certificates issued by this CA
    \item[DNsFromAHash] is a list of A-Hash URLs (in \textbf{AHashURL}) from where we periodically get a list of trusted DNs (there can be multiple A-Hash URLs listed here in case of one is offline). This has some attributes: \textbf{CheckingInterval} specifies how frequently we should get the list from the A-Hash (default: 600); \textbf{ID} specifies the ID of the A-Hash objects, where the list is stored (default: `3').
\end{description}

The TrustManager section itself could have some attributes: \textbf{FromFile} could have a filename, if we want to get the whole TrustManager configuration from a separate file; and \textbf{Force} indicates (with `yes' or `no') that we want to ensure that our service does not even get the request if the client of the incoming connection is not trusted. 

Example TrustManager configuration:

\begin{verbatim}
    <TrustManager Force="yes">
        <DN>/DC=eu/DC=KnowARC/O=NIIFI/CN=host/epsilon</DN>
        <DN>/DC=eu/DC=KnowARC/O=NIIFI/CN=host/phi</DN>
        <CA>/DC=eu/DC=KnowARC/CN=storage-1233659377.11</CA>
        <DNsFromAHash CheckingInterval="10">
            <AHashURL>https://localhost:60000/AHash</AHashURL>
        </DNsFromAHash>
    </TrustManager>
\end{verbatim}

Because of the heavy use of X509 certificates, of course, all the services should have their own certificates, which they can use when connecting to other services. Currently it is needed to put the paths of the certificate and private key files (and the trusted CAs as well) into the configuration of each services, like this:

\begin{verbatim}
    <ClientSSLConfig>
        <KeyPath>certs/hostkey-epsilon.pem</KeyPath>
        <CertificatePath>certs/hostcert-epsilon.pem</CertificatePath>
        <CACertificatesDir>certs/CA</CACertificatesDir>
    </ClientSSLConfig>
\end{verbatim}

This can be put in a separate file, and then use the \textbf{FromFile} attribute of the ClientSSLConfig section to specify where the file is.

% section security_implementation (end)

\newpage

\section{A-Hash} % (fold)
\label{sec:a_hash}

\subsection{Functionality} % (fold)

The A-Hash is a database for storing string tuples, and it provides conditional and atomic modification of them. The A-Hash can be used as a centralized service, or it could be deployed in a distributed way, using multiple nodes, where all the data is replicated on all the nodes.

The A-Hash stores \emph{objects}, where each object has an arbitrary string \emph{ID}, and contains any number of \emph{property}-\emph{value} pairs grouped in \emph{section}s, where \emph{property}, \emph{value} and \emph{section} are arbitrary strings. There could only be a single \emph{value} for a \emph{property} in a \emph{section}.

If you have an ID, you can get all property-value pairs of the corresponding object with the \emph{get} method, or you could specify only which sections or properties do you need. You can add or remove property-value pairs of an object or delete all occurrences of a property or create a new object with the \emph{change} method, and you can specify conditions, which means the change is only applied if the given conditions are met.

% subsection functionality (end)

\subsection{Interface} % (fold)

\begin{description}
    \item [get(IDs, neededMetadataList)] returns all or some of the property-value pairs of the requested objects.
    
    The \emph{IDs} is a list of string \emph{ID}s, \emph{neededMetadataList} is a list of (\emph{section}, \emph{property}) pairs. If the \emph{neededMetadataList} is empty, then for each \emph{ID} it returns all the \emph{value}s for all the \emph{property} in all the \emph{section} in that given object. If there are sections and properties specified in \emph{neededMetadataList}, then only those values are returned.

    The response contains a list of \emph{object}s, where an \emph{object} is an (\emph{ID}, \emph{metadataList}) pair, where \emph{metadataList} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples.

    \item [change(changeRequestList)] modifies, removes or creates objects if certain conditions are met, and returns information about the success of the modification requests.
    
    The \emph{changeRequestList} is a list of \emph{changeRequest}s, where a \emph{changeRequest} is a tuple of (\emph{changeID}, \emph{ID}, \emph{changeType}, \emph{section}, \emph{property}, \emph{value}, \emph{conditionList}), where \emph{changeID} is an arbitrary ID which is used in the response to refer to this part of the request; \emph{ID} points to the object we want to change; \emph{changeType} can be `\textbf{set}' (to set the property within the section to value), `\textbf{unset}' (to remove the property from the section regardless of the value), `\textbf{delete}' (to remove the whole object); \emph{conditionList} is a list of \emph{condition}s, where a \emph{condition} is a tuple of (\emph{conditionID}, \emph{conditionType}, \emph{section}, \emph{property}, \emph{value}), where \emph{conditionType} could be `\textbf{is}' (which will be true if the property in the section is set to the value), `\textbf{isnot}' (which will be true if the property in the section is not set to the value), `\textbf{isset}' (which will be true if the property of the section is set, regardless of the value), `\textbf{unset}' (which will be true if the property of the section is not set at all).
    If all conditions are met, tries to apply changes to the objects, creates a new object if a previously non-existent ID is given.
    
    The response contains the \emph{changeResponseList} which is a list of (\emph{changeID}, \emph{success}, \emph{failedConditionID}) tuples. The \emph{success} of the change could be \textbf{set}, \textbf{unset}, \textbf{deleted}, \textbf{failed}, \textbf{condition not met} (in this case the ID of the failed condition is put into \emph{failedConditionID}), \textbf{invalid change type} or \textbf{unknown}.
    
    
\end{description}
    
% subsection interface (end)

\subsection{Implementation} % (fold)

The A-Hash service has a modular architecture which means that you can decide if you want to deploy it a centralized or a distributed way by simply specifying different modules in the service's configuration. There are different low-level modules for storing the data on disk, e.g. serialized into `pickle' format. And there is a module which stores the data replicated on multiple nodes.

The replicated A-Hash module is built on Oracle Berkeley DB High Availability, an open source database library with a replication API. The replication is based on a single master, multiple clients framework where all clients can read from the database, while only the master is allowed to write to the database. This ensures that all database transactions are ACID (atomic, consistent, isolated and durable). The replicated A-Hash module consists of three modules, \verb!ReplicatedAHash!, \verb!ReplicationStore! and \verb!ReplicationManager!.

\begin{description}
    \item[ReplicatedAHash] %TODO: describe ReplicatedAHash, sendMessage, processMessage
	is a subclass of the \verb!CentralizedAHash! business logic class and implements the extra logic needed for communication between A-Hash instances. The \verb!ReplicatedAHash! adds the method \verb!sendMessage! to the A-Hash interface and has a method \verb!processMessage! for processing messages from \verb!sendMessage!. These two methods are needed for the communication framework of the \verb!ReplicationManager!. The actual replication is managed by the \verb!ReplicationStore! and \verb!ReplicationManager!, meaning that the \verb!ReplicatedAHash!, with the exception of the above mentioned methods, is actually just a centralized A-Hash with a specific storage module.
    \item[ReplicationStore] %TODO: describe ReplicationStore, connection with TransDBStore, config, etc.
	is a specialized replicated DB. It is a subclass of \verb!TransDBStore!\footnote{A module for storing data on disk transactionally using the Berkeley DB library.} with some additional features for handling replication. The \verb!ReplicationStore! periodically updates an A-Hash object containing information about the current master A-Hash and available clients. In addition it contains a \verb!ReplicationManager! object which governs the replication framework.
    \item[ReplicationManager] %TODO: describe ReplicationManager, elections, communication (interservice communication, security, callback functions)
	handles all aspects of the Berkeley DB API and takes care of replicas, elections and replication messages.
	\begin{itemize}
	\item The replicated A-Hash consists of a set of replicas; one master and multiple clients. This means that the entire database is fully replicated on all replicas. When the master receives a write message, it will broadcast this message to all clients. To make sure that the db is consistent and that there will always be possible to elect a master, the master requires an acknowledgement message from a quorum of clients before continuing.
	\item In the replicated A-Hash there is one master and multiple clients. When the A-Hash starts up or if a master goes offline, we need to find a new master. This is solved by the clients holding an election, according to the Paxos algorithm. Simply told, all clients sends a vote to all other clients, and the eligible client that gets the most votes wins the election. An eligible client is a client which has received the latest updates. (See the Berkeley DB documentation for more details.) There are cases where some servers are more suitable to be master than others (better hardware, faster Ethernet, etc.). In such cases you can influence the election by setting a higher priority on the best servers. While an election is being held, it is not possible to write or read from the A-Hash, as consistency cannot be guaranteed without a master.
	\item As the replicas in the replicated A-Hash are services running within HED, the replication messages needs to be sent through the communication framework of HED. To be able to use this framework, the Berkeley DB needs a callback function for sending messages and the \verb!ReplicationManager! needs to call Berkeley DB to process incoming messages. The \verb!ReplicationMessage! takes a send function as one of the initialization arguments, and calls this function in the callback method. By providing the \verb!ReplicatedAHash.sendMessage! to the \verb!ReplicationManager! messages will be sent through the HED communication services just as in regular inter service communication. When \verb!ReplicatedAHash! receives a replication message from \verb!sendMessage! it will call \verb!ReplicationManager.processMessage! which in turn will call the Berkeley DB processMessage. This communication framework is both used by Berkeley DB and directly by the \verb!ReplicationManager! for, e.g., bootstrapping the A-Hash replicas and to communicate newly discovered clients.
\end{itemize}
\end{description}

% subsection implementation (end)

\subsection{Configuration} % (fold)


The A-Hash has the following configuration variables:

\begin{description}
    \item[AHashClass] tells the A-Hash service the name of the business-logic A-Hash class. Currently there are two implementation, the \verb!CentralAHash! is a centralized version, and the \verb!ReplicatedAHash! is a replicated version.
    \item[StoreClass] specifies the name of the store class. This class is used to store the data on disk. Currently there are several versions (\verb!PickelStore!, \verb!CachedPickleStore!, \verb!StringStore!, \verb!ZODBStore!, \verb!TransDBStore!), however only the \verb!TransDBStore! is suitable for using in a replicated deployment, all the others are only for a centralized deployement. This variable is only configurable for the centralized deployement.
    \item[StoreCfg] could contain some parameters for the different store classes, and it almost always contains a \emph{DataDir} parameter which specifies a directory on the (local) filesystem where the A-Hash should save its files. This variable is only configurable for the centralized deployement.
\end{description}

For the replicated A-Hash there are some additional required configuration variables:

\begin{description}
    \item[LocalDir] specifies a directory on the local filesystem where this A-Hash replica should be stored. This directory must be unique for this replica, i.e., no two A-Hash replicas can share the same \verb!LocalDir!, and doing so will most likely leave both replicas broken beyond repair.
    \item[MyURL] is the URL to this A-Hash service and is used as an ID for this service in the replication group.
    \item[OtherURL] is the URL of another A-Hash in the replication group. More than one URL can be specified here.
\end{description}

Even though the default settings are useable, there are some optional variables available for the replicated A-Hash:
TODO: Put in more optional variables.
\begin{description}
    \item[Priority] is related to the master election. If a master dies, there will be an election to decide who will be the new master. Higher priority means a higher chance to be elected. A priority of 0 means that this replica can never be elected. Defaults to 10.
    \item[CheckPeriod] specifies in seconds how frequently the A-Hash should update the replica list. Defaults to 10 seconds.
    \item[CacheSize] configures the amount of the storage to be cached in memory. Defaults to 10MB
\end{description}


An example configuration:

\begin{verbatim}
    <Service name="pythonservice" id="ahash">
        <ClassName>storage.ahash.ahash.AHashService</ClassName>
        <AHashClass>storage.ahash.ahash.CentralAHash</AHashClass>
        <StoreClass>storage.store.cachedpicklestore.CachedPickleStore</StoreClass>
        <StoreCfg>
            <DataDir>ahash_data</DataDir>
        </StoreCfg>
    </Service>
\end{verbatim}

Example of replicated A-Hash:
\begin{verbatim}
    <Service name="pythonservice" id="ahash1">
        <ClassName>storage.ahash.ahash.AHashService</ClassName>
        <AHashClass>storage.ahash.replicatedahash.ReplicatedAHash</AHashClass>
        <LocalDir>ahash_data1</LocalDir>
        <MyURL>http://localhost:60000/AHash1</MyURL>
        <OtherURL>http://localhost:60001/AHash2</OtherURL>
        <Priority>50</Priority>
        <ClientSSLConfig FromFile='clientsslconfig.xml'/>
    </Service>
\end{verbatim}

% subsection configuration (end)

% section a_hash (end)
\newpage

\section{Librarians} % (fold)
\label{sec:librarian}

\subsection{Functionality} % (fold)
% 

The Librarian knows about the type of possible entries in the namespace of the ARC storage system: files, collections and mount points. It knows that these are organized in a tree-hiearchy with grouping them into collections, and it knows about Logical Names, and how to traverse a Logical Name to find the metadata of the file or collection (or mount point) which it refers to. However, the Librarian does not maintain the namespace of the ARC storage, e.g. it does not put new files into collections, or remove the link from its parent collection when a file is removed - this is the job of the Bartender. The only Librarian method which deals with Logical Names is the \emph{traverseLN} method, which traverses the Logical Name as long as possible, and returns the GUID and metadata of the last found entry in the path of the Logical Name. All the other methods work with GUIDs, and do not care about the hierarchical namespace. In this regard the Librarian can be treated as a higher level application-specifc interface to the metadata store (the A-Hash), because while the A-Hash does not care about what it stores, the Librarian knowa exactly that it stores metadata about files, collections, etc. With this interface you can create new files, collections and mount points with the \emph{new} method, you can get or modify the metadata of any entry with the \emph{get} and the \emph{modifyMetadata} methods, and you can remove an entry with the \emph{remove} method.

Besides being an interface to the metadata store, the Librarian has an other importat functionality: it provides a way for Shepherds to register themselves, and to periodically send reports about the state of their stored files. A Shepherd send a \emph{report} message to a Librarian if e.g. a new file arrived, or an old file gets corrupted, thus the Librarian can keep the states of the replicas up-to-date. The Librarian monitors the registered Shepherds, and if one of them stops sending reports, the Librarian will assume, that it is offline, and modify the states of the all the replicas which are on the Shepherd's storage node. In order to do this, the Librarian stores for each Shepherd a list of files on the given storage nodes, this make it possible to know which files have replicas on a given storage nodes without checking all the files in the system. This list of files gets updated every time a Shepherd sends a report.

The end-user of the ARC storage system never communicates directly with a Librarian, the Bartenders will contact the Librarians in order to fulfil the user's requests.

% subsection functionality (end)

\subsection{Interface} % (fold)

\begin{description}

    \item [get(getRequestList, neededMetadataList)] returns all or some of the metadata of the the requested GUIDs.
    
    The \emph{getRequestList} is a list of string \emph{GUID}s, \emph{neededMetadataList} is a list of (\emph{section}, \emph{property}) pairs. If the \emph{neededMetadataList} is empty, then for each \emph{GUID} it returns all the metadata \emph{value}s for all the \emph{property}s in all the \emph{section}s. If there are sections and properties specified in \emph{neededMetadataList}, then only those values are returned.

    The response contains a list of \emph{GUID, metadatList} pairs, where \emph{metadataList} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples.

    \item[new(newRequestList)] creates a new entry in the Librarian with the given metadata, and returns the GUID of the new entry.
    
    The \emph{newRequestList} is a list of (\emph{requestID}, \emph{metadataList}) where \emph{requestID} is an arbitrary ID used to identify this request in the list of responses; \emph{metadataList} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples.
    This method generates a \emph{GUID} for each request, and inserts the new entry (with the given metadata) into the A-Hash, then returns the GUIDs of the newly created entries. In the metadata of the new entry the `type' property in the `entry' section defines whether it is a file, a collection or a mount point. The `GUID' property can contain a GUID if we want to specify the GUID of the new entry, and don't want the Librarian to generate a random one.
    
    The response contains a list of (\emph{requestID}, \emph{GUID}, \emph{success}) tuples.
    
    \item[modifyMetadata(modifyMetadataRequestList)] modifies the metadata of the given entries.
    
    \emph{modifyMetadataRequestList} is a list of (\emph{changeID}, \emph{GUID}, \emph{changeType}, \emph{section}, \emph{property}, \emph{value}) tuples where \emph{changeType} can be `\textbf{set}' (set the property in the section to value), `\textbf{unset}' (remove the property-value pair from the section), `\textbf{add}' (set the property in the section to value only if it does not exist already) or `\textbf{setifvalue}=value' (only set the property in the section to value if it currently equals to the `value' in the \emph{changeType}).

    The response is a list of (\emph{changeID}, \emph{success}) where \emph{success} can be `\textbf{set}', `\textbf{unset}', `\textbf{condition failed}', `\textbf{failed}: reason'.
    
    \item [remove(removeRequestList)] removes the given entries.

    The \emph{removeRequestList} is a list of (\emph{requestID}, \emph{GUID}) pairs.
    
    The response is a list of (\emph{requestID}, \emph{success}) pairs where \emph{success} could be `\textbf{removed}' or `\textbf{failed}: reason'.
        
    \item [traverseLN(traverseRequestList)] traverses the given Logical Names and returns extensive information about them.
    
    The \emph{traverseRequestList} is a list of (\emph{requestID}, \emph{LN}) with the Logical Names to be traversed
    
    The response is a list of (\emph{requestID}, \emph{traversedList}, \emph{wasComplete}, \emph{traversedLN}, \emph{GUID}, \emph{metadataList}, \emph{restLN}) where:
    \begin{description}
        \item[traversedList] is a list of (\emph{LNpart}, \emph{GUID}) pairs, where \emph{LNpart} is a part of the \emph{LN}, \emph{GUID} is the GUID of the Librarian-entry referenced by that part of the \emph{LN}, the first element of this list is the shortest prefix of the \emph{LN}, the last element is the \emph{traversedLN} without its last part
        \item[wasComplete] indicates whether the full \emph{LN} was traversed
        \item[traversedLN] is the part of the \emph{LN} which was traversed, if \emph{wasComplete} is true, this should be the full \emph{LN}
        \item[GUID] is the \emph{GUID} of the \emph{traversedLN}
        \item[metadataList] is all the metadata of the of traversedLN in the form of (\emph{section}, \emph{property}, \emph{value}) tuples
        \item[restLN] is the postfix of the \emph{LN} which was not traversed for some reason, if \emph{wasComplete} is true, this should be an empty string
    \end{description}
    
    \item [report(serviceID, filelist)] is a report message from a Shepherd to a Librarian, contains the ID of the Shepherd (currently the URL), and a list of changed files on the Shepherd's storage node.
        
    The \emph{filelist} is a list of (\emph{GUID}, \emph{referenceID}, \emph{state}) tuples containing the state of changed or new files, where referenceID is the Sheperd-local ID of the given replica where GUID refers to the logical file of this replica. The \emph{state} could be `\textbf{invalid}' (if the periodic self-check of the Shepherd found a non-matching checksum or missing file), `\textbf{creating}' (if this is a new file not uploaded yet), `\textbf{thirdwheel}' (if the replica is marked for deletion because of too many replicas), `\textbf{alive}' (if the file is uploaded and the checksum is OK), or `\textbf{stalled}' (if the file was in `creating' state for a long time but hasn't arrived).
    
    The response is \emph{nextReportTime}, a number of seconds, which is the timeframe within the Librarian expects the next heartbeat from the Shepherd.

\end{description}

% subsection interface (end)

\subsection{Implementation} % (fold)

The Librarian service uses the A-Hash to store all the metadata, and it uses the GUIDs as IDs in the database. It has the URL of one or more A-Hash services from its configuration, and it can acquire more A-Hash URLs from these first A-Hashes in case of a replicated A-Hash deployement.

%TODO: here comes some sentences about how the Librarian reads from any A-Hash and finds the master A-Hash for writing.
The replicated A-Hash deployment is based on a single master, multiple clients scenario (see Section~\ref{sec:a_hash} for details). This means that the Librarian can read from any A-Hash, while it can only write to the master A-Hash. A list of A-Hashes, master and clients, is stored in an A-Hash object. The Librarian periodically gets this list to maintain its own list of A-Hash services. If the Librarian tries to write to the replicated A-Hash and the master is down, the Librarian will try to get a new list from one of the clients and retry the write request once before giving up. In case of reading from the replicated A-Hash, the Librarian will randomly loop through the list of clients until the read request is successful or until there are no more clients available.

The Librarian also accepts report messages from Shepherds, and stores the contents of these messages in the A-Hash. There is one A-Hash object which contains the ID and the expected time of the next heartbeat of the registered Shepherds, and there is one A-Hash object for each Shepherd which stores the states and GUIDs of the files on the Shepherd's storage node (see Section~\ref{sub:shepherds} for details). The Librarian checks periodically if there is a Shepherd which is late with its heartbeat, and it changes the states of all the replicas if needed.

Because the Librarian services store everything in the A-Hash, the Librarian itself is a stateless service. We can deploy multiple Librarians, if we configure them to use the same replicated group of A-Hashes (or the same central A-Hash) then the Bartenders could use any Librarian to achieve the same results, and the Shepherds also could report to any of them. One Shepherd should only report to one Librarian, but if that Librarian is offline, it should find another. The reports will be registered in the A-Hash as well, so it does not matter which Librarian gets a report.

% subsection implementation (end)

\subsection{Configuration} % (fold)

The Librarian has these configuration variables:

\begin{description}
    \item[AHashURL] is the URL of an A-Hash which the Librarian should use. There could be multiple URLs specified here.
    \item[HeartbeatTimeout] specifies in seconds how frequently the Sheperds should send reports to the Librarian.
    \item[CheckPeriod] specifies in seconds how frequently the Librarian should check for late heartbeats.
\end{description}

An example configuration:

\begin{verbatim}
    <Service name="pythonservice" id="librarian">
        <ClassName>storage.librarian.librarian.LibrarianService</ClassName>
        <AHashURL>https://localhost:60000/AHash</AHashURL>
        <HeartbeatTimeout>30</HeartbeatTimeout>
        <CheckPeriod>20</CheckPeriod>
        <ClientSSLConfig FromFile="clientsslconfig.xml"/>
    </Service>
\end{verbatim}

% subsection configuration (end)

% section librarian (end)
\newpage

\section{Shepherds} % (fold)
\label{sec:shepherds}

\subsection{Functionality} % (fold)

A Shepherd service is capable of managing a storage node. It keeps track all the files it stores with their GUIDs and checksums. It periodically checks each file to detect corruption, and send reports to a Librarian indicating that the storage node is up and running, and whether some file's state has been changed. If a file goes missing or has a bad checksum then the Librarian is notified about the error (here the Shepherd refers to the file with its GUID, that's why it needs to store the GUIDs of its files). It periodically asks the Librarian how many replicas its files have, and if a file has fewer replicas than needed, the Shepherd offers its copy for replication by calling the Bartender.

A Shepherd service is always connected to a storage element service (e.g.~a web server). For each supported storage element service we need a backend module which makes the Shepherd capable of communicating with it to initiate file transfers, to detect whether a transfer was successful or not, to generate local IDs and checksums, etc.

A file in a storage node could be identified with a \emph{referenceID} which is unique within that node. If we know the \emph{location} of a file, which is the ID of the Shepherd service (\emph{serviceID}, currently the URL of the Shepherd) and the \emph{referenceID}, we could call the Shepherds \emph{get} method with the \emph{referenceID} and a list of transfer protocols we can use, the Shepherd chooses a protocol from this list which it can provide, and create a transfer URL (\emph{TURL}) and returns it along with the \emph{checksum} of the file. We could download the file from this \emph{TURL}, and verify it with the \emph{checksum}. An end user of the storage system does not need to call this \emph{get} method, because the Bartender service will do it, the user just asks the Bartender and gets the TURL.

Storing a file starts with initiating the transfer with the \emph{put} method of the Shepherd, we should give the \emph{size} and \emph{checksum} of the file and its \emph{GUID} as well. We also specify a list of transfer protocols we are able to use, and the Shepherd chooses a \emph{protocol}, creates a \emph{TURL} for uploading and generates a \emph{referenceID}, then we can upload the file to the TURL. Again, the end user just asks the Bartender, and gets the TURL, the user does not need to call the \emph{put} method of the Shepherd directly.

These \emph{TURL}s are one-time URLs which means that after the client uploads or downloads the file these \emph{TURL}s cannot be used again to do the same. If we want to download the same file twice, we have to initiate the transfer twice, and will get two different \emph{TURL}s.

With the \emph{stat} method we can get some information about a replica, e.g.~checksum, GUID, state, etc. The \emph{delete} method removes the replica.

In normal operation the \emph{put} and \emph{get} calls is made by a Bartender but the actual uploading and downloading is done by the user's client. In the case of replication a Shepherd with a valid replica initiates the replication, this Shepherd asks the Bartender to choose a new Shepherd, the Bartender initiates putting the new replica on a chosen Shepherd and receives the TURL, then the Bartender returns the TURL to the initiator Shepherd, which uploads its replica to the given TURL.

The replicas have a state, which could be `\textbf{creating}' when the transfer is initiated but the file is not uploaded yet, `\textbf{alive}' if the file is uploaded and has a proper checksum, `\textbf{invalid}' if it does not exists anymore or has a bad checksum, `\textbf{thirdwheel}' if the replica is marked for deletion because the file has too many replicas, or `\textbf{stalled}' if it was in a `creating' state for a long time but hasn't arrived.

% subsection functionality (end)

\subsection{Interface} % (fold)

\begin{description}
    
    \item[get(getRequestList)] initiates a download and returns the TURL
    
    The \emph{getRequestList} is a list of (\emph{requestID}, \emph{getRequestData}) where \emph{requestID} is an arbitrary ID used in the reply, \emph{getRequestData} is a list of (\emph{property}, \emph{value}) pairs, where mandatory properties are: `\textbf{referenceID}' which refers to the file to get and `\textbf{protocol}' indicates a protocol the client can use (there could be multiple protocols in getRequestData).
    
    The response is a list of (\emph{requestID}, \emph{getResponseData}), where \emph{getResponseData} is a list of (\emph{property}, \emph{value}) pairs, such as: `\textbf{TURL}' is a transfer URL which can be used by the client to download the file; `\textbf{protocol}' is the protocol of the TURL; `\textbf{checksum}' is the checksum of the replica; `\textbf{checksumType}' is the name of the checksum method and `\textbf{error}' could contain an error message if there is one.
     
    \item[put(putRequestList)] initiates an upload and returns the TURL
        
    The \emph{putRequestList} is a list of (\emph{requestID}, \emph{putRequestData}) where \emph{requestID} is an ID used for the response,     \emph{putRequestData} is a list of (\emph{property}, \emph{value}) pairs such as `\textbf{GUID}', `\textbf{checksum}', `\textbf{checksumType}', `\textbf{size}' (the size of the file in bytes), `\textbf{protocol}' (a protocol the client can use, can be multiple) and `\textbf{acl}' (which is currently not used).
    
    The response is a list of (\emph{requestID}, \emph{putResponseData}), where \emph{putResponseData} is a list of (\emph{property}, \emph{value}) pairs such as: `\textbf{TURL}' is the transfer URL where the client can upload the file, `\textbf{protocol}' is the chosen protocol of the TURL and `\textbf{referenceID}' is the generated ID for this new replica, `\textbf{error}' could contain an error message.
    
    \item[delete(deleteRequestList)] removes a replica.
    
    The \emph{deleteRequestList} is a list of (\emph{requestID}, \emph{referenceID}) pairs containing the IDs of the files to remove.
    
    The response is a list of (\emph{requestID}, \emph{status}), where \emph{status} could be `\textbf{deleted}' or `\textbf{nosuchfile}'.
    .
    \item[stat(statRequestList)] returns information about replicas.

    The \emph{statRequestList} is a list of (\emph{requestID}, \emph{referenceID}) where \emph{referenceID} points to the file whose data we want to get. 
    
    The response is a list of (\emph{requestID}, \emph{referenceID}, \emph{state}, \emph{checksumType}, \emph{checksum}, \emph{acl}, \emph{size}, \emph{GUID}, \emph{localID})    
\end{description}

% subsection interface (end)

\subsection{Implementation} % (fold)
\label{sub:shepherd_implementation}

The Shepherd is communicating with the storage element services via backend modules. Currently there are two backend modules implemented, one for the \emph{Hopi} service (which is a simple HED-based HTTP server), one for the \emph{apache} webserver.

In both cases the Shepherd and the transfer services should have access to the same local filesystem where the Shepherd creates two separate directories: one for storing all the files (e.g.~\verb!./store!) and one for the file transfers (e.g.~\verb!./transfer!). The store directory always contains all the files the Shepherd manages, the transfer directory is empty at the beginning.

Let's see the scenario for the Hopi service which should be in a special `slave' mode for this kind of operation: if a client asks for a file called \verb!file1!, and this file is in the store directory (\verb!./store/file!), then the Shepherd service creates a hardlink into the transfer directory (e.g.~\verb!./transfer/abc!) and sets this file read-only. If the Hopi service is configured that way that it handles the HTTP path \verb!/prb! and it is serving files from the directory \verb!./transfer! then after the hardlink is created, we have this URL for this file: \verb!http://localhost:60000/prb/abc!. Now we can give this URL to the client. Then the client \verb!GET!s this URL and gets the file. The Hopi service removes (unlinks) this file immediately after the \verb!GET! request arrived, which makes this \verb!http://localhost:60000/prb/abc! URL invalid (so this is a one-time URL), but because of the hardlink the file is still there in the store directory, it is just removed from the transfer directory. Now if some other user wants this file, the Shepherd creates an other hardlink, e.g.~\verb!./transfer/qwe! and now we have an URL \verb!http://localhost:60000/prb/qwe!.

If a client wants to upload a new file, then the Shepherd creates an empty file in the store directory, e.g.~\verb!./store/file2! and creates a hardlink into the transfer directory, e.g.~\verb!./transfer/oiu! and makes it writable, and now we have a URL \verb!http://localhost:60000/prb/oiu!, and the client is able to do a \verb!PUT! to this URL. When the client \verb!PUT!s the file there, the Hopi service immediately removes the uploaded file from the transfer directory, but because it has a hardlink in the store directory, the file is stored there as \verb!./store/file2!. The backend module for the Hopi service periodically checks whether a new file has two or just one hard links. If it has only one that means that a file is uploaded, so it could notify the Shepherd that the file is arrived. In order to do that, all the backend modules get a callback method `file\_arrived' from the Shepherd.

All the backend modules should have this common interface which the Shepherd can use to communicate with the storage element services:

\begin{description}
    \item[prepareToGet(referenceID, localID, protocol)] returns the \emph{TURL}.
    
    Initiate transfer with \emph{protocol} for the file which has these IDs: \emph{localID} and \emph{referenceID}. The reason for including here the referenceID as well is that this information could be used by the backend module later, e.g.~when the transfer finished and the state of the file needs to be changed.
    
    \item[prepareToPut(referenceID, localID, protocol)] returns the \emph{TURL}.
    
    Initiate transfer with \emph{protocol} for the file which has these IDs: \emph{localID} and \emph{referenceID}.
    
    \item[copyTo(localID, turl, protocol)] returns \emph{success}.
    
    Upload the file referenced by \emph{localID} to the given \emph{TURL} with the given \emph{protocol}.
    
    \item[copyFrom(localID, turl, protocol)] returns \emph{success}.
    
    Download the file from the given \emph{TURL} with the given \emph{protocol}, and store it as \emph{localID}.
    
    \item[list()] returns a list of \emph{localID}s currently in the store directory.
    \item[getAvailableSpace()] returns the available disk space in bytes.
    \item[generateLocalID()] returns a new unique \emph{localID}.
    \item[matchProtocols(protocols)] only leave that protocols in the list \emph{protocols} which are supported by this file transfer service.
    \item[checksum(localID, checksumType)] generates a \emph{checksumType} checksum of the file referenced by \emph{localID}.
\end{description}

% subsection implementation (end)

\subsection{Configuration} % (fold)

The Shepherd has these configuration variables:

\begin{description}
    \item[ServiceID] is the ID of the service, currently it should be the URL of the service, because there is no information system where we can discover services by IDs.
    \item[CheckPeriod] specifies in seconds how frequently should the Shepherd check the existence and the checksum of all the files on the storage node.
    \item[MinCheckInterval] specifies in seconds how much time should the Shepherd wait between two subsequent file checks. This ensures that the Shepherd does not use up all the resources of the hosting machine. After checking the cheksum of a file, the Shepherd will always wait this long before checking the next file, even if this means that checking all the files takes longer than the `CheckPeriod'.
    \item[CreatingTimeout] specifies in seconds after how much time the Shepherd decideds that a started file upload won't happen. This is not fatal, if the file is uploaded eventually, it will be a valid replica, but after this much time the system starts creating new replicas just in case.
    \item[StoreClass] specifies which type of store the Shepherd should use for storing its metadata on disk.
    \item[StoreCfg] can have parameters for the store class, e.g. the local directory to be used.
    \item[BackendClass] specifes which backend the Shepherd should use for communicating with the storage element service.
    \item[BackendCfg] has some configuration parameters for the backend class: \textbf{DataDir} and \textbf{TransferDir} is the directories the backend will use (see Section~\ref{sub:shepherd_implementation}), \textbf{TURLPrefix} should be the URL of the storage element service ending with a `/'.
    \item[LibrarianURL] is the URL of a Librarian where the Shepherd should send the reports.
    \item[BartenderULR] is the URL of a Bartender whom the Shepherd should connect if it wants to offer a replica for replication or recover a corrupted replica.
\end{description}

\begin{verbatim}
    <Service name="pythonservice" id="shepherd">
        <ClassName>storage.shepherd.shepherd.ShepherdService</ClassName>
        <ServiceID>https://localhost:60000/Shepherd</ServiceID>
        <CheckPeriod>20</CheckPeriod>
        <MinCheckInterval>0.1</MinCheckInterval>
        <CreatingTimeout>600</CreatingTimeout>
        <StoreClass>storage.store.cachedpicklestore.CachedPickleStore</StoreClass>
        <StoreCfg>
            <DataDir>./shepherd_data1</DataDir>
        </StoreCfg>
        <BackendClass>storage.shepherd.hardlinkingbackend.HopiBackend</BackendClass>
        <BackendCfg>
            <DataDir>./shepherd_store</DataDir>
            <TransferDir>./shepherd_transfer</TransferDir>
            <TURLPrefix>https://localhost:60000/hopi/</TURLPrefix>
        </BackendCfg>
        <LibrarianURL>https://localhost:60000/Librarian</LibrarianURL>
        <BartenderURL>https://localhost:60000/Bartender</BartenderURL>
        <ClientSSLConfig FromFile="clientsslconfig.xml"/>
    </Service>    
\end{verbatim}

% subsection configuration (end)

% section shepherds (end)
\newpage

\section{Bartenders} % (fold)
\label{sec:bartenders}

\subsection{Functionality} % (fold)

The Bartender provides an easy to use interface of the ARC storage system to the users. You can put, get and delete files using their logical names (\emph{LN}s) with the \emph{putFile}, \emph{getFile} and \emph{delFile} methods, create, remove and list collections with \emph{makeCollection}, \emph{unmakeCollection} and \emph{list}. You can remove the link to a file or sub-collection from its parent collection without removing the file or sub-collection itself with the \emph{unlink} method. The metadata of a file or collection (e.g.~whether the collection is closed, number of needed replicas, access policies) can be changed with \emph{modify}. A \emph{stat} call gives all the information about a file or collection, and you can move (or create hardlinks of) collections and files within the namespace with \emph{move}. You can upload an entirely new replica to a file (e.g.~if the file lost all its replicas, or when a Shepherd service offers its replica for replications) with \emph{addReplica}. You can create and remove mount points with the \emph{makeMountpoint} and \emph{unmakeMountpoint} methods.

The Bartender communicates with the Librarians to get and modify the metadata of files and collections, and it communicates with the Shepherds to initiate file transfers. The Bartender also has \emph{gateway modules} which are capable of communicating different kinds of thrid-party storage solutions. With these gateway modules it is possible to create mount points within the namespace of the ARC storage, and to access the namespace of the third-party storage through these mount points, which means the user can use the Bartender to get listing of directories on the third-party storage or to get files from the third-party storage. Thus the user can use a single client tool to access different kind of storages easily.

The high-level authorization is done by the Bartender. It makes decisions based on the identity of the connecting client and the access policy rules of the files and collections. The Bartender uses the security framework of the HED to evaluate the requests and the policies, which are genereted by the Bartender from the internal representation of the access rules. For more details see Section~\ref{sec:security_implementation}.

% subsection functionality (end)

\subsection{Interface} % (fold)

\begin{description}

    \item[putFile(putFileRequestList)] creates new files at the requested Logical Names, chooses a Shepherd and initiates the file transfer, and returns a TURL for each new file.
        
    The \emph{putFileRequestList} is a list of (\emph{requestID}, \emph{LN}, \emph{metadata}, \emph{protocols}), where \emph{requestID} is an arbitrary ID which will be used in the response; \emph{LN} is the requested Logical Name of the new file, \emph{protocols} is a list of protocols we can use for uploading, \emph{metadata} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples which should contain some mandatory properties in the `\textbf{states}' section: `\textbf{size}', `\textbf{checksum}', `\textbf{checksumType}', and  `\textbf{neededReplicas}'. 

    The response is a list of (\emph{requestID}, \emph{success}, \emph{TURL}, \emph{protocol}), where \emph{TURL} is a URL with a chosen \emph{protocol} which we can use to upload the file, the \emph{success} string could be `\textbf{done}', `\textbf{missing metadata}', `\textbf{parent does not exists}', `\textbf{internal error:} reason', etc.

    \item[getFile(getFileRequestList)] finds the files referenced by the given Logical Name, chooses a replica and initiates the download, and returns a TURL for each file.
    
    The \emph{getFileRequestList} is a list of (\emph{requestID}, \emph{LN}, \emph{protocols}) where \emph{requestID} is used in the response, \emph{LN} is the Logical Name referring to the file we want to get, \emph{protocols} is a list of transfer protocols we support.
    
    The response is a list of (\emph{requestID}, \emph{success}, \emph{TURL}, \emph{protocol}), where \emph{TURL} is the transfer URL using \emph{protocol}, with which we can download the file, \emph{success} could be `\textbf{done}', `\textbf{not found}', `\textbf{is not a file}', `\textbf{file has no valid replica}', `\textbf{error while getting TURL:} reason', etc.

    \item[delFile(delFileRequestList)] removes the files references by the given Logical Names.

    The \emph{delFileRequestList} is a list of (\emph{requestID}, \emph{LN}) with the Logical Name of the files we want to delete. 
    
    The response is a list of (\emph{requestID}, \emph{status}), where the \emph{status} could be `\textbf{deleted}' or `\textbf{nosuchLN}'.
        
    \item[unlink(unlinkRequestList)] remove a link from a collection without deleting the file or sub-collection itself.
    
    The \emph{unlinkRequestList} is a list of (\emph{requestID}, \emph{LN}) with the Logical Name of the files and sub-collections we want to unlink.
    
    The response is a list of (\emph{requestID}, \emph{success}) where \emph{success} can be `\textbf{unset}', `\textbf{no such LN}', `\textbf{denied}' or `\textbf{nothing to unlink}'
    
    \item[stat(statRequestList)] returns all metadata of files, collections or mount points

    The \emph{statRequestList} is a list of (\emph{requestID}, \emph{LN}) with the Logical Names we want to get information about.
    
    The response is a list of (\emph{requestID}, \emph{metadata}), where \emph{metadata} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples (see the data model in Section~\ref{sec:data_model}).
    
    \item[makeCollection(makeCollectionRequestList)] creates new collections.
    
    The \emph{makeCollectionRequestList} is a list of (\emph{requestID}, \emph{LN}, \emph{metadata}) where \emph{metadata} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples where in the `\textbf{entries}' section there could be the initial content of the catalog in the form of name-GUID pairs (these entries will be hard links to the given GUIDs with the given name) and in the `\textbf{states}' section there is the `\textbf{closed}' property (if it is `yes' then no more files can be added or removed later).
    
    The response is a list of (\emph{requestID}, \emph{success}), where \emph{success} can be `\textbf{done}', `\textbf{LN exists}', `\textbf{parent does not exist}', `\textbf{failed to create new catalog entry}', `\textbf{failed to add child to parent}', `\textbf{internal error}', etc.
    
    \item[unmakeCollection(unmakeCollectionRequestList)] deletes empty collections.

    The \emph{unmakeCollectionRequestList} is a list of (\emph{requestID}, \emph{LN}) with the Logical Names of the collections we want to remove. 

    The response is a list of (\emph{requestID}, \emph{success}), where \emph{success} could be `\textbf{removed}', `\textbf{no such LN}', `\textbf{collection is not empty}', `\textbf{failed}: reason'.
    
    \item[list(listRequestList, neededMetadata)] returns the contents of the requested collections.
        
    The \emph{listRequestList} is a list of (\emph{requestID}, \emph{LN}) where \emph{LN} is the Logical Name of the collection we want to list, \emph{neededMetadata} is a list of (\emph{section}, \emph{property}) pairs which filters the returned metadata.
    
    The response is a a list of (\emph{requestID}, \emph{entries}, \emph{status}) where entries is a list of (\emph{name}, \emph{GUID}, \emph{metadata}) where \emph{metadata} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples (see the data model in Section~\ref{sec:data_model}); the \emph{status} could be `\textbf{found}', `\textbf{not found}', `\textbf{is a file}'.
    
    \item[move(moveRequestList)] moves file or collections within the namespace which changes the Logical Name of the file or collection. This method only alters metadata, does not move real file data.

    The \emph{moveRequestList} is a list of (\emph{requestID}, \emph{sourceLN}, \emph{targetLN}, \emph{preserveOriginal}) where \emph{sourceLN} is the Logical Name referring to the file or collection we want to move (rename) and \emph{targetLN} is the new path, and if \emph{preserveOriginal} is true the \emph{sourceLN} would not be removed, so with \emph{preserveOriginal} we are actually creating a hard link.
    
    The response is a list of (\emph{requestID}, \emph{status}), where status could be `\textbf{moved}', `\textbf{nosuchLN}', `\textbf{targetexists}', `\textbf{invalidtarget}', `\textbf{failed adding child to parent}', `\textbf{failed removing child from parent}'.
    
    \item[modify(modifyRequestList)] modifies the metadata of files, collections and mount poins. Only the `states', the `policy' and the `metadata' sections can be modified, and there are separate access control actions for the three.
    
    The \emph{modifyRequestList} is a list of (\emph{changeID}, \emph{LN}, \emph{changeType}, \emph{section}, \emph{property}, \emph{value}) where \emph{changeType} can be `\textbf{set}' (set the \emph{property} in the \emph{section} to \emph{value}), `\textbf{unset}' (remove the \emph{property}-\emph{value} pair from the \emph{section}), `\textbf{add}' (set the \emph{property} in the \emph{section} to \emph{value} only if it is not exists already). 

    The response is a list of (\emph{changeID}, \emph{success}), where \emph{success} could be  `\textbf{set}', `\textbf{unset}', `\textbf{entry exists}' (for an `add' request), `\textbf{denied}', `\textbf{no such LN}', `\textbf{failed}: reason'.
   
    \item[makeMountpoint(makeMountpointRequestList)] used to create the mountpoint

    \emph{makeMountpointRequestList}, list of (\emph{requestID}, \emph{status})

    \emph{makeMountpoint} is quite similar to the \emph{makeCollection}. It is used to provide the access of the external storage system within the global namespace. \emph{makeMountpointRequestList} is a list of (\emph{requestID}, \emph{LN}, \emph{URL}, \emph{metadata} ) where \emph{metadata} is a list of (\emph{section}, \emph{property}, \emph{value}) tuples where in the section there could be the initial content of the catalog in the form of name-GUID pairs (these entries will be hard links to the given GUIDs with the given name). \emph{LN} will be requested Logical name and the \emph{URL} is the path to the external point that is required to access. In the policies section there could be some access policies, and in the metadata section there could be any other \emph{metadata} in key-value pairs. 

    The \emph{status} in the response could be \emph{done}, \emph{mountpoint exists}, \emph{parent does not exist}, \emph{internal error}, \emph{'failed: credentials not found'}, \emph{'cannot create anything in mountpoint'}

    \item[unmakeMountPoint(unmakeMountpointRequestList)] delete the mountpoint

    \emph{unmakeMountpointRequestList}, list of (\emph{requestID}, \emph{status})	

    \emph{unmakeMountpointRequestList} is a list of (requestID, LN) with the Logical Names of the mountpoints. \emph{status} could be \emph{removed}, \emph{no such LN}, \emph{Failed: Credential not available}. 

    \item[CredentialDelegation(CredentialDelegationRequest)] returns (\emph{status}, \emph{ID})
    
    From the client side \emph{createDelegation()} is called which interns initiate the \emph{CredentialDelegationInitRequest} for the service. After the successful handshake this method initiates an other request of \emph{UpdateCredentials(..)}, which completes this process. For complete details see ARC-Security-Document. Response from this interface could be \emph{done}, \emph{cannot delegate credentials, requested bartender does not supports the gateway}

    \item[RemoveCredentials(RemoveCredentialRequest)] returns (\emph{message}, \emph{status})

    \emph{RemoveCredentialRequest} initiates the process of deleting the user credential from the service side. The response could be \emph{Credential removed successfully}, \emph{cannot access proxyFile or ProxyStore problem at service side} 

\end{description}

% subsection interface (end)

\subsection{Implementation} % (fold)

For managing collections the Bartender needs only to communicate with a Librarian service, for managing files it needs to communicate with Shepherd services as well. When a Bartender wants to create a new replica of a file (or the first replica of a new file) it should find available Shepherd services. Fortunately the Librarian has a list of registered Shepherd services, so the Bartender can use that.

The Bartender can utilize gateway modules for accessing file listings and transfer URL from third-party storages. It works as an optional backend component of the Bartender service. \emph{Gateway} module is based on four basic methods \emph{get}, \emph{put}, \emph{list} and \emph{remove}. Since the task is to contact external store, which requires user to delegate the credentials so that it can communicate on behelf of the user.     

In order to access the \emph{files} and \emph{directories} from the external storage system, limited user credentials are passed to the \emph{Bartender} service.  Users can also remove their \emph{credentials} from the service side. These credentials are used in the methods like \emph{get}, \emph{put}, \emph{remove} and \emph{list} of the \emph{Gateway} module. These limited credentials are not used in any type of transfer, while listing files/directories, \emph{Gateway} uses the user credential, whereas in the \emph{get} and \emph{put} methods, it only uses the credentials to check the validity of the URL(s) before sending it to the user.
% subsection implementation (end)

\subsection{Configuration} % (fold)

The Bartender has these configuration variables:

\begin{description}
    \item[LibrarianURL] specifies the URL of one or more Librarian services.
    \item[ProxyStore] is a local directory where the Bartender will store the delegated proxies.
    \item[GatewayClass] is the name of the gateway module we want to use in this Bartender.
    \item[GatewayCfg] contains the configuration parameters of the gateway class: the \textbf{ProxyStore} is the directory where the gateway module could find delegated proxies, and the \textbf{CACertificatesDir} is a directory where the certificates of the trusted CAs are. 
\end{description}

An example configuration:

\begin{verbatim}
    <Service name="pythonservice" id="bartender">
        <ClassName>storage.bartender.bartender.BartenderService</ClassName>
        <LibrarianURL>https://sal1.uppmax.uu.se:60000/Librarian</LibrarianURL>
        <ProxyStore>/var/arc/spool/proxy_store</ProxyStore>
        <GatewayClass>storage.bartender.gateway.gateway.Gateway</GatewayClass>
        <GatewayCfg>
            <ProxyStore>/var/arc/spool/proxy_store</ProxyStore>
            <CACertificatesDir>/etc/grid-security/certificates/
        </GatewayCfg>
        <ClientSSLConfig FromFile='clientsslconfig.xml'/>
    </Service>
\end{verbatim}

% subsection configuration (end)

% section bartenders (end)
\newpage

\section{Client tools} % (fold)
\label{sec:client_tools}

\subsection{Prototype CLI tool} % (fold)
\label{sub:prototype_cli_tool}

In the first prototype release there is one client tool called \verb!arc_storage_cli!, which is written in Python, and only need a basic Python installation to run. It is capable of communicating with a given Bartender service, and uploading and downloading TURLs via HTTP. However in order to use proxy certificates or to transfer files with GridFTP protocol, you need to have the ARC client libraries installed.

The credentials of the user and the URL of the Bartender can be configured either with an XML file located at \verb!~/.arc/client.xml! or with environment variables.

Example for the \verb!client.xml! file:

\begin{verbatim}
    <ArcConfig>
      <ProxyPath>/Users/zsombor/Development/arc/proxy</ProxyPath>
      <CACertificatesDir>/Users/zsombor/Development/arc/certs/CA</CACertificatesDir>
      <BartenderURL>https://localhost:60000/Bartender</BartenderURL>
    </ArcConfig>
\end{verbatim}

Example for the environment variables:

\begin{verbatim}
    export ARC_BARTENDER_URL=https://localhost:60000/Bartender
    export ARC_KEY_FILE=/Users/zsombor/Development/arc/certs/userkey-john.pem
    export ARC_CERT_FILE=/Users/zsombor/Development/arc/certs/usercert-john.pem 
    export ARC_CA_DIR=/Users/zsombor/Development/arc/certs/CA    
\end{verbatim}

You can specify the number of needed replicas for new file with the \verb!ARC_NEEDED_REPLICAS! environment variable.

The \verb!arc_storage_cli! has its built-in help. The methods can be listed with:

\begin{verbatim}
$ arc_storage_cli
Usage:
  arc_storage_cli <method> [<arguments>]
  Supported methods: stat, make[Collection], unmake[Collection], list, move,
    put[File], get[File], del[File], pol[icy], unlink,
    credentialsDelegation, removeCredentials
\end{verbatim}
Without arguments, each method prints its own help:
\begin{verbatim}
$ arc_storage_cli move
Usage: move <sourceLN> <targetLN>
\end{verbatim}

Here is an example of uploading, downloading and stat files:

\begin{verbatim}
$ cat testfile 
This is a testfile.
$ arc_storage_cli put testfile /tmp/
- The size of the file is 20 bytes
- The md5 checksum of the file is 9a9dffa22d227afe0f1959f936993a80
- Calling the Bartender's putFile method...
- done in 0.08 seconds.
- Got transfer URL: http://localhost:60000/hopi/d15900f5-34ee-4bba-bb10-73d60d1c0d75
- Uploading from 'testfile'
    to 'http://localhost:60000/hopi/d15900f5-34ee-4bba-bb10-73d60d1c0d75' with http...
Uploading 20 bytes... data sent, waiting... done.
- done in 0.0042 seconds.
'testfile' (20 bytes) uploaded as '/tmp/testfile'.
$ arc_storage_cli stat /tmp/testfile
- Calling the Bartender's stat method...
- done in 0.05 seconds.
'/tmp/testfile': found
  states
    checksumType: md5
    neededReplicas: 1
    size: 20
    checksum: 9a9dffa22d227afe0f1959f936993a80
  timestamps
    created: 1210232135.57
  parents
    51e12fab-fd3d-43ec-9bc5-17041da3f0b2/testfile: parent
  locations
    http://localhost:60000/Shepherd fc0d3d99-6406-4c43-b2eb-c7ec6d6ab7fe: alive
  entry
    type: file
$ arc_storage_cli get /tmp/testfile newfile
- Calling the Bartender's getFile method...
- done in 0.05 seconds.
- Got transfer URL: http://localhost:60000/hopi/dab911d0-110f-468e-b0c3-627af6e3af31
- Downloading from 'http://localhost:60000/hopi/dab911d0-110f-468e-b0c3-627af6e3af31'
    to 'newfile' with http...
Downloading 20 bytes... done.
- done in 0.0035 seconds.
'/tmp/testfile' (20 bytes) downloaded as 'newfile'.
$ cat newfile 
This is a testfile.
\end{verbatim}

% subsection prototype_cli_tool (end)

\subsection{FUSE module} % (fold)
\label{sub:fuse_module}

The \textbf{FUSE module} provides a high-level access to the storage system. Filesystem in Userspace (FUSE) provides a simple library and a kernel-userspace interface. Using FUSE and the ARC Python interface, the FUSE module allows users to mount the storage namespace into the local namespace, enabling the use of, e.g., graphical file browsers. Currently no delegation handling is implemented in the fuse module, so that to change ownership, group access, etc., you need to use the \verb!arc_storage_cli!.

As for the \verb!arc_storage_cli! the credentials of the user and the URL of the Bartender can be configured either with an XML file located at \verb!~/.arc/client.xml! or with environment variables. See Section \ref{sub:prototype_cli_tool} for user configuration.

An example of uploading, downloading and list files with FUSE:

\begin{verbatim}
$ python arcfs.py ./mnt
$ cat testfile
This is a test file
$ mkdir mnt/tmp
$ cp testfile mnt/tmp
$ ls -l mnt/tmp/testfile
-rw-r--r-- 1 jonkni jonkni 20 2009-03-21 23:27 mnt/tmp/testfile
$ cp mnt/tmp/testfile newfile
$ cat newfile
This is a test file
$ cat mnt/tmp/testfile
This is a test file
$ fusermount -u ./mnt
\end{verbatim}

% subsection fuse_module (end)

% section client_tools (end)

\section{Grid integration} % (fold)
\label{sec:grid_integration}

To access data through the ARC middleware client tools, one needs to go through Data Manager Components (DMCs). These are protocol specific plugins to the client tools. E.g, if you need to access data from a HTTPS service, you will go through the HTTP DMC using a URL starting with \verb!https://!and if you want to access data from a SRM service, you will use the SRM DMC using a URL starting with \verb!srm://!. Similarly, if you want access the ARC storage system, you will use the ARC DMC with protocol using a URL starting with \verb!arc://!. 

As with the \verb!arc_storage_cli!, the credentials of the user and the URL of the Bartender can be configured with an XML file located at \verb!~/.arc/client.xml!. When the ARC DMC is initiated it will create an instance of \verb!DataPointARC! which provides functionality for listing collections and file IO. When downloading/uploading files from/to the storage system, the data point will use the provided Bartender URL to acquire a TURL from a Bartender. This TURL will be given to a \verb!DataTransfer!. Calling the constructor of the DataHandle class with a URL will trigger the iGetDataPoint class of all registered DMCs to be called until a non-NULL pointer is returned. This means that as long as there exists a data point plugin for the protocol of the returned TURL, the transfer can proceed.

Example of uploading, downloading and list files:

\begin{verbatim}
$ cat testfile
This is a test file
$ arccp testfile arc:///tmp/testfile
$ arcls -l arc:///tmp/testfile
testfile file * * * *
$ arccp arc:///tmp/testfile newfile
$ cat newfile
This is a test file
\end{verbatim}

% section grid_integration (end)

% chapter technical_description (end)

% \bibliography{grid}
\end{document}

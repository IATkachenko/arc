README file for the ARC DGBridge

== Files ==

README.DGBridge      - This file
dgbridge_service.ini - Sample ARC DGBridge configuration. Copy this to /etc/arc/service.ini


== Prerequisites ==

The NorduGrid ARC CE must be installed. The easiest way to accomplish
this is to first setup the NorduGrid repositories. On RHEL5/CentOS5 as root:

  % wget http://download.nordugrid.org/software/nordugrid-release/releases/1.0/redhat/el5/x86_64/nordugrid-release-1.0-1.el5.noarch.rpm
  % yum localinstall nordugrid-release-1.0-1.el5.noarch.rpm

== Installation ==

Installation of the ARC CE including the 3GBridge back-end on RHEL/Centos:

  % yum groupinstall "ARC Server"

On other RPM based platforms the actual name of the RPM is slightly different.

=== Configuration ===

Copy the file:

  dgbridge_service.ini

to /etc/arc/service.ini

Then tailor the configuration. All lines after the new line in a section must be filled in. Examles are in the comments in service.ini.

==== 3gbridge user creation ====

You must set up a user that will run the ARC services. This can be root but it is not recommended for security reasons.
Then set the user config option to that user in service.ini.

==== Setup directories ====

Now you must decide where ARC will store its data. This is normally: /var/spool/nordugrid/
When you've decided what directory to create, you must create the subdirectories:
   jobstatus/
   runtime/
   session/
These directories must be writable by the user chosen above.

==== Setup runtime environments ====

An important part of the setup is creating the RTEs. The RTEs will decide which applications and 3GBridges that are available. 
The RTEs are simple shell scripts that must set four variables:
   DG_App
   DG_Grid
   DG_Endpoint
   DG_AllowedVOs

An example RTE setting up DSP to be run on Asuka at SZTAKI:
   #!/bin/sh
   
   # This is a test RTE for the DG bridge on ASUKA
   # running the DSP app with ATTIC enabled WS
   
   DG_App="dsp"
   DG_Grid="SZDG"
   DG_Endpoint="http://asuka.lpds.sztaki.hu:8091"
   DG_AllowedVOs="all"
  
These files must be places in the runtime directory configured in service.ini.
The naming convention requires the file to be named woth only uppercase letters and without a suffix. 
RTEs pointing to a specific 3G bridge must be placed in a subdirectory named 3GBRIDGE/[bridge identifier], again the subdirectories must be all uppercase letters.
The RTE above is stored on the develop infrastructure as: /var/spool/nordugrid/runtime/3GBRIDGE/ASUKA/DSP.

==== Setup logging via cron ====

Add a cron entry to log every 5 minute:

  echo '0-59/5 * * * * /usr/libexec/arc/DGLog2XML.py'|crontab -u 3gbridge -

* Setup apache
You must setup apache or another webserver to serve the datastaging and monitor data. You can use whatever method available. 
The data can even be served from another machine as long as the backend can access it using the filesystem, e.g. WebDAV mounted using FUSE.

The development infrastructure uses an apache server configured as below:
  Listen 9090
  <VirtualHost edgi-dev.nbi.dk:9090>

    Alias /3GBridge "/var/www/3GBridge"

    <Directory /var/www/3GBridge>
      Options Indexes
      AllowOverride None
      Order allow,deny
      Allow from all
    </Directory>
  </VirtualHost>

Remember to also configure the data staging in the service.ini config file.

==== Certificates ====

To use ARC you must have a valid server certificate. The location must be set in the configuration.

==== Authorization ====

Authorization in the ARC bridge is a two step process. First the user certificate must be authorized.
Then the VO membership will be checked against the RTE.

The authorized users' DNs must be put in the gridmap file. The DNs must be mapped to the local user. An example:
   "/O=Grid/O=NorduGrid/OU=nbi.dk/CN=Christian Ulrik Soettrup" 3gbridge

Normally it is a bad idea to map the clients to the same user running the daemons, but it can be done on the bridge as long
as only 3GBridge RTEs are available. It of course also contingent on the daemons not running as root.

== Starting daemons ==

  % service arched start

== Testing ==

To test a simple DSP job:

  % arcproxy --voms <myvo>
  % arcsub -c ARC1:https://arex.example.com:60000/arex -f dsp.xrsl

where <myvo> is the allowed VOMS VO and arex.example.com is the fully qualified
hostname of the machine running the 3G bridge.

A simple dsp.xrsl looks like:

  $ cat dsp.xrsl
  &(executable="/3G")
  (arguments="-f 22 -i 22 -p 723 -n pools.txt")
  (runtimeenvironment="3GBRIDGE/ASUKA/DSP")
  (inputfiles=
  ("pools.txt"
    "attic://voldemort.cs.cf.ac.uk:7048/dl/meta/pointer/0c19f2b2-589a-4dad-902b-f6d2a3e7ad44;md5=7b7eb86bf50c58cbf92dc12ff5adf7f4:size=9652")
  )
  (outputfiles=
    ("cost.txt" "" )
  )

After submission the job status can be checked with something like:

  % arcstat <jobid>

where <jobid> looks like:

  https://arex.example.com:60000/arex/1234567812345678


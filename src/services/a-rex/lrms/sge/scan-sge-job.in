#!@posix_shell@ -f
# set -xv
#
# Periodically read log files of PBS and put mark files
# for job, which finished.
# If log files are not available scan for finished (absent) jobs 
# in PBS and put mark files for job, which finished.
#
#
# usage: scan_sge_job control_dir ...

basedir=`dirname $0`
basedir=`cd $basedir; pwd`

# Assume that gm-kick is installed in the same directory
GMKICK=${basedir}/gm-kick

##############################################################
# Set SGE specific environment.
##############################################################
if [ ! -f "${basedir}/configure-sge-env.sh" ] ; then
    echo "${basedir}/configure-sge-env.sh not found." 1>&2
    exit 1
fi
. "${basedir}/configure-sge-env.sh" 1>&2 || exit $?

##############################################################


umask 022

if [ -z "$1" ] ; then exit 1 ; fi

TMP_DIR=${TMPDIR:-@tmp_dir@}

# first control_dir is used for storing own files

control_dir=$1
control_dirs=
while [ $# -gt 0 ] ; do
  control_dirs="${control_dirs} $1"
  shift
done


# GD: no attempt to look for SGE Manager logfiles, restrict to job logs.


# Get all jobs
pids=`${SGE_BIN_PATH}/qstat -u '*' 2>/dev/null | sed -n 's/^  *\([0-9][0-9]*\) .*/\1/p'`
if [ $? != 0 ]; then
  echo "[`date`] Failed running ${SGE_BIN_PATH}/qstat" 1>&2
  sleep 60
  exit 1
fi

# Go through directories
for ctr_dir in $control_dirs ; do
  # Obtain ids of pending/running jobs stored in job.*.local
  rjobs=`find $ctr_dir -name 'job.*.status' 2>/dev/null | xargs egrep -lv 'DELETED|FINISHED' 2>/dev/null | sed s/status$/local/`
  if [ -z "$rjobs" ] ; then continue ; fi
  ids=`echo $rjobs | xargs grep -h '^localid=' 2>/dev/null | sed 's/^localid=\([^ ]*\)/\1/'`
  if [ -z "$ids" ] ; then continue ; fi
  # compare them to running jobs and find missing
  bids=
  for id in $ids ; do
    found=`echo "$pids" | grep "^$id"`
    if [ -z "$found" ] ; then
      bids="$bids $id"
    fi
  done
  # go through missing ids
  for id in $bids ; do
    # find grid job corresponding to current local id
    jobfile=`find $ctr_dir -name 'job.*.local' 2>/dev/null | xargs grep -l "localid=$id\$" 2>/dev/null`
    if [ -z "$jobfile" ] ; then continue ; fi
    # extract grid id
    gridid=`basename "$jobfile" '.local' | sed 's/^job\.//'`
    gramifile="${ctr_dir}/job.${gridid}.grami"
    donefile="${ctr_dir}/job.${gridid}.lrms_done"
    countfile="${ctr_dir}/job.${gridid}.lrms_job"
    failedfile="${ctr_dir}/job.${gridid}.failed"
    errorsfile="${ctr_dir}/job.${gridid}.errors"
    if [ -f "$donefile" ] ; then continue ; fi
    statusfile="${ctr_dir}/job.${gridid}.status"
    if [ ! -f "$statusfile" ] ; then continue ; fi
    status=`cat "$statusfile"`
    if [ ! "$status" = "INLRMS" ] ; then continue ; fi
    # get session directory of this job
    session=`grep -h '^sessiondir=' "$jobfile" | sed 's/^sessiondir=\(.*\)/\1/'`
    if [  -d "$session" ] ; then
      # try to obtain the exit code
      diagfile="${session}.diag"
      diagfile_tmp=`mktemp "$TMP_DIR/diag_tmp.XXXXXX"` || { sleep 60; exit 1; }
      diagfile_acct=`mktemp "$TMP_DIR/diag_acct.XXXXXX"` || { sleep 60; exit 1; }
      exitcode=`grep '^exitcode=' "$diagfile" | sed 's/^exitcode=//'`

      # qacct can take quite long. Here is a workaround.
      # Find the accounting file, and copy the last 10000
      # records to a temp file.
      acctfile=$SGE_ROOT/$SGE_CELL/common/accounting
      if [ -f  "$acctfile" ]; then
        briefacct=`mktemp "$TMP_DIR/accounting.XXXXXX"` || { sleep 60; exit 1; }
        tail -n 1000 "$acctfile" > "$briefacct"
        if [ $? = 0 ]; then extraargs="-f $briefacct"; fi
      fi

      # get accounting info. write diag file
      ${SGE_BIN_PATH}/qacct -j $id $extraargs 2> /dev/null \
          | perl -e 'while(<>){
                         $nodename=$1         if /^hostname\s+(\S+)/;
                         $id=$1               if /^jobnumber\s+(\S+)/;
                         $exitcode=$1         if /^exit_status\s+(\S+)/;
                         $failed=$1           if /^failed\s+(.*\S)/;
                         $CPUTime=$1          if /^cpu\s+(\d+)/;
                         $KernelTime=$1       if /^ru_stime\s+(\d+)/;
                         $WallTime=$1         if /^ru_wallclock\s+(\d+)/;
                         $UsedMemory=$1       if /^maxvmem\s+(\S+)M/;
                         $UsedMemory=$1*1024  if /^maxvmem\s+(\S+)G/;
                       }
                       END {
                         exit unless $id;
                         print "nodename=${nodename}\n";
                         print "CPUTime=${CPUTime}.0s\n";
                         print "WallTime=${WallTime}.0s\n";
                         print "KernelTime=${KernelTime}.0s\n";
                         print "UserTime=".int($CPUTime-$KernelTime).".0s\n";
                         print "UsedMemory=".int($UsedMemory*1024)."kB\n";
                         print "failed=$failed\n";
                         print "\nexitcode=$exitcode\n";
                       }' \
          > "$diagfile_acct"
      if [ "x$briefacct" != "x" ]; then rm -f "$briefacct"; fi

      # If the last qacct record is about migration,
      # we should wait for the next qacct record to appear
      # Delete file, like there was no accounting present at all!
      if grep -q "^failed=24 *: migrating"    "$diagfile_acct" \
      || grep -q "^failed=25 *: rescheduling" "$diagfile_acct"; then
          rm -f "$diagfile_acct"
          echo "Last qacct entry reports that job $id was migrated. Waiting for more info to appear in qacct" 1>&2
      fi

      # Add accounting info to $diagfile
      if [ -s "$diagfile_acct" ]; then

          # Accouting info is present
          accountinginfo=1
          cat "$diagfile" \
                        | grep -v "^nodename=" \
                        | grep -v "^WallTime=" \
                        | grep -v "^KernelTime=" \
                        | grep -v "^UserTime=" \
                        | grep -v "^CPUTime=" \
                        | grep -v "^MaxResidentMemory=" \
                        | grep -v "^AverageTotalMemory=" \
                        | grep -v "^exitcode=" \
          > "$diagfile_tmp"
          cat "$diagfile_tmp"  >  "$diagfile"
          cat "$diagfile_acct" >> "$diagfile"

          exitcode=`grep '^exitcode=' "$diagfile" | tail -n 1 | sed 's/^exitcode=//'`
          failedreason=`grep '^failed=' "$diagfile" | tail -n 1 | sed 's/^failed=//'`
          failedcode=`echo $failedreason | awk '{print $1}'`

          # Check for exceeded resources limits
          if [ -s "$gramifile" ]; then
            req_walltime=`sed -n "s/^joboption_walltime=//p" "$gramifile" | tail -n 1`
            req_cputime=`sed -n "s/^joboption_cputime=//p" "$gramifile" | tail -n 1`
            req_memory=`sed -n "s/^joboption_memory=//p" "$gramifile" | tail -n 1`

            used_walltime=`sed -n 's/^WallTime=\(.*\).0s/\1/p' "$diagfile" | tail -n 1`
            used_cputime=`sed -n 's/^CPUTime=\(.*\).0s/\1/p' "$diagfile" | tail -n 1`
            used_memory=`sed -n 's/^UsedMemory=\(.*\)kB/\1/p' "$diagfile" | tail -n 1`

            if [ ! -z "$used_memory" ] && [ ! -z "$req_memory" ] \
            && [ "$req_memory" != "" ] && [ "$req_memory" -gt 0 ] \
            && [ $(( 100*used_memory/1024/req_memory )) -gt 95 ]; then
              overlimit="memory"
            fi
            if [ ! -z "$used_cputime" ] && [ ! -z "$req_cputime" ] \
            && [ "$req_cputime" != "" ] && [ "$req_cputime" -gt 0 ] \
            && [ $(( 100*used_cputime/req_cputime )) -gt 95 ]; then
              overlimit="cputime"
            fi
            if [ ! -z "$used_walltime" ] && [ ! -z "$req_walltime" ] \
            && [ "$req_walltime" != "" ] && [ "$req_walltime" -gt 0 ] \
            && [ $(( 100*used_walltime/req_walltime )) -gt 95 ]; then
              overlimit="walltime"
            fi

            echo ++++++++++++++++++++++++++   >> "$errorsfile"
            echo Resources:                   >> "$errorsfile"
            echo ++++++++++++++++++++++++++   >> "$errorsfile"
            echo req_memory=$req_memory Mb    >> "$errorsfile"
            echo req_cputime=$req_cputime     >> "$errorsfile"
            echo req_walltime=$req_walltime   >> "$errorsfile"
            echo used_memory=$used_memory kB  >> "$errorsfile"
            echo used_cputime=$used_cputime   >> "$errorsfile"
            echo used_walltime=$used_walltime >> "$errorsfile"
            if [ ! -z "$overlimit" ]; then
              echo overlimit=$overlimit       >> "$errorsfile"
            fi
            echo ++++++++++++++++++++++++++   >> "$errorsfile"

          fi # grami file

          if [ "$failedcode" = "0" ]; then
            if [ "$exitcode" = "0" ]; then
              echo "SGE job $id finished succesfully" 1>&2
              echo "$exitcode" > "$donefile"
            else
              echo "SGE job $id failed with exit code $exitcode" 1>&2
              echo "$exitcode Job finished with non-zero exit code" > "$donefile"
            fi
          else
            # SGE reports a problem
            if [ -z "$failedcode" ]; then
              : # Should never happen
            elif [ "$failedcode" = "0" ]; then
              : # Should never happen
            elif [ "$failedcode" = "25" ]; then
              failedreason="SGE error $failedcode: Job will be rescheduled"
            elif [ "$failedcode" = "24" ]; then
              failedreason="SGE error $failedcode: Job will be migrated"
            elif [ "$failedcode" = "100" ]; then
	      # This happens when SGE signals the job, as in the case when a
	      # resource limit is exceeded.  We don't know for sure whether
	      # they were enforced or not but if a job is killed by SGE, this
	      # might the likely cause.
	      if [ -z "$overlimit" ]; then
                failedreason="SGE error $failedreason"
              elif [ $overlimit = "memory" ]; then
                failedreason="job killed: vmem"
              elif [ $overlimit = "cputime" ]; then
                failedreason="job killed: cput"
              elif [ $overlimit = "walltime" ]; then
                failedreason="job killed: wall"
              fi
            else
              failedreason="SGE error $failedreason"
            fi
            exitcode=$((failedcode+256))
            exitcode=271
            echo "SGE job $id failed: $failedreason" 1>&2
            echo $exitcode $failedreason > "$donefile"

            # Change exit code in the diag file
            cp "$diagfile" "$diagfile_tmp"
            if [ $? = 0 ]; then
              cat "$diagfile_tmp" | grep -v "^exitcode=" > "$diagfile"
              echo "exitcode=$exitcode"  >>  "$diagfile"
              rm -f "$diagfile_tmp"
            fi

          fi # failedcode

          # wake up GM
          $GMKICK "$statusfile" >> "$errorsfile"

          rm -f "$countfile"
          rm -f "$diagfile_tmp" "$diagfile_acct"

          # we're done, go to next job id
          continue

      fi # accounting info ok

      rm -f "$diagfile_acct"

    fi # session directory exists

    # This section is only reached when accounting info is not present
    # There is a certain lag between the end of the job
    # and the time when accouting information becomes available.
    # We do 5 retries, keeping the count in $countfile

    counter=0
    if [ -f "$countfile" ] ; then
      counter=`cat "$countfile"`
      counter=$(( $counter + 1 ))
    fi

    if [ "$counter" -gt 5 ]; then
      # Cannot wait more for accounting info.
      if [ -z "$exitcode" ]; then
        echo "SGE job $id finished with unknown exit code." 1>&2
        echo "256 Job disappeared from SGE." > "$donefile"
      else
        echo "SGE job $id failed with exit code $exitcode." 1>&2
        echo "$exitcode Job finished with non-zero exit code" > "$donefile"
      fi
      rm -f "$countfile"

      # wake up GM
      $GMKICK "$statusfile" >> "$errorsfile"

    else
      # test again for job existence, only count if not known
      ${SGE_BIN_PATH}/qstat -j $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "$counter" > "$countfile"
      fi
    fi
  done # loop over bids
done # loop over control_dirs
sleep 60
exit 0


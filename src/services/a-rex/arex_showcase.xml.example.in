<?xml version="1.0"?>
<!--



*************************************************************************
This file shows all the possible options that are recognized by A-REX and
related components. Do not use this as basis for creating your own config.
Rather start from a simpler template, and add to it options that you deem
appropiate.
*************************************************************************



-->
<ArcConfig
  xmlns="http://www.nordugrid.org/schemas/arcconfig/2009/08"
  xmlns:loader="http://www.nordugrid.org/schemas/loader/2009/08"
  xmlns:tcp="http://www.nordugrid.org/schemas/tcp/2009/08"
  xmlns:tls="http://www.nordugrid.org/schemas/tls/2009/08"
  xmlns:authz="http://www.nordugrid.org/schemas/arcauthz/2009/08"
  xmlns:idmap="http://www.nordugrid.org/schemas/identitymap/2009/10"
  xmlns:spdp="http://www.nordugrid.org/schemas/simplelistpdp/2009/08"
  xmlns:arex="http://www.nordugrid.org/schemas/a-rex/2009/08"
  xmlns:ip="http://www.nordugrid.org/schemas/a-rex/InfoProvider/2009/08"
  xmlns:lrms="http://www.nordugrid.org/schemas/a-rex/LRMS/2009/08"
  xmlns:janitor="http://www.nordugrid.org/schemas/Janitor/2009/08"
>
  <!-- Common configuration of the daemon -->
  <Server>
    <PidFile>/var/run/arched.pid</PidFile>
    <Logger>
      <File>/var/log/arc/arched.log</File>
      <Level>DEBUG</Level>
    </Logger>
  </Server>
  <!-- Where to find plugins -->
  <loader:ModuleManager>
    <loader:Path>@prefix@/lib/arc/</loader:Path>
  </loader:ModuleManager>
  <!-- Simply load all needed plugins -->
  <loader:Plugins>
    <loader:Name>mcctcp</loader:Name>
    <loader:Name>mcctls</loader:Name>
    <loader:Name>mcchttp</loader:Name>
    <loader:Name>mccsoap</loader:Name>
    <loader:Name>arcshc</loader:Name>
    <loader:Name>identitymap</loader:Name>
    <loader:Name>arex</loader:Name>
  </loader:Plugins>
  <!-- Create a chain -->
  <loader:Chain>
    <!-- TCP listening socket -->
    <loader:Component name="tcp.service" id="tcp">
      <loader:next id="tls"/>
      <tcp:Listen><tcp:Port>60000</tcp:Port></tcp:Listen>
    </loader:Component>
    <!-- Transport-level security -->
    <loader:Component name="tls.service" id="tls">
      <loader:next id="http"/>
      <!-- Location of server's security keys -->
      <tls:KeyPath>/etc/grid-security/hostkey.pem</tls:KeyPath>
      <tls:CertificatePath>/etc/grid-security/hostcert.pem</tls:CertificatePath>
      <tls:CACertificatesDir>/etc/grid-security/certificates</tls:CACertificatesDir>
      <tls:VOMSCertTrustDNChain>
        <tls:VOMSCertTrustRegex>.*</tls:VOMSCertTrustRegex>
      </tls:VOMSCertTrustDNChain>
      <!-- SecHandler below calls specified Policy Decision Point components.
           In this example only one PDP is defined - simplelist.pdp. This
           PDP compares Distinguished Name of connecting client against
           list of allowed DNs. DNs are stored in external file one per line.
           They may be enclosed in '"'. -->
      <loader:SecHandler name="arc.authz" id="pdps" event="incoming">
        <authz:PDP name="simplelist.pdp" spdp:location="/etc/grid-security/grid-mapfile"/>
      </loader:SecHandler>
      <!-- Evaluate requestor's identity into local identity.
           Comment it if no user mapping is needed. -->
      <loader:SecHandler name="identity.map" id="map" event="incoming">
        <!-- Safe choice if all other rules failed -->
        <idmap:PDP name="allow.pdp"><idmap:LocalName>nobody</idmap:LocalName></idmap:PDP>
      </loader:SecHandler>
    </loader:Component>
    <!-- HTTP processing is done here -->
    <loader:Component name="http.service" id="http">
      <loader:next id="soap">POST</loader:next>
      <loader:next id="plexer">GET</loader:next>
      <loader:next id="plexer">PUT</loader:next>
      <loader:next id="plexer">HEAD</loader:next>
    </loader:Component>
    <!-- This one parses content into XML tree -->
    <loader:Component name="soap.service" id="soap">
      <loader:next id="plexer"/>
    </loader:Component>
    <!-- Directing messages to proper service -->
    <loader:Plexer name="plexer.service" id="plexer">
      <loader:next id="a-rex">^/arex</loader:next>
    </loader:Plexer>
    <!-- A-Rex service -->
    <loader:Service name="a-rex" id="a-rex">
      <!-- Optional endpoint element is advised in case of multiple IP adresses -->
      <arex:endpoint>https://localhost:60000/arex</arex:endpoint>
      <!-- Use information generated by identity.map plugin or default provided below -->
      <arex:usermap><arex:defaultLocalName>nobody</arex:defaultLocalName></arex:usermap>
      <!-- legacy configuration file -->
      <arex:gmconfig type="INI">/etc/arc_arex.conf</arex:gmconfig>
      <arex:gmrun>internal</arex:gmrun>
      <arex:commonName>A-REX</arex:commonName>
      <arex:longDescription>ARC execution service</arex:longDescription>
      <arex:LRMSName>pbs</arex:LRMSName>
      <arex:OperatingSystem>LINUX</arex:OperatingSystem>
      <arex:debugLevel>VERBOSE</arex:debugLevel>
      <arex:serviceMail>support@cluster.org</arex:serviceMail>
      <arex:InfoproviderWakeupPeriod>30</arex:InfoproviderWakeupPeriod>
      <arex:useJanitor>yes</arex:useJanitor>
      <arex:loadLimits>
          <arex:maxJobsTracked>1000</arex:maxJobsTracked>
          <arex:maxJobsRun>100</arex:maxJobsRun>
          <arex:maxJobsPerDN>200</arex:maxJobsPerDN>
          <arex:maxJobsTransferred>20</arex:maxJobsTransferred>
          <arex:maxJobsTransferredAdditional>2</arex:maxJobsTransferredAdditional>
          <arex:maxFilesTransferred>4</arex:maxFilesTransferred>
          <arex:wakeupPeriod>30</arex:wakeupPeriod>
          <arex:maxLoadShare>4</arex:maxLoadShare>
          <arex:loadShareType>voms:role</arex:loadShareType>
      </arex:loadLimits>
      <arex:dataTransfer>
          <arex:secureTransfer>true</arex:secureTransfer>
          <arex:passiveTransfer>true</arex:passiveTransfer>
          <arex:localTransfer>false</arex:localTransfer>
          <arex:preferredPattern>http://mysite.ac.uk|.uk$|.ndgf.org$</arex:preferredPattern>
          <arex:timeouts>
              <arex:minSpeed>50</arex:minSpeed>
              <arex:minSpeedTime>300</arex:minSpeedTime>
              <arex:minAverageSpeed>200</arex:minAverageSpeed>
              <arex:maxInactivityTime>300</arex:maxInactivityTime>
          </arex:timeouts>
          <arex:maxRetries>4</arex:maxRetries>
          <arex:mapURL link="true">
              <arex:from>gsiftp://example.org:2811/data/</arex:from>
              <arex:to>/local/data/</arex:to>
              <arex:at>/local/data/</arex:at>
          </arex:mapURL>
          <arex:mapURL link="false">
              <arex:from>gsiftp://other.org/path/</arex:from>
              <arex:to>/local/path/</arex:to>
              <arex:at>/local/data/</arex:at>
          </arex:mapURL>
          <arex:Globus>
              <arex:gridmapfile>/etc/grid-security/grid-mapfile</arex:gridmapfile>
              <arex:CACertificatesDir>/etc/grid-security/certificates</arex:CACertificatesDir>
              <arex:CertificatePath>/etc/grid-security/hostcert.pem</arex:CertificatePath>
              <arex:KeyPath>/etc/grid-security/hostkey.pem</arex:KeyPath>
              <arex:TCPPortRange>8000-9000</arex:TCPPortRange>
              <arex:UDPPortRange>8000-9000</arex:UDPPortRange>
          </arex:Globus>
          <arex:httpProxy>http://proxy.org:8080</arex:httpProxy>
          <arex:enableDTR>true</arex:enableDTR>
          <arex:deliveryService>https://example.com:60002/datadeliveryservice</arex:deliveryService>
          <arex:localDelivery>true</arex:localDelivery>
      </arex:dataTransfer>
      <arex:jobLogPath>/var/log/arc/arex-jobs.log</arex:jobLogPath>
      <arex:jobReport>
          <arex:destination>https://grid.uio.no:8001/logger</arex:destination>
          <arex:expiration>26800</arex:expiration>
          <arex:type>SGAS</arex:type>
          <arex:parameters>ddd</arex:parameters>
          <arex:CACertificatesDir>/etc/grid-security/certificates</arex:CACertificatesDir>
          <arex:CertificatePath>/etc/grid-security/hostcert.pem</arex:CertificatePath>
          <arex:KeyPath>/etc/grid-security/hostkey.pem</arex:KeyPath>
      </arex:jobReport>
      <arex:authPlugin timeout="20" onSuccess="PASS" onFailure="FAIL" onTimeout="FAIL">
          <arex:state>ACCEPTED</arex:state>
          <arex:command>/opt/nordugrid/libexec/bank %C/job.%I.local %S</arex:command>
      </arex:authPlugin>
      <arex:control>
          <arex:username>.</arex:username>
          <arex:controlDir>/var/spool/arc/jobstatus</arex:controlDir>
          <arex:sessionRootDir>/export/sessions</arex:sessionRootDir>
          <arex:sessionRootDir>/export2/sessions</arex:sessionRootDir>
          <arex:cache>
              <arex:location>
                  <arex:path>/export/cache</arex:path>
                  <arex:link>/mnt/cache</arex:link>
              </arex:location>
              <arex:location>
                  <arex:path>/data/cache</arex:path>
              </arex:location>
              <arex:remotelocation>
                  <arex:path>/mnt/cache</arex:path>
              </arex:remotelocation>
              <arex:highWatermark>90</arex:highWatermark>
              <arex:lowWatermark>70</arex:lowWatermark>
              <arex:cacheLogLevel>INFO</arex:cacheLogLevel>
              <arex:cacheLifetime>60d</arex:cacheLifetime>
          </arex:cache>
          <arex:defaultTTL>259200</arex:defaultTTL>
          <arex:defaultTTR>2592000</arex:defaultTTR>
          <arex:maxReruns>5</arex:maxReruns>
          <arex:noRootPower>false</arex:noRootPower>
      </arex:control>

      <arex:helperUtility>
          <arex:username>grid01</arex:username>
          <arex:command>/usr/local/bin/myutility</arex:command>
      </arex:helperUtility>
      <arex:helperUtility>
          <arex:username>root</arex:username>
          <arex:command>/opt/ng/bin/cache-clean</arex:command>
      </arex:helperUtility>

      <arex:LRMS>
          <arex:type>pbs</arex:type>
          <arex:defaultShare>knowarc</arex:defaultShare>

          <arex:sharedFilesystem>true</arex:sharedFilesystem>
          <arex:sharedScratch>/mnt/scratch</arex:sharedScratch>
          <arex:scratchDir>/scratch</arex:scratchDir>
          <arex:runtimeDir>/export/SOFTWARE</arex:runtimeDir>
          <arex:GNUTimeUtility>/usr/bin/time</arex:GNUTimeUtility>

          <lrms:pbs_bin_path>/opt/torque/bin</lrms:pbs_bin_path>
          <lrms:pbs_log_path>/usr/spool/torque/server_logs</lrms:pbs_log_path>
          <lrms:dedicated_node_string>gridnode</lrms:dedicated_node_string>
          <lrms:maui_bin_path>/usr/local/bin</lrms:maui_bin_path>
          <lrms:condor_location>/opt/condor</lrms:condor_location>
          <lrms:condor_rank>(1-LoadAvg/2)*(1-LoadAvg/2)*Memory/1000*KFlops/1000000</lrms:condor_rank>
          <lrms:sge_bin_path>/opt/n1ge6/bin/lx24-x86</lrms:sge_bin_path>
          <lrms:sge_root>/opt/n1ge6</lrms:sge_root>
          <lrms:sge_cell>default</lrms:sge_cell>
          <lrms:sge_qmaster_port>536</lrms:sge_qmaster_port>
          <lrms:sge_execd_port>537</lrms:sge_execd_port>
          <lrms:lsf_bin_path>/usr/local/lsf/bin/</lrms:lsf_bin_path>
          <lrms:lsf_profile_path>/usr/share/lsf/conf</lrms:lsf_profile_path>
          <lrms:lsf_architecture>ia64</lrms:lsf_architecture>
          <lrms:ll_bin_path>/opt/ibmll/LoadL/full/bin</lrms:ll_bin_path>
          <lrms:ll_consumable_resources>yes</lrms:ll_consumable_resources>
          <lrms:slurm_bin_path>/usr/bin</lrms:slurm_bin_path>
          <lrms:slurm_wakeupperiod>30</lrms:slurm_wakeupperiod>
      </arex:LRMS>

      <arex:InfoProvider>
          <arex:debugLevel>INFO</arex:debugLevel>

          <ip:ProviderLog>/var/log/arc/infoprovider.log</ip:ProviderLog>

          <ip:ClusterName>Titan</ip:ClusterName>
          <ip:OtherInfo>This cluster is specially designed for XYZ applications: www.xyz.org</ip:OtherInfo>
          <ip:AdminDomain>HU/NIIF-TOP</ip:AdminDomain>
          <ip:StatusInfo>http://mycluster.org/grid/monitor</ip:StatusInfo>
          <ip:StatusInfo>http://mycluster.org/ganglia/</ip:StatusInfo>
          <ip:Benchmark>specfp2000 333</ip:Benchmark> <!-- ComputingManager.Benchmark -->
          <ip:Downtime>/var/run/arc/nextdowntime.ini</ip:Downtime> <!-- ComputingEndpoint.Downtime* -->

          <ip:PublishNordugrid>true</ip:PublishNordugrid> <!-- publish XML rendering of Nordugrid Info System -->

          <!-- only for publication in legacy infosys -->
          <ip:ClusterOwner>University of NeverLand</ip:ClusterOwner>
          <ip:ClusterOwner>King of the Jungle</ip:ClusterOwner>
          <ip:OpSys>Linux-2.6.9-34.EL</ip:OpSys>
          <ip:OpSys>glibc-2.3.4</ip:OpSys>
          <ip:Middleware>acme-1.0</ip:Middleware>
          <ip:AuthorizedVO>nordugrid.org</ip:AuthorizedVO>
          <ip:AuthorizedVO>ATLAS/no</ip:AuthorizedVO>
          <ip:LocalSE>gsiftp://my.storage/data2/</ip:LocalSE>
          <ip:InteractiveContactstring>gsissh://frontend.cluster:2200</ip:InteractiveContactstring>
          <!-- end legacy options -->

          <ip:Location>
              <ip:Name>Budapest, Hungary</ip:Name>
              <ip:Address>Victor Hugo 11</ip:Address>
              <ip:Place>Budapest</ip:Place>
              <ip:Country>Hungary</ip:Country>
              <ip:PostCode>HU-1200</ip:PostCode>
              <ip:Latitude>47.51</ip:Latitude>
              <ip:Longitude>19.05</ip:Longitude>
          </ip:Location>
          <ip:Contact>
              <ip:Name>NIIF User Support</ip:Name>
              <ip:Detail>mailto:support@niif.hu</ip:Detail>
              <ip:Type>usersupport</ip:Type>
          </ip:Contact>
          <ip:Contact>
              <ip:Name>Who to contact</ip:Name>
              <ip:OtherInfo>E-mail is preferred</ip:OtherInfo>
              <ip:Detail>http://www.niif.hu/contacts</ip:Detail>
              <ip:Type>general</ip:Type>
          </ip:Contact>

          <ip:AccessPolicy>
              <Rule>vo:atlas</Rule>
              <Rule>vo:nordugrid.org</Rule>
              <UserDomainID>voms:lcg-voms.cern.ch:15001/atlas</UserDomainID>
              <UserDomainID>voms:voms.ndgf.org:15015/nordugrid.org</UserDomainID>
          </ip:AccessPolicy>

          <ip:MappingPolicy>
              <ShareName>atlas</ShareName>
              <Rule>vo:atlas</Rule>
              <UserDomainID>voms:lcg-voms.cern.ch:15001/atlas</UserDomainID>
          </ip:MappingPolicy>
          <ip:MappingPolicy>
              <ShareName>knowarc</ShareName>
              <Rule>vo:nordugrid.org</Rule>
              <UserDomainID>voms:voms.ndgf.org:15015/nordugrid.org</UserDomainID>
          </ip:MappingPolicy>

          <!-- a group of similar nodes -->
          <!-- published as ExecutionEnvironment -->
          <ip:ExecutionEnvironment name="dualcore">

              <!-- options for selecting the nodes that belong into this nodeGroup -->
              <!-- needs backend suppot. Only inplemented for Fork and PBS -->
              <ip:NodeSelection>
                  <ip:Regex>^compute-2-[0-9]*</ip:Regex> <!-- INI tag: NodeSelectionRegex-->
                  <ip:Regex>^node-2-[0-9]*</ip:Regex>
                  <ip:Tag>bigmem</ip:Tag> <!-- INI tag: NodeSelectionTag-->
                  <ip:Command>/local/scripts/listnodes.sh quad</ip:Command> <!-- INI tag: NodeSelectionCommand-->
              </ip:NodeSelection>

              <ip:OtherInfo>shared area on GPFS</ip:OtherInfo>
              <ip:PhysicalCPUs>1</ip:PhysicalCPUs>
              <ip:LogicalCPUs>2</ip:LogicalCPUs>
              <ip:VirtualMachine>false</ip:VirtualMachine>
              <ip:Platform>amd64</ip:Platform>
              <ip:OSFamily>linux</ip:OSFamily>
              <ip:OSName>centos</ip:OSName>
              <ip:OSVersion>5.1</ip:OSVersion>
              <ip:CPUVendor>AuthenticAMD</ip:CPUVendor>
              <ip:CPUModel>Athlon 64 Dual Core 5000+</ip:CPUModel>
              <ip:CPUVersion>2</ip:CPUVersion>
              <ip:CPUClockSpeed>2600</ip:CPUClockSpeed>
              <ip:CPUTimeScalingFactor>1.4</ip:CPUTimeScalingFactor>
              <ip:WallTimeScalingFactor>1.4</ip:WallTimeScalingFactor>
              <ip:MainMemorySize>2048</ip:MainMemorySize>
              <ip:VirtualMemorySize>40000</ip:VirtualMemorySize>
              <ip:NetworkInfo>100mbitethernet</ip:NetworkInfo>
              <ip:ConnectivityIn>false</ip:ConnectivityIn>
              <ip:ConnectivityOut>true</ip:ConnectivityOut>
              <ip:Benchmark>specfp2000 333</ip:Benchmark>
              <ip:Benchmark>cfp2006 1200</ip:Benchmark>
          </ip:ExecutionEnvironment>

          <ip:ExecutionEnvironment name="doublequadcore">
              <ip:Homogeneous>false</ip:Homogeneous>
              <ip:PhysicalCPUs>2</ip:PhysicalCPUs>
              <ip:LogicalCPUs>8</ip:LogicalCPUs>
              <ip:Platform>adotf</ip:Platform>
              <ip:CPUVendor>adotf</ip:CPUVendor>
              <ip:CPUModel>adotf</ip:CPUModel>
              <ip:CPUClockSpeed>2400</ip:CPUClockSpeed>
              <ip:CPUTimeScalingFactor>1</ip:CPUTimeScalingFactor>
              <ip:WallTimeScalingFactor>1</ip:WallTimeScalingFactor>
              <ip:MainMemorySize>8096</ip:MainMemorySize>
              <ip:VirtualMemorySize>20000</ip:VirtualMemorySize>
              <ip:Benchmark>specint2000 944</ip:Benchmark>
              <ip:Benchmark>cint2006 1300</ip:Benchmark>
          </ip:ExecutionEnvironment>

          <!-- published as ComputingShare -->
          <ip:ComputingShare name="knowarc">
              <ip:Description>Condor pool for PGS</ip:Description>
              <ip:OtherInfo>High priority queue</ip:OtherInfo>
              <ip:MappingQueue>knowarc</ip:MappingQueue>
              <ip:ExecutionEnvironmentName>dualcore</ip:ExecutionEnvironmentName>
              <ip:MaxVirtualMemory>2000</ip:MaxVirtualMemory>
              <ip:MaxSlotsPerJob>8</ip:MaxSlotsPerJob>
              <ip:SchedulingPolicy>fairshare</ip:SchedulingPolicy>
              <ip:Preemption>false</ip:Preemption>
              <lrms:slurm_project>knowarc</lrms:slurm_project>
              <lrms:queue_node_string>gridlong_nodes</lrms:queue_node_string>
              <lrms:sge_jobopts>-P atlas -r yes</lrms:sge_jobopts>
              <lrms:condor_requirements>Memory&gt;=1000 &amp;&amp; (Arch == "INTEL" || Arch == "x86_64") || (Arch == "x86_64")</lrms:condor_requirements>
              <lrms:lsf_architecture>ia64</lrms:lsf_architecture>
          </ip:ComputingShare>

          <ip:ComputingShare name="atlas">
              <ip:Description>Condor pool - test of Atlas VO</ip:Description>
              <ip:ExecutionEnvironmentName>doublequadcore</ip:ExecutionEnvironmentName>
              <ip:ExecutionEnvironmentName>dualcore</ip:ExecutionEnvironmentName>
              <ip:MaxSlotsPerJob>1</ip:MaxSlotsPerJob>
              <ip:Preemption>true</ip:Preemption>
          </ip:ComputingShare>
      </arex:InfoProvider>

    </loader:Service>
  </loader:Chain>
</ArcConfig>

